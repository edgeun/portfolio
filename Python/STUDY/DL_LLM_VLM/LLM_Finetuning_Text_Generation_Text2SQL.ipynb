{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfET1RFtekDM"
   },
   "outputs": [],
   "source": [
    "# 데이터베이스에 대한 자연어 질의를 SQL 질의로 변경해주는 소규모 LLM 모델 개발  # 참고: LLM을 활용한 실전 AI 애플리케이션 개발\n",
    "# sLLM이 생성한 SQL이 데이터 요청을 잘 해결하는지 GPT-4 API를 활용해 성능을 평가\n",
    "# 데이터 출처: https://huggingface.co/datasets/shangrilar/ko_text2sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "HswgjifcgBWW",
    "outputId": "d9621aec-b9e9-45c9-b028-329f066131a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.8/275.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.4/150.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.6/174.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.4/215.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flask 3.1.0 requires Werkzeug>=3.1, but you have werkzeug 3.0.2 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.3.1 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
      "langchain 0.3.12 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.7.1 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n",
      "tensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autotrain-advanced 0.7.77 requires huggingface-hub==0.22.2, but you have huggingface-hub 0.27.1 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install transformers==4.40.1 bitsandbytes==0.43.1 accelerate==0.29.3 datasets==2.19.0 tiktoken==0.6.0 huggingface_hub==0.22.2 autotrain-advanced==0.7.77 -qqq\n",
    "!pip install --upgrade huggingface-hub -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5ccS1aZkDcV"
   },
   "outputs": [],
   "source": [
    "# 미세조정 수행 전 기초 모델 불러와서 평가하기\n",
    "## 기초 모델 생성\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def make_inference_pipeline(model_id):  # 입력한 모델 id에 맞춰 토크나이저와 모델을 불러오고, 양자화 파이프라인을 만들어서 반환하는 함수\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632,
     "referenced_widgets": [
      "ead3354525034dea9e4d3d26642f4219",
      "b168ccc6dc494a73b76a8a40c3f80d77",
      "cd7fb401a54e4cf5bd9b03327e6629d1",
      "0634771ea4ce4aa0ac758298b7c38aef",
      "99be42bddc414b0f8e1a9bc6b401a77c",
      "c845a3195e2a4b9b9ddf00c3b8535846",
      "6a26b26762034e0e9135c0841b6fcc6d",
      "881fc16f19de42a6a6fc85f094328e7f",
      "4df92616db7c40ff8a25a51c14112003",
      "0427e64b3cbb4889b115cc98e0c31146",
      "5206627bf5da463f94447df8d49f4164",
      "846ff3045b044b53a987b8bd58eaae40",
      "f6ab60e19c904338bf41deba09d4ac84",
      "453925ec083648d981bbc89af68ef548",
      "b5ee0547c0e44dedaf001211a311bbda",
      "5a38ec33872f41a9aeb69ac76be5d695",
      "9918f1b0bdca42b980c5175e2c1f2798",
      "9ee0f1e9766b41788e06a490ebf21e8a",
      "1e67da5e299b40c6a24641fd3e4b3af7",
      "7563e41f48ad4d9eb188e365153a6eef",
      "325fab9cef7144b7b7e7fba4150e0053",
      "a940d18ad81f4a75b378fef148196a75",
      "75ff47a0cfe64e74a8651fa8f084edcb",
      "4915019248054c1bb119e9cd7828ed92",
      "941c6014b45b45338de9a5a330562512",
      "ec69eff439474b38a259ad8defaa0a34",
      "2295bd0b2e1344d9bf08f5940947d0f0",
      "2b7ec9f5d0eb4d8ab269e8133f402ef2",
      "f6453cfe20b74530b2edc017a4d95058",
      "f3ac20ebe1af44c78165310a330df891",
      "87ec45334d344b388ee0c93d82bc1629",
      "8042d95729404fba9d491453d8c71d46",
      "8619d029345441ffb64d35f5705f1dcb",
      "b51d5f2cb2734b07b3abfe7903a759fc",
      "b699d3d1d5054a37861c7f4cda6a7c72",
      "125c396b78be424592ffb47a031c21b0",
      "12716c563b6d4f158108d82bfdb7b2e8",
      "f722265235fe4ca39fb69788d74cbd1a",
      "f3c539e446734b9dae97fbf4ab24b20f",
      "ad42e0c43a4a4cd882080a992bb74a04",
      "9ce8ec81daf14bd9a3140464c174d911",
      "68d3f5d8fb26489dacc1b123a1ffa4b0",
      "6c94b9ff8c9d495f962d721bfab3b55e",
      "fed9a97072f441bc95493203cc6b8865",
      "49d9b8c6d60241c9970b9c630684e69f",
      "0a531494d90f42f3855d24ea0a4d0e56",
      "a18b01c8a60845f18a812733c46b31f4",
      "0c2e1d9f56a249e2816f90a3c014cd82",
      "bb9777061e9b4cbf83d11e32d4e9eb90",
      "d442551cd00a4d33b24e03659024e503",
      "0e183223d0d34fdba9f485c190b5b976",
      "d3269c0485164e91a4d145cfa4795244",
      "561e0ad2e2a24d84bc1a2f227791db0d",
      "33c553ed81544e019c2825e859f6ea69",
      "4ea4d0fb93d94b5da9a1d1464b40f2ab",
      "8368c55e5b4d4e2994e0ae86b510a7d5",
      "1968ba7c550e49f6a46dc24db57d0d72",
      "a82d987b171e4787892caa578b71171b",
      "de1255e5f13b4087b629853f7880a84e",
      "4249dac9ea2446628bb1167ec34ee3e5",
      "ede237dea6db431295afd49126d1d9ef",
      "aa2cd1207ab04dd5995360a1a12e00be",
      "d856c13a765d4a81a5b3de8dfeea881f",
      "9f2b188c706a4973afea354d41b6d25e",
      "eb20d4469d624fa6a71bd59c8731bdd3",
      "04dfbd830b2e411c9218252c516bfe45",
      "add7f5964fc340a2800a2b4f1f23b922",
      "d13a6193d936447eafa3ba86bfaf9e22",
      "652ad82c394c46a5949bc80f9c040e66",
      "aac9117a252f488f94f1df302e45549e",
      "dad34bb270a4467e8f12ec4f83b42bdc",
      "f45ad14f10d24122808bc581c230d741",
      "83175111cdcd4956bcfe36475853f02b",
      "6e10252329b647709b47265988ae55fa",
      "e3fd312d3cdd484ebfeae6e5d085e8e9",
      "b5c7171d3622476e93cfcd5865209e6b",
      "3f7c0e8c7ed14d6eba7bd34ab90c04bb",
      "087e436514af45d492f962a29bc5250d",
      "fc654d5b93e54ddb826303ebd7772075",
      "571c4d0d2922430a9acb4723d8488c60",
      "d2f6d2df92764c7da6def4565e5b968c",
      "bf05f33b4b2a4a988eb6b1c02427437d",
      "451522650b9f45fab3d366e504024cf5",
      "ac8e1a0aab024d1586cf22435cfcbc0f",
      "1eeb681d775f4fb296e9d4d4eac05040",
      "f0a8bb8409c74611888edee3e5ef4596",
      "8103d6fd64ea436f931047e7b58e4e26",
      "5247522b3a7740d1b47300231e44120d",
      "7d84bf6e2b974780a707e03ebec4d67c",
      "62cd923644124697a50efc6ffa2d2ec0",
      "7e58c6d7d64443f7bce714abbc18a754",
      "51a180cda38c4448a181d3efaafd4f05",
      "91407efd8d7b44b4a9cd0b8f44182ef8",
      "c1e07f5e274a4ef6a8807e676f53b306",
      "0777e299895a43058ccedd74f36de866",
      "0084497e4ec0499282e7f3a8ab05e6e4",
      "d9a5619cbfae4191a15dacdf849e612b",
      "0cd173a48d47466bb61013d5df899945",
      "f6d7ff95c4134c6aa78429ae9f381adf",
      "bd7a175d38dc49e786dfc6cfdbe5750c",
      "4627e98831934f65ace1e7026fb72dca",
      "e2a357cfca8849ce91a5da8609038f20",
      "f3871a066a47448fa587d862454351ba",
      "96f790c906ed4809919f8652dcdd914d",
      "7946d6194bc645b39e72d25a84843dc8",
      "462731140d46421a88e21aa089dceb86",
      "9da29b03d77340429349d2a0bd4171c3",
      "eb94113f4dcf46a686933e02a364bddf",
      "bbca4255507846d896978decbd28b95f",
      "aec5a0c5825d4cc4ab7aa09313215552",
      "8e4c962a24d64a0885ea6fcdb32399b0",
      "b10f986b0beb4ec791cf15beeb36bcfa",
      "042ad6276b7a4a46bd15bc0488f5237b",
      "f2168785368147b2b535b78f2255fe2a",
      "85cd9cc0e9d14c7b80540c315ff943e3",
      "a3cf9c2e6d9f4f84899b9285ab69f351",
      "236442c8e2bf4b17bbafc0dbdabe143b",
      "6618622258814859bc1f893ea80afb8a",
      "d95d4843f8db4cd6932d838ad80718e5",
      "02f5f058e398480fb78c6f85fabf731f",
      "9eb3e3f30f224bba9e8b7bcfc95639e8",
      "492552c5701f4571af7be07238b24ccd",
      "525b9235504a48dc9ed2eb02ce1447da",
      "c5f2143e81204b3cb8ac2d7973e74e1b",
      "bfc9a1005ece49d8ab7e5e6f11f8b488",
      "37e502d904f44cb295d7940506190033",
      "b8e7b4a07a4d4afeb1ed14284291643c",
      "407e4b9eabca42eba8aab0f27b74f313",
      "91266a4d26694367a1b6589348b45951",
      "ad87e91a966d47d2abbfe694196c1c75",
      "2d8281612f3445a4b0be1cd73d3b1735",
      "15d91833aa5049a6b798a37f67d22124",
      "a5ec6c8c2d174b539325a0c427861e48",
      "f323fe124d0147bcb36c542e898596d2",
      "85af5beb1253437cb98fc06640b8c480",
      "e49c39f90edc4b40a2440224b7adb9a3",
      "14a2952af9614617b33532c097718cc1",
      "31df33151c9542748ab383f410d28297",
      "5c429e957d124a6c8888953504f1a876",
      "394519cb20534361ad77e0c7df0d380e",
      "023ade73235244d58a7e79fc9cebb54d",
      "e78366558547416db9331b6170b77f75",
      "3713c39b59414d27ba2e504a119c0735"
     ]
    },
    "id": "Jy04yfxy8L_g",
    "outputId": "a68ff5b0-cb6e-4af1-c89d-af8633a27157"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead3354525034dea9e4d3d26642f4219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846ff3045b044b53a987b8bd58eaae40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ff47a0cfe64e74a8651fa8f084edcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/573 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51d5f2cb2734b07b3abfe7903a759fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d9b8c6d60241c9970b9c630684e69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8368c55e5b4d4e2994e0ae86b510a7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add7f5964fc340a2800a2b4f1f23b922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087e436514af45d492f962a29bc5250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d84bf6e2b974780a707e03ebec4d67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7a175d38dc49e786dfc6cfdbe5750c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/2.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4c962a24d64a0885ea6fcdb32399b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/643M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492552c5701f4571af7be07238b24ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ec6c8c2d174b539325a0c427861e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'beomi/Yi-Ko-6B'  # Beomi님의 한/영 혼합 데이터셋으로 사전 학습한 모델\n",
    "hf_pipe = make_inference_pipeline(model_id)  # 'beomi/Yi-Ko-6B'모델로 파이프라인을 만들고 hf_pipe 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0vM8xsA83Z0",
    "outputId": "04d2e81c-5d77-4138-83c3-c59970109dae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n\\n### SQL 봇의 결과:\\n\\nSELECT COUNT(*) FROM players WHERE username LIKE\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "\n",
    "당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "### DDL:\n",
    "CREATE TABLE players (\n",
    "    player_id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "    username VARCHAR(255) UNIQUE NOT NULL,\n",
    "    email VARCHAR(255) UNIQUE NOT NULL,\n",
    "    password_hash VARCHAR(255) NOT NULL,\n",
    "    date_joined DATETIME NOT NULL,\n",
    "    last_login DATETIME\n",
    ");\n",
    "\n",
    "### Question:\n",
    "사용자 이름에 'admin'이 포함되어 있는 계정의 수를 알려주세요.\n",
    "\n",
    "### SQL:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "hf_pipe(example, do_sample=False, return_full_text=False, max_length=512, truncation=True)  # 예시 데이터를 작성해서 hf_pipe에 입력하고 결과 확인\n",
    "\n",
    "# 쿼리를 잘 생성하지만 이후에 반복으로 '봇의 결과: ~~'가 계속 생성되어 형식에 맞춰 답변하도록 추가적인 학습이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xiJS6a9uHl2n"
   },
   "outputs": [],
   "source": [
    "# 필요한 함수 생성\n",
    "## 1. SQL 프롬프트 생성 함수\n",
    "def make_prompt(ddl, question, query=''):  # DDL, 질문(question), 쿼리를 입력으로 받아 SQL 생성 프롬프트를 만듦\n",
    "    prompt = f\"\"\"\n",
    "                당신은 SQL을 생성하는 SQL 봇입니다. DDL의 테이블을 활용한 Question을 해결할 수 있는 SQL 쿼리를 생성하세요.\n",
    "\n",
    "                ### DDL:\n",
    "                {ddl}\n",
    "\n",
    "                ### Question:\n",
    "                {question}\n",
    "\n",
    "                ### SQL:\n",
    "                {query}\n",
    "            \"\"\"  # 프롬프트를 구성:\n",
    "                 # - SQL 생성에 필요한 정보를 포함하며, DDL(테이블 정의), 문제(질문), 이미 제공된 SQL(query, 선택사항)을 포함\n",
    "                 # - 쿼리(query)는 기본값으로 빈 문자열('')이 사용됨\n",
    "    return prompt  # 최종적으로 구성된 프롬프트를 반환\n",
    "\n",
    "\n",
    "## 2. 평가를 위한 요청 jsonl 작성 함수\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def make_requests_for_gpt_evaluation(df, filename, dir='requests'):  # DataFrame(df), 저장할 파일 이름(filename), 저장 경로(dir)를 입력으로 받음\n",
    "    if not Path(dir).exists():\n",
    "        Path(dir).mkdir(parents=True)  # 만약 디렉토리가 없으면 생성\n",
    "    prompts = []  # 빈 프롬프트 리스트 생성\n",
    "    for idx, row in df.iterrows():  # DataFrame(df)의 각 행(row)에 대해 반복\n",
    "        prompts.append(\"\"\"Based on below DDL and Question, evaluate gen_sql can resolve Question. If gen_sql and gt_sql do equal job, return \"yes\" else return \"no\". Output JSON Format: {\"resolve_yn\": \"\"}\"\"\" + f\"\"\"\n",
    "\n",
    "                        DDL: {row['context']}\n",
    "                        Question: {row['question']}\n",
    "                        gt_sql: {row['answer']}\n",
    "                        gen_sql: {row['gen_sql']}\"\"\"\n",
    "\n",
    "                        )\n",
    "\n",
    "                        # 각 행의 정보를 사용해 평가 프롬프트를 생성\n",
    "                        # - row['context']: DDL 정의\n",
    "                        # - row['question']: 질문\n",
    "                        # - row['answer']: 정답 SQL(gt_sql)\n",
    "                        # - row['gen_sql']: 생성된 SQL(gen_sql)\n",
    "                        # 프롬프트에 평가 조건을 명시적으로 포함\n",
    "\n",
    "    jobs = [{\"model\": \"gpt-4-turbo-preview\", \"response_format\" : { \"type\": \"json_object\" }, \"messages\": [{\"role\": \"system\", \"content\": prompt}]} for prompt in prompts]\n",
    "    with open(Path(dir, filename), \"w\") as f:  # 지정된 디렉토리(dir)와 파일 이름(filename)으로 파일을 열기(쓰기 모드)\n",
    "        for job in jobs:  # 생성된 요청(job)을 반복 처리\n",
    "             json_string = json.dumps(job)  # 요청(job)을 JSON 문자열로 변환\n",
    "             f.write(json_string + \"\\n\")    # JSON 문자열을 파일에 저장하며, 각 요청은 줄바꿈으로 구분\n",
    "\n",
    "    # 각 프롬프트를 기반으로 GPT 요청(job) 생성\n",
    "    # - 모델: gpt-4-turbo-preview\n",
    "    # - 메시지 형식: system 역할 메시지에 프롬프트 추가\n",
    "    # - 응답 형식(response_format): JSON 객체 형태로 지정\n",
    "\n",
    "\n",
    "## 3. 결과 jsonl 파일을 csv로 변환하는 함수\n",
    "def change_jsonl_to_csv(input_file, output_file, prompt_column=\"prompt\", response_column=\"response\"):\n",
    "    # JSONL 입력 파일(input_file), 변환된 CSV 출력 파일(output_file), 그리고 컬럼 이름(prompt_column, response_column)을 입력으로 받음\n",
    "    prompts = []    # 빈 프롬프트 리스트 생성\n",
    "    responses = []  # 빈 응답 리스트 생성\n",
    "    with open(input_file, 'r') as json_file:  # JSONL 파일을 읽기 모드로 열기\n",
    "        for data in json_file:  # JSONL 파일의 각 줄에 대해 반복 처리\n",
    "            prompts.append(json.loads(data)[0]['messages'][0]['content'])  # 각 JSONL 데이터의 첫 번째 메시지에서 프롬프트(content)를 추출해 리스트에 추가\n",
    "            responses.append(json.loads(data)[1]['choices'][0]['message']['content'])  # JSONL 데이터의 두 번째 메시지에서 응답(content)을 추출해 리스트에 추가\n",
    "\n",
    "    df = pd.DataFrame({prompt_column: prompts, response_column: responses})   # 추출된 프롬프트와 응답 데이터를 DataFrame으로 변환\n",
    "                                                                              # - prompt_column 이름으로 프롬프트 저장\n",
    "                                                                              # - response_column 이름으로 응답 저장\n",
    "    df.to_csv(output_file, index=False)  # 변환된 DataFrame을 지정된 CSV 파일(output_file)로 저장\n",
    "    return df  # 최종적으로 변환된 DataFrame을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "18ndWm1qN-dw",
    "outputId": "95d7e1c1-ca99-411b-89b6-5f00b022ec48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autotrain-advanced 0.7.77 requires datasets[vision]~=2.19.0, but you have datasets 3.2.0 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires huggingface-hub==0.22.2, but you have huggingface-hub 0.27.1 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "autotrain-advanced 0.7.77 requires tqdm==4.66.2, but you have tqdm 4.67.1 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.3.1 which is incompatible.\n",
      "langchain 0.3.12 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 2.7.1 which is incompatible.\n",
      "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.1 which is incompatible.\n",
      "tensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.16.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets huggingface_hub -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1sKU0otDllN"
   },
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "d4a5ec3aa93749889068796ab21391c8",
      "4c667c10ef8b427aaa10c4747605723f",
      "87036b2420e3461c9193759af68511b4",
      "5da2541c0d144c1f899d7abb77c280e0",
      "277b6914b01b4b11b8af911b28d25155",
      "95011e6d393045e7a33254ef5b37dd41",
      "c14482bde76e4592b8033dc2171cbec3",
      "0ee09141d17e46c09541bbaedb27c15f",
      "b14d897913cc4f06b8f9f1b4a722329c",
      "08b5a1443c114131a6d34709d53286ff",
      "b64a5e421241444d9f68cedded753cd4",
      "1f965c55317147d3947b9225e575b3dd",
      "e2e7731be89c465fb5762608858bcb73",
      "d113b79017b64879adbfd588783a925e",
      "08d1e23301b8498392a1f40290fab2ab",
      "630836049f6549da9fe135f5c6198a7f",
      "8789519821b0444991fa0d78980bce42",
      "3c7e51974037451fb3a7e9c94b2bc995",
      "9c33edf4b2684419a10ecb2e99834b15",
      "fb44651c9ee44fb48262c902d6fed41f",
      "03fa6973cb6147dcb09dbd16f45228bd",
      "2cd0621035284096ab8b86d203ad1c75",
      "6fe9114c5c20428c804319f86f03cc8b",
      "47af1570195f488486553bc3aeb57e44",
      "49c4013d95d740a6a00b4a02d333a3d8",
      "560111da99224304bdd096a6af806373",
      "07ab6f374f98419bac315cc03a331ac3",
      "108919e0097b496abb5ac44f1541504e",
      "405b30d4703d4132a05af7e60ec0fb42",
      "5c6cb2a5bef8485593adc40a469e90b5",
      "e42acc58cc94405f81574c807614756f",
      "63b0c4ba1375410195166f2c4541d0e3",
      "40e0ee6dfa914c219372c3947055c88d",
      "0fce0a68e608439bb73d9dcade926699",
      "fdf8408dca2d43c8bd6117ecde495bec",
      "45f7fccf07384f409672cef9d77bbe57",
      "5a7ac0fe66874eefb910e944cf78fd79",
      "8e480cb3d8df43e1acc87ae5f82282b5",
      "76b8de7d46bd4bf6b57e808f7f4516d6",
      "a6b44559d93a4fd2bc1b63c9dfce5a6e",
      "1b9878a06dea4d59b03f248237c1cb72",
      "9667887b0a84412bb1bd84cf83ad5a82",
      "323b918ba89e40a482a98285760ea353",
      "6ca3ee90b71d4aa18027c35350e2962f",
      "fdbeb24492864bfa9136a9abe176f711",
      "5e0e981dbd3c4788a7c03f1b3895bc2a",
      "bc9cd00e046c4b658efc815e64dc81ad",
      "0cea2da8a3cc4e5ba41783fdee44270b",
      "46f95078def747af81759b29e7b25e07",
      "cd408fc448b84b44807432dfd558dbae",
      "fee6da3a720c4bf58969dedeb8877e90",
      "238fc780e3854291b453597cdbbd1f48",
      "1536484f905848148b344b660df6efd6",
      "1c943a54dd5847008e4eda1610cf728f",
      "2c7d9aa515734f71b9a81297d3550e6f"
     ]
    },
    "id": "b25liuS5nexf",
    "outputId": "49f3461d-76c0-4964-9c20-cb600734ff1c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a5ec3aa93749889068796ab21391c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f965c55317147d3947b9225e575b3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/25.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe9114c5c20428c804319f86f03cc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/61.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fce0a68e608439bb73d9dcade926699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/38246 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbeb24492864bfa9136a9abe176f711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# 데이터셋 불러오기\n",
    "df = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")['test']  # shangrilar/ko_text2sql 데이터셋의 origin 버전에서 test 스플릿 데이터를 불러옴\n",
    "df = df.to_pandas()  # 허깅페이스 데이터셋 형식을 판다스 데이터프레임 형식으로 변환\n",
    "for idx, row in df.iterrows():  # 판다스 데이터프레임의 각 행을 반복(iterate)하면서 인덱스와 데이터를 가져옴\n",
    "    prompt = make_prompt(row['context'], row['question'])  # 각 행의 context(DDL 정보)와 question(SQL 질문)을 기반으로 프롬프트를 생성\n",
    "    df.loc[idx, 'prompt'] = prompt  # 생성된 프롬프트를 prompt 컬럼에 저장함  # .loc를 사용하여 특정 인덱스(idx)에 값을 지정\n",
    "\n",
    "# sql 생성\n",
    "gen_sqls = hf_pipe(df['prompt'].tolist(),   # pipeline(hf_pipe)을 사용하여 프롬프트로부터 SQL 쿼리를 생성\n",
    "                   do_sample=False,         # 확률적 샘플링 비활성화\n",
    "                   return_full_text=False,  # 입력 텍스트를 제외하고 모델 출력만 반환\n",
    "                   max_length=512,          # 출력 텍스트 길이 512 토큰으로 제한\n",
    "                   truncation=True)         # 입력 텍스트가 너무 길 경우 최대 길이 기준으로 잘라내서 모델에 전달\n",
    "\n",
    "gen_sqls = [x[0]['generated_text'] for x in gen_sqls]  # 생성된 결과에서 SQL 텍스트만 추출\n",
    "df['gen_sql'] = gen_sqls\n",
    "\n",
    "# 평가를 위한 requests.jsonl 생성\n",
    "eval_filepath = \"text2sql_evaluation.jsonl\"\n",
    "make_requests_for_gpt_evaluation(df, eval_filepath)  # 생성된 SQL과 실제 정답(SQL)을 평가하기 위해 JSONL 형식의 요청 파일을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NgkY8N_oK1u"
   },
   "outputs": [],
   "source": [
    "# OpenAI API키 입력\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"my_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRkIcG4cSqid",
    "outputId": "38975a37-217b-4389-f2ed-c583bb095a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting request #0\n",
      "INFO:root:Starting request #1\n",
      "INFO:root:Starting request #2\n",
      "INFO:root:Starting request #3\n",
      "INFO:root:Starting request #4\n",
      "INFO:root:Starting request #5\n",
      "INFO:root:Starting request #6\n",
      "INFO:root:Starting request #7\n",
      "INFO:root:Starting request #8\n",
      "INFO:root:Starting request #9\n",
      "INFO:root:Starting request #10\n",
      "INFO:root:Starting request #11\n",
      "INFO:root:Starting request #12\n",
      "INFO:root:Starting request #13\n",
      "INFO:root:Starting request #14\n",
      "INFO:root:Starting request #15\n",
      "INFO:root:Starting request #16\n",
      "INFO:root:Starting request #17\n",
      "INFO:root:Starting request #18\n",
      "INFO:root:Starting request #19\n",
      "INFO:root:Starting request #20\n",
      "INFO:root:Starting request #21\n",
      "INFO:root:Starting request #22\n",
      "INFO:root:Starting request #23\n",
      "INFO:root:Starting request #24\n",
      "INFO:root:Starting request #25\n",
      "INFO:root:Starting request #26\n",
      "INFO:root:Starting request #27\n",
      "INFO:root:Starting request #28\n",
      "INFO:root:Starting request #29\n",
      "INFO:root:Starting request #30\n",
      "INFO:root:Starting request #31\n",
      "INFO:root:Starting request #32\n",
      "INFO:root:Starting request #33\n",
      "INFO:root:Starting request #34\n",
      "INFO:root:Starting request #35\n",
      "INFO:root:Starting request #36\n",
      "INFO:root:Starting request #37\n",
      "INFO:root:Starting request #38\n",
      "INFO:root:Starting request #39\n",
      "INFO:root:Starting request #40\n",
      "INFO:root:Starting request #41\n",
      "INFO:root:Starting request #42\n",
      "INFO:root:Starting request #43\n",
      "INFO:root:Starting request #44\n",
      "INFO:root:Starting request #45\n",
      "INFO:root:Starting request #46\n",
      "INFO:root:Starting request #47\n",
      "INFO:root:Starting request #48\n",
      "INFO:root:Starting request #49\n",
      "INFO:root:Starting request #50\n",
      "INFO:root:Starting request #51\n",
      "INFO:root:Starting request #52\n",
      "INFO:root:Starting request #53\n",
      "INFO:root:Starting request #54\n",
      "INFO:root:Starting request #55\n",
      "INFO:root:Starting request #56\n",
      "INFO:root:Starting request #57\n",
      "INFO:root:Starting request #58\n",
      "INFO:root:Starting request #59\n",
      "INFO:root:Starting request #60\n",
      "INFO:root:Starting request #61\n",
      "INFO:root:Starting request #62\n",
      "INFO:root:Starting request #63\n",
      "INFO:root:Starting request #64\n",
      "INFO:root:Starting request #65\n",
      "INFO:root:Starting request #66\n",
      "INFO:root:Starting request #67\n",
      "INFO:root:Starting request #68\n",
      "INFO:root:Starting request #69\n",
      "INFO:root:Starting request #70\n",
      "INFO:root:Starting request #71\n",
      "INFO:root:Starting request #72\n",
      "INFO:root:Starting request #73\n",
      "INFO:root:Starting request #74\n",
      "INFO:root:Starting request #75\n",
      "INFO:root:Starting request #76\n",
      "INFO:root:Starting request #77\n",
      "INFO:root:Starting request #78\n",
      "INFO:root:Starting request #79\n",
      "INFO:root:Starting request #80\n",
      "INFO:root:Starting request #81\n",
      "INFO:root:Starting request #82\n",
      "INFO:root:Starting request #83\n",
      "INFO:root:Starting request #84\n",
      "INFO:root:Starting request #85\n",
      "INFO:root:Starting request #86\n",
      "INFO:root:Starting request #87\n",
      "INFO:root:Starting request #88\n",
      "INFO:root:Starting request #89\n",
      "INFO:root:Starting request #90\n",
      "INFO:root:Starting request #91\n",
      "INFO:root:Starting request #92\n",
      "INFO:root:Starting request #93\n",
      "INFO:root:Starting request #94\n",
      "INFO:root:Starting request #95\n",
      "INFO:root:Starting request #96\n",
      "INFO:root:Starting request #97\n",
      "INFO:root:Starting request #98\n",
      "INFO:root:Starting request #99\n",
      "INFO:root:Starting request #100\n",
      "INFO:root:Starting request #101\n",
      "INFO:root:Starting request #102\n",
      "INFO:root:Starting request #103\n",
      "INFO:root:Starting request #104\n",
      "INFO:root:Starting request #105\n",
      "INFO:root:Starting request #106\n",
      "INFO:root:Starting request #107\n",
      "INFO:root:Starting request #108\n",
      "INFO:root:Starting request #109\n",
      "INFO:root:Starting request #110\n",
      "INFO:root:Starting request #111\n",
      "WARNING:root:Request 15 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29526, Requested 528. Please try again in 108ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 17 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29558, Requested 536. Please try again in 188ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 46 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29579, Requested 555. Please try again in 268ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 100 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29630, Requested 460. Please try again in 180ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 101 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29744, Requested 579. Please try again in 646ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 102 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29741, Requested 526. Please try again in 534ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 104 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29932, Requested 554. Please try again in 972ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 42 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29838, Requested 534. Please try again in 744ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 53 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29855, Requested 562. Please try again in 834ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 105 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29854, Requested 598. Please try again in 904ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 106 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29448, Requested 593. Please try again in 82ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 107 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29468, Requested 546. Please try again in 28ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 109 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29496, Requested 620. Please try again in 232ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 108 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29493, Requested 552. Please try again in 90ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 59 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29806, Requested 464. Please try again in 540ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 39 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29805, Requested 387. Please try again in 384ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 55 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29802, Requested 449. Please try again in 502ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 111 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29803, Requested 372. Please try again in 350ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 48 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29658, Requested 599. Please try again in 514ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 41 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29459, Requested 561. Please try again in 40ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 25 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29605, Requested 574. Please try again in 358ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 68 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29601, Requested 490. Please try again in 182ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 50 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29906, Requested 321. Please try again in 454ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 30 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29894, Requested 458. Please try again in 704ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 44 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29853, Requested 520. Please try again in 746ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 60 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29641, Requested 538. Please try again in 358ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 94 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29637, Requested 475. Please try again in 224ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 56 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29640, Requested 608. Please try again in 496ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 14 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29621, Requested 454. Please try again in 150ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 54 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29598, Requested 515. Please try again in 226ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 10 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29590, Requested 568. Please try again in 316ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 49 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29721, Requested 530. Please try again in 502ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 20 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29713, Requested 565. Please try again in 556ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 91 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29650, Requested 622. Please try again in 544ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 40 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29646, Requested 546. Please try again in 384ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 37 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29636, Requested 543. Please try again in 358ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 43 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29888, Requested 452. Please try again in 680ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 27 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29888, Requested 464. Please try again in 704ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 58 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29873, Requested 643. Please try again in 1.032s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 51 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29860, Requested 561. Please try again in 842ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 26 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29835, Requested 540. Please try again in 750ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 36 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29798, Requested 537. Please try again in 670ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 18 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29774, Requested 471. Please try again in 489ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 38 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29723, Requested 542. Please try again in 530ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "/content/api_request_parallel_processor.py:164: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(\n",
      "WARNING:root:Pausing to cool down until Thu Jan 23 15:56:37 2025\n",
      "INFO:root:Starting request #15\n",
      "WARNING:root:Pausing to cool down until Thu Jan 23 15:56:37 2025\n",
      "INFO:root:Starting request #17\n",
      "INFO:root:Starting request #46\n",
      "INFO:root:Starting request #100\n",
      "INFO:root:Starting request #101\n",
      "INFO:root:Starting request #102\n",
      "INFO:root:Starting request #104\n",
      "INFO:root:Starting request #42\n",
      "INFO:root:Starting request #53\n",
      "INFO:root:Starting request #105\n",
      "INFO:root:Starting request #106\n",
      "INFO:root:Starting request #107\n",
      "INFO:root:Starting request #109\n",
      "INFO:root:Starting request #108\n",
      "INFO:root:Starting request #59\n",
      "INFO:root:Starting request #39\n",
      "INFO:root:Starting request #55\n",
      "INFO:root:Starting request #111\n",
      "INFO:root:Starting request #48\n",
      "INFO:root:Starting request #41\n",
      "INFO:root:Starting request #25\n",
      "INFO:root:Starting request #68\n",
      "INFO:root:Starting request #50\n",
      "INFO:root:Starting request #30\n",
      "INFO:root:Starting request #44\n",
      "INFO:root:Starting request #60\n",
      "INFO:root:Starting request #94\n",
      "INFO:root:Starting request #56\n",
      "INFO:root:Starting request #14\n",
      "INFO:root:Starting request #54\n",
      "INFO:root:Starting request #10\n",
      "INFO:root:Starting request #49\n",
      "INFO:root:Starting request #20\n",
      "INFO:root:Starting request #91\n",
      "INFO:root:Starting request #40\n",
      "INFO:root:Starting request #37\n",
      "INFO:root:Starting request #43\n",
      "INFO:root:Starting request #27\n",
      "INFO:root:Starting request #58\n",
      "INFO:root:Starting request #51\n",
      "INFO:root:Starting request #26\n",
      "INFO:root:Starting request #36\n",
      "INFO:root:Starting request #18\n",
      "INFO:root:Starting request #38\n",
      "WARNING:root:Request 59 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29591, Requested 464. Please try again in 110ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 55 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29590, Requested 449. Please try again in 78ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 48 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29587, Requested 599. Please try again in 372ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 111 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29973, Requested 372. Please try again in 690ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 41 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29971, Requested 561. Please try again in 1.064s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 68 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29971, Requested 490. Please try again in 922ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 25 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29966, Requested 574. Please try again in 1.08s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 50 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29962, Requested 321. Please try again in 566ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 30 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29960, Requested 458. Please try again in 836ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 94 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29953, Requested 475. Please try again in 856ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 44 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29956, Requested 520. Please try again in 952ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 56 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29953, Requested 608. Please try again in 1.122s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 60 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29949, Requested 538. Please try again in 973ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 14 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29951, Requested 454. Please try again in 810ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 49 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29945, Requested 530. Please try again in 950ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 10 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29944, Requested 568. Please try again in 1.024s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 91 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29937, Requested 622. Please try again in 1.118s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 40 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29934, Requested 546. Please try again in 960ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 20 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29935, Requested 565. Please try again in 1s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 37 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29935, Requested 543. Please try again in 956ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 58 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29930, Requested 643. Please try again in 1.146s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 27 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29922, Requested 464. Please try again in 772ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 18 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29922, Requested 471. Please try again in 786ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 26 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29920, Requested 540. Please try again in 920ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 36 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29922, Requested 537. Please try again in 918ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 51 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29912, Requested 561. Please try again in 945ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 38 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29906, Requested 542. Please try again in 896ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 43 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29770, Requested 452. Please try again in 444ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 54 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29708, Requested 515. Please try again in 446ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Pausing to cool down until Thu Jan 23 15:56:53 2025\n",
      "INFO:root:Starting request #59\n",
      "WARNING:root:Pausing to cool down until Thu Jan 23 15:56:53 2025\n",
      "INFO:root:Starting request #55\n",
      "INFO:root:Starting request #48\n",
      "INFO:root:Starting request #111\n",
      "INFO:root:Starting request #41\n",
      "INFO:root:Starting request #68\n",
      "INFO:root:Starting request #25\n",
      "INFO:root:Starting request #50\n",
      "INFO:root:Starting request #30\n",
      "INFO:root:Starting request #94\n",
      "INFO:root:Starting request #44\n",
      "INFO:root:Starting request #56\n",
      "INFO:root:Starting request #60\n",
      "INFO:root:Starting request #14\n",
      "INFO:root:Starting request #49\n",
      "INFO:root:Starting request #10\n",
      "INFO:root:Starting request #91\n",
      "INFO:root:Starting request #40\n",
      "INFO:root:Starting request #20\n",
      "INFO:root:Starting request #37\n",
      "INFO:root:Starting request #58\n",
      "INFO:root:Starting request #27\n",
      "INFO:root:Starting request #18\n",
      "INFO:root:Starting request #26\n",
      "INFO:root:Starting request #36\n",
      "INFO:root:Starting request #51\n",
      "INFO:root:Starting request #38\n",
      "INFO:root:Starting request #43\n",
      "INFO:root:Starting request #54\n",
      "WARNING:root:Request 91 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29776, Requested 622. Please try again in 796ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 37 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29775, Requested 543. Please try again in 636ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 27 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29772, Requested 464. Please try again in 471ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 18 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29770, Requested 471. Please try again in 482ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 26 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29768, Requested 540. Please try again in 616ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 36 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29767, Requested 537. Please try again in 608ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 58 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29768, Requested 643. Please try again in 822ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 10 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29769, Requested 568. Please try again in 674ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 51 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29763, Requested 561. Please try again in 648ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 38 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29765, Requested 542. Please try again in 614ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 54 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29759, Requested 515. Please try again in 548ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Request 43 failed with error {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-oFL2A0fkavp5zf7DKodvQoqA on tokens per min (TPM): Limit 30000, Used 29766, Requested 452. Please try again in 436ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}\n",
      "WARNING:root:Pausing to cool down until Thu Jan 23 15:57:08 2025\n",
      "INFO:root:Starting request #91\n",
      "WARNING:root:Pausing to cool down until Thu Jan 23 15:57:08 2025\n",
      "INFO:root:Starting request #37\n",
      "INFO:root:Starting request #27\n",
      "INFO:root:Starting request #18\n",
      "INFO:root:Starting request #26\n",
      "INFO:root:Starting request #36\n",
      "INFO:root:Starting request #58\n",
      "INFO:root:Starting request #10\n",
      "INFO:root:Starting request #51\n",
      "INFO:root:Starting request #38\n",
      "INFO:root:Starting request #54\n",
      "INFO:root:Starting request #43\n",
      "INFO:root:Parallel processing complete. Results saved to results/text2sql_evaluation.jsonl\n",
      "WARNING:root:85 rate limit errors received. Consider running at a lower rate.\n"
     ]
    }
   ],
   "source": [
    "# GPT-4 평가 수행\n",
    "!python api_request_parallel_processor.py \\\n",
    "--requests_filepath requests/{eval_filepath}  \\\n",
    "--save_filepath results/{eval_filepath} \\\n",
    "--request_url https://api.openai.com/v1/chat/completions \\\n",
    "--max_requests_per_minute 2500 \\\n",
    "--max_tokens_per_minute 100000 \\\n",
    "--token_encoding_name cl100k_base \\\n",
    "--max_attempts 5 \\\n",
    "--logging_level 20\n",
    "\n",
    "# !python api_request_parallel_processor.py \\                 # py파일 드라이브에 업로드 후 실행\n",
    "# --requests_filepath requests/{eval_filepath}  \\             # 평가를 위해 요청할 JSONL 파일의 경로를 지정  # 이전 코드에서 생성한 eval_filepath 변수를 사용해 경로를 지정\n",
    "# --save_filepath results/{eval_filepath} \\                   # OpenAI API 응답 결과를 저장할 파일 경로를 지정\n",
    "# --request_url https://api.openai.com/v1/chat/completions \\  # OpenAI API의 엔드포인트 URL\n",
    "# --max_requests_per_minute 2500 \\                            # 초당 최대 요청 수 지정  # 초당 2500개의 요청을 보낼 수 있음\n",
    "# --max_tokens_per_minute 100000 \\                            # 초당 사용할 수 있는 최대 토큰 수 지정\n",
    "# --token_encoding_name cl100k_base \\                         # OpenAI API에서 사용하는 토큰 인코딩 방식\n",
    "# --max_attempts 5 \\                                          # 각 요청에 대해 최대 재시도 횟수\n",
    "# --logging_level 20                                          # 로그 레벨 지정  # 10: DEBUG, 20: INFO, 30: WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GpK2_Jg25aL",
    "outputId": "f45b55d5-af81-44f3-f06f-a1951e03d478"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT의 평가 결과 JSONL을 불러와서 CSV로 변환 후 정확히 답변한 SQL 쿼리의 갯수 출력\n",
    "base_eval = change_jsonl_to_csv(f\"results/{eval_filepath}\", \"results/yi_ko_6b_eval.csv\", \"prompt\", \"resolve_yn\")\n",
    "base_eval['resolve_yn'] = base_eval['resolve_yn'].apply(lambda x: json.loads(x)['resolve_yn'])\n",
    "num_correct_answers = base_eval.query(\"resolve_yn == 'yes'\").shape[0]\n",
    "num_correct_answers  # 미세 조정 전 모델 평가 결과: 총 112개 프롬프트 중 정확한 답변 17개 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRhEwo6a28Z9"
   },
   "outputs": [],
   "source": [
    "# 학습 데이터 불러와서 csv로 저장\n",
    "from datasets import load_dataset\n",
    "\n",
    "df_sql = load_dataset(\"shangrilar/ko_text2sql\", \"origin\")[\"train\"]  # train 스플릿 데이터 불러옴\n",
    "df_sql = df_sql.to_pandas()  # 판다스 데이터프레임 형식으로 변환\n",
    "df_sql = df_sql.dropna().sample(frac=1, random_state=42)  # 결측치 제거하고, frac=1 모든 데이터를 100% 섞음, 랜덤시드 고정\n",
    "df_sql = df_sql.query(\"db_id != 1\")  # query()는 필터링하는 함수, 가독성은 간결하나 .loc보다 느림(큰 데이터에는 부적절)  # id가 1(게임 카테고리)인 걸 제외하고 가져옴(-> 평가 데이터로 사용)\n",
    "\n",
    "for idx, row in df_sql.iterrows():  # df_sql 데이터를 순회하면서 인덱스와 각 행을 가져옴\n",
    "    df_sql.loc[idx, 'text'] = make_prompt(row['context'], row['question'], row['answer'])  # 프롬프트 생성 함수에 각 DDL, 질문, 정답 쿼리를 입력으로 넣고 생성된 프롬프트를 'text'에 저장\n",
    "\n",
    "!mkdir data  # 새 폴더 생성\n",
    "df_sql.to_csv('data/train.csv', index=False)  # 제작한 데이터셋을 csv로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8Hl1VKN3KIh",
    "outputId": "1143581b-718f-4039-a709-e154fdd45f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:28\u001b[0m | \u001b[36mautotrain.cli.run_llm\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m343\u001b[0m - \u001b[1mRunning LLM\u001b[0m\n",
      "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2025-01-23 15:59:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m180\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: func, train, version, inference, config, deploy, backend\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100% 33876/33876 [00:00<00:00, 527797.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100% 33876/33876 [00:00<00:00, 537364.81 examples/s]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:30\u001b[0m | \u001b[36mautotrain.backend\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:30\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m327\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'yi-ko-6b-text2sql/training_params.json']\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:30\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1m{'model': 'beomi/Yi-Ko-6B', 'project_name': 'yi-ko-6b-text2sql', 'data_path': 'yi-ko-6b-text2sql/autotrain-data', 'train_split': 'train', 'valid_split': None, 'add_eos_token': False, 'block_size': 1024, 'model_max_length': 1024, 'padding': None, 'trainer': 'sft', 'use_flash_attention_2': False, 'log': 'none', 'disable_gradient_checkpointing': False, 'logging_steps': -1, 'evaluation_strategy': 'epoch', 'save_total_limit': 1, 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'lr': 0.0002, 'epochs': 1, 'batch_size': 8, 'warmup_ratio': 0.1, 'gradient_accumulation': 8, 'optimizer': 'adamw_torch', 'scheduler': 'linear', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'chat_template': None, 'quantization': 'int4', 'target_modules': 'all-linear', 'merge_adapter': False, 'peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'model_ref': None, 'dpo_beta': 0.1, 'max_prompt_length': 128, 'max_completion_length': None, 'prompt_text_column': 'autotrain_prompt', 'text_column': 'autotrain_text', 'rejected_text_column': 'autotrain_rejected_text', 'push_to_hub': False, 'username': None, 'token': None}\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mStarting SFT training...\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m311\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m352\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['db_id', 'context', 'question', 'answer', 'autotrain_text'],\n",
      "    num_rows: 33876\n",
      "})\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m353\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m423\u001b[0m - \u001b[1mconfiguring logging steps\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_logging_steps\u001b[0m:\u001b[36m436\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_training_args\u001b[0m:\u001b[36m441\u001b[0m - \u001b[1mconfiguring training args\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mconfigure_block_size\u001b[0m:\u001b[36m504\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 15:59:43\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100% 5/5 [00:18<00:00,  3.74s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:00:03\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:00:03\u001b[0m | \u001b[36mautotrain.trainers.clm.train_clm_sft\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "Generating train split: 9685 examples [00:25, 386.94 examples/s]\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:00:29\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m231\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
      " 17% 25/151 [10:55<55:00, 26.19s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:11:25\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.6552, 'grad_norm': 0.1893770843744278, 'learning_rate': 0.0001866666666666667, 'epoch': 0.16515276630883569}\u001b[0m\n",
      "{'loss': 0.6552, 'grad_norm': 0.1893770843744278, 'learning_rate': 0.0001866666666666667, 'epoch': 0.17}\n",
      " 33% 50/151 [21:50<44:04, 26.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:22:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.2259, 'grad_norm': 0.11112586408853531, 'learning_rate': 0.00014962962962962963, 'epoch': 0.33030553261767137}\u001b[0m\n",
      "{'loss': 0.2259, 'grad_norm': 0.11112586408853531, 'learning_rate': 0.00014962962962962963, 'epoch': 0.33}\n",
      " 50% 75/151 [32:44<33:09, 26.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:33:14\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.1931, 'grad_norm': 0.12217958271503448, 'learning_rate': 0.0001125925925925926, 'epoch': 0.495458298926507}\u001b[0m\n",
      "{'loss': 0.1931, 'grad_norm': 0.12217958271503448, 'learning_rate': 0.0001125925925925926, 'epoch': 0.5}\n",
      " 66% 100/151 [43:39<22:15, 26.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:44:08\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.1824, 'grad_norm': 0.10184281319379807, 'learning_rate': 7.555555555555556e-05, 'epoch': 0.6606110652353427}\u001b[0m\n",
      "{'loss': 0.1824, 'grad_norm': 0.10184281319379807, 'learning_rate': 7.555555555555556e-05, 'epoch': 0.66}\n",
      " 83% 125/151 [54:34<11:20, 26.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 16:55:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.1771, 'grad_norm': 0.0912252888083458, 'learning_rate': 3.851851851851852e-05, 'epoch': 0.8257638315441783}\u001b[0m\n",
      "{'loss': 0.1771, 'grad_norm': 0.0912252888083458, 'learning_rate': 3.851851851851852e-05, 'epoch': 0.83}\n",
      " 99% 150/151 [1:05:28<00:26, 26.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 17:05:57\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'loss': 0.1732, 'grad_norm': 0.09132850915193558, 'learning_rate': 1.4814814814814817e-06, 'epoch': 0.990916597853014}\u001b[0m\n",
      "{'loss': 0.1732, 'grad_norm': 0.09132850915193558, 'learning_rate': 1.4814814814814817e-06, 'epoch': 0.99}\n",
      "100% 151/151 [1:05:54<00:00, 26.18s/it]\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 17:06:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m226\u001b[0m - \u001b[1m{'train_runtime': 3954.7205, 'train_samples_per_second': 2.449, 'train_steps_per_second': 0.038, 'train_loss': 0.26720985099179856, 'epoch': 0.9975227085053675}\u001b[0m\n",
      "{'train_runtime': 3954.7205, 'train_samples_per_second': 2.449, 'train_steps_per_second': 0.038, 'train_loss': 0.26720985099179856, 'epoch': 1.0}\n",
      "100% 151/151 [1:05:54<00:00, 26.19s/it]\n",
      "\u001b[1mINFO    \u001b[0m | \u001b[32m2025-01-23 17:06:24\u001b[0m | \u001b[36mautotrain.trainers.clm.utils\u001b[0m:\u001b[36mpost_training_steps\u001b[0m:\u001b[36m263\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 미세 조정 명령어\n",
    "base_model = 'beomi/Yi-Ko-6B'  # 베이스 모델\n",
    "finetuned_model = 'yi-ko-6b-text2sql'  # 파인튜닝이 완료된 새 모델\n",
    "\n",
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model {base_model} \\\n",
    "--project-name {finetuned_model} \\\n",
    "--data-path data/ \\\n",
    "--text-column text \\\n",
    "--lr 2e-4 \\\n",
    "--batch-size 8 \\\n",
    "--epochs 1 \\\n",
    "--block-size 1024 \\\n",
    "--warmup-ratio 0.1 \\\n",
    "--lora-r 16 \\\n",
    "--lora-alpha 32 \\\n",
    "--lora-dropout 0.05 \\\n",
    "--weight-decay 0.01 \\\n",
    "--gradient-accumulation 8 \\\n",
    "--mixed-precision fp16 \\\n",
    "--use-peft \\\n",
    "--quantization int4 \\\n",
    "--trainer sft\n",
    "\n",
    "# !autotrain llm \\                       # AutoTrain CLI 명령어\n",
    "# --train \\                              # 모델을 학습 및 미세 조정 수행\n",
    "# --model {base_model} \\                 # 모델 베이스 경로\n",
    "# --project-name {finetuned_model} \\     # 모델 이름(위에서 변수 생성)\n",
    "# --data-path data/ \\                    # 학습 데이터가 저장된 경로(위에서 생성한 data 폴더)\n",
    "# --text-column text \\                   # 학습 데이터의 text 컬럼(생성한 프롬프트)\n",
    "# --lr 2e-4 \\                            # 학습률(Learning Rate), 작은 수치값\n",
    "# --batch-size 8 \\                       # 배치 사이즈 설정\n",
    "# --epochs 1 \\                           # 에포크 설정\n",
    "# --block-size 1024 \\                    # 입력 시퀀스의 최대 길이(토큰 단위)\n",
    "# --warmup-ratio 0.1 \\                   # 학습 초기에 학습률을 점진적으로 증가시키는 비율  # 0.1: 전체 학습 중 10% 동안 만 학습률을 서서히 증가 시킴\n",
    "# --lora-r 16 \\                          # 로라 설정  # 사용할 랭크, 적응 가능한 파라미터의 크기를 결정\n",
    "# --lora-alpha 32 \\                      # 로라 설정  # 로라의 스케일링 팩터\n",
    "# --lora-dropout 0.05 \\                  # 로라 설정  # 로라 드롭아웃 확률, 과적합 방지\n",
    "# --weight-decay 0.01 \\                  # 가중치 감쇠 비율, 과적합을 방지하지만 수치가 너무 크면 모델 성능에 영향을 줌\n",
    "# --gradient-accumulation 8 \\            # 기울기 누적 단계 설정  # 8: 8번의 배치 업데이트 후 기울기 업데이트\n",
    "# --mixed-precision fp16 \\               # 혼합 정밀도 학습(Mixed Precision)을 활성화  # 16비트 부동소수점(half-precision)으로 계산하여 GPU 메모리 사용량과 학습 속도를 개선\n",
    "# --use-peft \\                           # PEFT(경량화된 미세 조정) 사용\n",
    "# --quantization int4 \\                  # 모델 양자화 진행, 4비트 정수 기반으로 모델을 양자화\n",
    "# --trainer sft                          # 학습 방식 선택  # sft: 지도 학습 기반 미세 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK-gw7Pz6cXK"
   },
   "outputs": [],
   "source": [
    "# 허깅페이스 로그인\n",
    "from huggingface_hub import login\n",
    "\n",
    "# 허깅페이스 허브 로그인\n",
    "token = \"my_token\"  # 허깅페이스 액세스 토큰 입력\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336,
     "referenced_widgets": [
      "ff516600bcb444c8962a7943c7ac1ef2",
      "fe5ba802a69c48a0b1112c18a6a4f686",
      "2b979dec894a479ab440359ce201dce3",
      "5c213f3859ec43e5bd4d9c8045fab333",
      "97d2d3369c104772adf5a18b40107eaa",
      "52fef472d89b422491ccd3b784eb1582",
      "cc15580d1074478d81db563e75eb6439",
      "2f02dc9146d84920bf633b1d4824e848",
      "bb3509782c704d1bbdce00b9468d64ad",
      "bc47803b2b114a5eabbe952dfa217b87",
      "e10a7eccb7c248cd89960d3e43e5e48b",
      "0528e48a51a541f49e3633b7a98e40ba",
      "09c535bc8aae4e00b52b9b7af922079d",
      "034d532394394b448360d04b7f15e862",
      "6cf11a75c51e4e6ca317abe1eed7453d",
      "e382daf77f6e45fbb1367db246720c66",
      "34082833d2204b9eadc5edf7689f0db7",
      "46e27b7069104b3a9ffddf0970d37640",
      "02b0de2343804fada59d21dbc03287b0",
      "dd09c7e2dde846a381628831ee80f12f",
      "a7317b0f6dc544278d156552c35b7280",
      "77f37d5be54a4bc9aad1f490a911bdb1",
      "4c4a714626134a6bb516bbd428b658ff",
      "d08290eb7eda480eb411477c3d876e9f",
      "3104a40014144632a0b1aa20694df236",
      "9547bf232c8d498788e20f121260d0fa",
      "ae77179abcc54501bf093ffddef981d8",
      "90bd46d8d0c34ac3a2fb5ea8bb342552",
      "0e646783d9c944b883b917d0fdcd8c59",
      "b80dce88a27e49eb8ebbad8576d9aafc",
      "16044e70089f44e88483eca1c577721d",
      "de30b58fdad641e595195c596df37140",
      "130093b265584e5db735b51de08cafa8",
      "07aaba11522b426790ec765c471188dc",
      "979d4011b0fa47d9881b4eba71997b20",
      "3ee6e3b76a694a4cb43d2f702415a846",
      "78a8d379bfb04a67a5e485995856f7ca",
      "6c144e8a55d145ca82fba6687edd8ff8",
      "c46883aec7b44affa12c7361129daff6",
      "c1f943f4c6ef40e589e5c24d39fa4672",
      "f919c75717554c61978382c7ec9f5093",
      "18ba473462324cfca6c2182c34f8cde1",
      "e05d01fb37984c789a6f5c8bb0724bb3",
      "41e064340e3f400fa4f4784ad9de77af",
      "cd7062eb1f8044e0908eb2171e831638",
      "719a9030f19d4a66ab58a738668372e9",
      "1c4ec99f61d042e48b8aa11254ace58e",
      "c1d7cf9cbcda463f8d97e8718e2a6724",
      "8df649b4d7e84254a0e5a6ddb0417482",
      "3025c566f78448868f51a8c6052c54b6",
      "a7ed4f759bda43eaa7197e3887210ddd",
      "fc2c46f33539427ca5ca55d0d91ef114",
      "e9f1ac6e2a1b43838dedf10c0146d6fd",
      "50e01c8458e44654816ba0bd135a8c01",
      "1c97a920575a442faad64ed6660b5271",
      "a6ea24799ad14cc39e5ecb3ce7633a23",
      "48b688ae47354b1ab57f6384cb075a4d",
      "07f95d4d33f84c548a4ae787f0f18d0f",
      "260e87b64ffc4d80aeaf69a466d8c81f",
      "f27a6582bd1841f187bb0ce6c48c4e7a",
      "2ea9929a0ed14c81ac25ef82c9b463fa",
      "8558e0f167ea49a980c8a777ed1cb180",
      "8c44121ffc984faaa5d07dc20d50ca75",
      "099d718f7018451ca1dead62d3b2a896",
      "f7fdbe6fb6904551a623f1f20ab527a3",
      "d399d816df81440fa319d7fd1f3ef6da"
     ]
    },
    "id": "PHFkZArD35qk",
    "outputId": "0f9d586b-44b2-4531-a350-48af7bc75f12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff516600bcb444c8962a7943c7ac1ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0528e48a51a541f49e3633b7a98e40ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4a714626134a6bb516bbd428b658ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07aaba11522b426790ec765c471188dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7062eb1f8044e0908eb2171e831638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ea24799ad14cc39e5ecb3ce7633a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/edgeun/yi-ko-6b-text2sql/commit/28033e0cd79112c763cb8a031703a60f9de2e059', commit_message='Upload tokenizer', commit_description='', oid='28033e0cd79112c763cb8a031703a60f9de2e059', pr_url=None, repo_url=RepoUrl('https://huggingface.co/edgeun/yi-ko-6b-text2sql', endpoint='https://huggingface.co', repo_type='model', repo_id='edgeun/yi-ko-6b-text2sql'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA 어댑터 결합 및 허깅페이스 허브 업로드\n",
    "import torch\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "model_name = base_model  # 위에서 지정한 기본 모델\n",
    "device_map = {\"\": 0}  # 모델을 로드할 디바이스 설정  # 0: 모델의 모든 파라미터를 GPU 0번 디바이스에 로드\n",
    "\n",
    "# LoRA와 기초 모델 파라미터 합치기\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,      # CPU 메모리 사용을 최소화하여 모델을 로드\n",
    "    return_dict=True,            # 모델 출력을 dictionary 형식으로 반환\n",
    "    torch_dtype=torch.float16,   # 16비트 부동소수점 형식으로 로드(메모리 사용량 감소)\n",
    "    device_map=device_map,       # GPU 0번 디바이스에 모델을 로드\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, finetuned_model)  # PEFT를 통해 미세 조정된 LoRA 어댑터를 로드하여 기본 모델(base_model)에 결합\n",
    "model = model.merge_and_unload()   # LoRA 어댑터 파라미터를 기본 모델에 병합, 병합 후 로라 어댑터는 메모리에서 언(un)로드\n",
    "\n",
    "# 토크나이저 설정\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # eos 토큰으로 패딩 토큰 설정\n",
    "tokenizer.padding_side = \"right\"  # 입력 문장 패딩을 오른쪽에 추가\n",
    "\n",
    "# 허깅페이스 허브에 모델 및 토크나이저 저장\n",
    "model.push_to_hub(finetuned_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(finetuned_model, use_temp_dir=False)  # 모델 및 토크나이저 허깅페이스 허브에 업로드  # 임시 디렉토리 지정안하고 현재 경로에서 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478,
     "referenced_widgets": [
      "6b8c2e23cb284425bafd86afe7f187fb",
      "781f3a2c08af4bbd97e59a2adcaf4957",
      "80c07a4e38894bf59255727e68e680ef",
      "3a8bf3cd95b14cf3808864b798aa6269",
      "1f03b670668348358239edf8ef5bfb44",
      "b20a995bc68746fa8d6eb0e9fb3267f0",
      "bc5049b2ca5e439daff8635285660ff3",
      "3c0ebbca4daa4dcf91ca57baae6d2b56",
      "cd0d4fc48cc4426ca56a337da8a1cd27",
      "231ca04acd2e4cdcb1725a5bb6ae46f0",
      "d26bf987b3464f1f94c8d80a4d1e21b9",
      "25f18b56adc74e0d8e29516b060835f5",
      "77e47a26740d4083b2548e9e849f3d99",
      "8dde3068120143fcafabe0f889a0f716",
      "72d2fa16562840e4be2d816d00325d68",
      "2a27d079347a438e820d0170c8d26bd9",
      "bb3e079a17904bf897f9fc14decfa857",
      "0b40fc7e3c30402ab009bf7a16b4e84a",
      "013cf0bb908c41e7b2baa357e28ea926",
      "7117d811af5a404ab2416a64c74dcbb2",
      "1971b4814fd94af9be4631b5e1a3ed68",
      "790266f047e34ac0aedca95aca547aab",
      "082d5d24a8ac4e7990dd42057c09df38",
      "eb0916306fe949dea7adbefcfb1fd39d",
      "eb7e3d14f3cc48399934d1414cf2d5ce",
      "35e7fc283867465d8b8072cc6f42e41c",
      "196111bc0f0d455cbb9984c1b30527cd",
      "05c30f8d83f840728a22cfc8d99d7509",
      "4bbd4fdc485d4b4986e247cecd07970f",
      "86b7ffc75dd44a73a6fbd58ace12caa3",
      "2d87fc2086334850a710753ba253d0ce",
      "d5fe9ae64667416fad7ee3d398f092b6",
      "19e958b86fde46c89eb52b082122d5dc",
      "20110a30b0784587a567e874b798b492",
      "aeb4c9f32513450ca2429fc46ee638df",
      "1542a976decd4c689bd31c5403907a0e",
      "5b390af582be41de91cdb4c8a2d151da",
      "b773b2bc4aae46fdbfb9a2e519aa94f2",
      "fc30753a133c4a63a0bd961a187aa7ad",
      "10e8268d8615468eb6e7592b7a2dfa2b",
      "72a1d7350f5c4caea52a7c95f844742d",
      "7588c56842ea4463900a9272b6ec71c8",
      "f486634597f8421a9605ec213e5fba4e",
      "9bdf066606054ec5adcdda95be7311ec",
      "f399ff920b0f4fff919e7390faeb1bf6",
      "68fc2ae22663436eb38e763a4811d008",
      "3123c80603a244b4a6c1c0460597df60",
      "c5c415f40ff74bc295b8bb2f7c5322be",
      "e81e7dc2476e4a89bd1ecdd732be848d",
      "9f950700c6fe42e8991d8907fc48fd2c",
      "7d12022d63f54c6da9dfcc3192db0336",
      "48f65f39a8af4d0b9f1173c748648558",
      "c418b5bb2638405a8a9b6a94dd305fc7",
      "93dfd3554bdf4d8890b929351634ab1b",
      "c5c5ffc41cad4483b21d14d7032bb2d6",
      "b06a361ad70f4e86a50c406a511ec76d",
      "ef55e3bb504e4129a3110a5378009599",
      "0f7f6efebb634896b6654287309d7aba",
      "3c65fd9295b74471b87c53c6c646e187",
      "8c7b0988db0b4791b3773c9e1ba333f6",
      "cf7382273c634ecfa71f269405668c50",
      "31865e3026a344a3ba1f097c5b3a240c",
      "2f2ccf39fee244c8bff8e1c6a0ee46ab",
      "5c81265f222f4d3cb536ed262157cc05",
      "9bec142ac40f4a799edb04914b5654bf",
      "599254a711924c55bc155c0bb44af49e",
      "9ed0fa756c70467681b43f0c2e10550b",
      "238626dfe06e4c309ff74e8b77d18212",
      "41d760f13d1844cb95d7d14cba54df48",
      "92b1de3f6aaa4859915d0f7f109e80a4",
      "6da4da2fc7894ce5b877e2f11fd2a7d3",
      "ffd92eb9b72044199190825958225c30",
      "700f6a568c3740a8a9daa7497dbb51a5",
      "325f2b9730a644b285c6525a66530dbe",
      "f0a63b3d622447b5b59d4c1af0a39fe8",
      "ef66bb57adcd4684ac7c80fe7e631d26",
      "0ee3c7872a8c48dd9b20e570ba29e8d3",
      "d8a5a0312e394344be09d83aac364c94",
      "b0b8ca393bdc4aeba3e47cfcacbf4ed7",
      "0356dadcaf364b32a254753b7a6c072c",
      "3011998a1dca44c5a7f8ddd377280512",
      "cbe0c955d6054a428a706cba0be07b75",
      "5d05022cf0504fa9b7cd92da09f11869",
      "d6410728dc3742029f6587b37681a164",
      "76d9d4627b784185ab5c52fc9e20db55",
      "1758e29e86dc4c04aca01f1a3ed08263",
      "35e06291c4a54d8c8d48414fae65e4f5",
      "8b9afffecc5a4060ae631ec608bbc4e9",
      "19f3aa7822df483d97f847ec3567053b",
      "58106841723b49f7b870efba4f97cd3a",
      "18b836aaa9ac4b428e9e853ffc2c0a3b",
      "046bad9f3c89424ba760a2e17db3194c",
      "6b5204e1bb56462e9cd485eb649d5b3f",
      "167988734c2b4e7eb2fb07b8b17642f1",
      "fed6766633cd4e2d92af60c82df2d64d",
      "2865c52ec7d14ec19972a7e8b04ffdc3",
      "e98b719fe5494fa3b5929f7cc4105328",
      "247acb2a482d479f80b9241e3e5f876a",
      "69d05d7000f44d80a83cdaab1570caf1",
      "1df6319e950144199c034b7d622a404d",
      "1bbcfa490e384a3680005b3edfdabce3",
      "00c869ecd65b4c54af1dbbffa0138b24",
      "17209cf844e54c0c9ab9d75031e81817",
      "ce39e112aa6b4edc91b6c56541fc7bb3",
      "e7966cd4aa304050abce4a33b986d6af",
      "3f839c87354d47649a83eeddea9054db",
      "fab8ef4faf6e431db04ecbe98430fd9d",
      "1caeb2ce1fcf413fa3a56c5bc101b034",
      "8b5177900e034bc7b2702df28fb61d77",
      "cfa5899861bb4cf8b83b6ed4b68434c2",
      "b4057bec2e2e48798a4008fef27cd67c",
      "adf07177aa5e441198a023e2437b4be5",
      "3fc9cb32076940799835ff0dbcd78fb4",
      "e26f4269f7dd4e68916635b9aae26c08",
      "113a66ea36674151865276a45ff41f60",
      "482042cd396345c9a17111e52dc708db",
      "0e2205b3350845c99cf38767a6fcfe8b",
      "c6aee378386c45ffa8a97ee0eed10b78",
      "293d2b9e60644eeeb391dce6aa521a7f",
      "bef183d35c074f80a1cbc050abcb2561",
      "4c419cef582746cfa723940224c1b09e"
     ]
    },
    "id": "LAPNIZ7P4DRV",
    "outputId": "a828fd36-dcc5-4d6e-e143-094f80582e7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8c2e23cb284425bafd86afe7f187fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f18b56adc74e0d8e29516b060835f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082d5d24a8ac4e7990dd42057c09df38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20110a30b0784587a567e874b798b492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f399ff920b0f4fff919e7390faeb1bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06a361ad70f4e86a50c406a511ec76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed0fa756c70467681b43f0c2e10550b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a5a0312e394344be09d83aac364c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f3aa7822df483d97f847ec3567053b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df6319e950144199c034b7d622a404d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4057bec2e2e48798a4008fef27cd67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"SELECT COUNT(*) FROM players WHERE username LIKE '%admin%';\\n            \"}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 미세 조정한 모델로 예시 데이터에 대한 SQL 생성\n",
    "model_id = \"edgeun/yi-ko-6b-text2sql\"  # 허브에 업로드한 미세 조정 모델 경로\n",
    "hf_pipe = make_inference_pipeline(model_id)\n",
    "\n",
    "hf_pipe(example, do_sample=False,\n",
    "       return_full_text=False, max_length=1024, truncation=True)  # 기초 모델 평가에 사용된 example를 파이프라인에 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPvNulxh4wx7",
    "outputId": "00c7f70f-4e27-434e-dcec-f4640938ca1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting request #0\n",
      "INFO:root:Starting request #1\n",
      "INFO:root:Starting request #2\n",
      "INFO:root:Starting request #3\n",
      "INFO:root:Starting request #4\n",
      "INFO:root:Starting request #5\n",
      "INFO:root:Starting request #6\n",
      "INFO:root:Starting request #7\n",
      "INFO:root:Starting request #8\n",
      "INFO:root:Starting request #9\n",
      "INFO:root:Starting request #10\n",
      "INFO:root:Starting request #11\n",
      "INFO:root:Starting request #12\n",
      "INFO:root:Starting request #13\n",
      "INFO:root:Starting request #14\n",
      "INFO:root:Starting request #15\n",
      "INFO:root:Starting request #16\n",
      "INFO:root:Starting request #17\n",
      "INFO:root:Starting request #18\n",
      "INFO:root:Starting request #19\n",
      "INFO:root:Starting request #20\n",
      "INFO:root:Starting request #21\n",
      "INFO:root:Starting request #22\n",
      "INFO:root:Starting request #23\n",
      "INFO:root:Starting request #24\n",
      "INFO:root:Starting request #25\n",
      "INFO:root:Starting request #26\n",
      "INFO:root:Starting request #27\n",
      "INFO:root:Starting request #28\n",
      "INFO:root:Starting request #29\n",
      "INFO:root:Starting request #30\n",
      "INFO:root:Starting request #31\n",
      "INFO:root:Starting request #32\n",
      "INFO:root:Starting request #33\n",
      "INFO:root:Starting request #34\n",
      "INFO:root:Starting request #35\n",
      "INFO:root:Starting request #36\n",
      "INFO:root:Starting request #37\n",
      "INFO:root:Starting request #38\n",
      "INFO:root:Starting request #39\n",
      "INFO:root:Starting request #40\n",
      "INFO:root:Starting request #41\n",
      "INFO:root:Starting request #42\n",
      "INFO:root:Starting request #43\n",
      "INFO:root:Starting request #44\n",
      "INFO:root:Starting request #45\n",
      "INFO:root:Starting request #46\n",
      "INFO:root:Starting request #47\n",
      "INFO:root:Starting request #48\n",
      "INFO:root:Starting request #49\n",
      "INFO:root:Starting request #50\n",
      "INFO:root:Starting request #51\n",
      "INFO:root:Starting request #52\n",
      "INFO:root:Starting request #53\n",
      "INFO:root:Starting request #54\n",
      "INFO:root:Starting request #55\n",
      "INFO:root:Starting request #56\n",
      "INFO:root:Starting request #57\n",
      "INFO:root:Starting request #58\n",
      "INFO:root:Starting request #59\n",
      "INFO:root:Starting request #60\n",
      "INFO:root:Starting request #61\n",
      "INFO:root:Starting request #62\n",
      "INFO:root:Starting request #63\n",
      "INFO:root:Starting request #64\n",
      "INFO:root:Starting request #65\n",
      "INFO:root:Starting request #66\n",
      "INFO:root:Starting request #67\n",
      "INFO:root:Starting request #68\n",
      "INFO:root:Starting request #69\n",
      "INFO:root:Starting request #70\n",
      "INFO:root:Starting request #71\n",
      "INFO:root:Starting request #72\n",
      "INFO:root:Starting request #73\n",
      "INFO:root:Starting request #74\n",
      "INFO:root:Starting request #75\n",
      "INFO:root:Starting request #76\n",
      "INFO:root:Starting request #77\n",
      "INFO:root:Starting request #78\n",
      "INFO:root:Starting request #79\n",
      "INFO:root:Starting request #80\n",
      "INFO:root:Starting request #81\n",
      "INFO:root:Starting request #82\n",
      "INFO:root:Starting request #83\n",
      "INFO:root:Starting request #84\n",
      "INFO:root:Starting request #85\n",
      "INFO:root:Starting request #86\n",
      "INFO:root:Starting request #87\n",
      "INFO:root:Starting request #88\n",
      "INFO:root:Starting request #89\n",
      "INFO:root:Starting request #90\n",
      "INFO:root:Starting request #91\n",
      "INFO:root:Starting request #92\n",
      "INFO:root:Starting request #93\n",
      "INFO:root:Starting request #94\n",
      "INFO:root:Starting request #95\n",
      "INFO:root:Starting request #96\n",
      "INFO:root:Starting request #97\n",
      "INFO:root:Starting request #98\n",
      "INFO:root:Starting request #99\n",
      "INFO:root:Starting request #100\n",
      "INFO:root:Starting request #101\n",
      "INFO:root:Starting request #102\n",
      "INFO:root:Starting request #103\n",
      "INFO:root:Starting request #104\n",
      "INFO:root:Starting request #105\n",
      "INFO:root:Starting request #106\n",
      "INFO:root:Starting request #107\n",
      "INFO:root:Starting request #108\n",
      "INFO:root:Starting request #109\n",
      "INFO:root:Starting request #110\n",
      "INFO:root:Starting request #111\n",
      "INFO:root:Parallel processing complete. Results saved to results/text2sql_evaluation_finetuned.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 미세 조정한 모델 성능 측정\n",
    "# sql 생성 수행 (기초 모델 평가와 동일)\n",
    "gen_sqls = hf_pipe(df['prompt'].tolist(), do_sample=False,  # 평가용 test 데이터\n",
    "                   return_full_text=False, max_length=1024, truncation=True)\n",
    "gen_sqls = [x[0]['generated_text'] for x in gen_sqls]\n",
    "df['gen_sql'] = gen_sqls\n",
    "\n",
    "# 미세 조정 모델 평가를 위한 requests.jsonl 생성\n",
    "ft_eval_filepath = \"text2sql_evaluation_finetuned.jsonl\"  # 미세 조정 모델용 요청 jsonl 경로 설정\n",
    "make_requests_for_gpt_evaluation(df, ft_eval_filepath)\n",
    "\n",
    "# GPT-4 평가 수행(기초 모델 평가와 동일)  # requests, results 폴더 내 jsonl 파일 미세 조정 모델용 경로로 변경(ft_eval_filepath)\n",
    "!python api_request_parallel_processor.py \\\n",
    "--requests_filepath requests/{ft_eval_filepath} \\\n",
    "--save_filepath results/{ft_eval_filepath} \\\n",
    "--request_url https://api.openai.com/v1/chat/completions \\\n",
    "--max_requests_per_minute 2500 \\\n",
    "--max_tokens_per_minute 100000 \\\n",
    "--token_encoding_name cl100k_base \\\n",
    "--max_attempts 5 \\\n",
    "--logging_level 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCcJGX1d45GP",
    "outputId": "3407a2ff-8069-4652-ef50-86603ec64495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_eval = change_jsonl_to_csv(f\"results/{ft_eval_filepath}\", \"results/yi_ko_6b_eval.csv\", \"prompt\", \"resolve_yn\")\n",
    "ft_eval['resolve_yn'] = ft_eval['resolve_yn'].apply(lambda x: json.loads(x)['resolve_yn'])\n",
    "num_correct_answers = ft_eval.query(\"resolve_yn == 'yes'\").shape[0]\n",
    "num_correct_answers  # test 데이터 총 112개 중 79개의 쿼리 해결  # 미세 조정 전 15.17% -> 미세 조정 후 모델 테스트 70.53%"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
