{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi8DWUTQcKQC"
   },
   "source": [
    "### PEFT(Parameter-Efficient Fine-Tuning) 란?\n",
    "PEFT(Parameter-Efficient Fine-Tuning)는 대형 언어 모델의 전체 파라미터를 업데이트하지 않고, 일부 선택된 파라미터만 조정함으로써 Fine Tuning의 효율성을 극대화하기 위한 미세조정(Fine-Tuning) 방법론을 의미\n",
    "\n",
    "- LoRA(Low-Rank Adaptation): 적은 수의 파라미터만을 조정해 모델을 효과적으로 튜닝하는 기법\n",
    "- QLoRA(Quantized LoRA): LoRA와 함께, 모델을 양자화해 메모리 사용량을 크게 줄이면서도 성능 저하를 최소화하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efbUK7dtfaCj"
   },
   "source": [
    "### 1. Gemma-2-9B-it 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XScIDxK5gDcQ",
    "outputId": "a6ae72d1-7fc0-41b5-b752-dda4ec5ef21d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.44.2 \\\n",
    "            datasets==2.18.0 \\\n",
    "            accelerate==0.29.3 \\\n",
    "            evaluate==0.4.1 \\\n",
    "            bitsandbytes==0.43.1 \\\n",
    "            huggingface_hub>=0.23.2 \\\n",
    "            trl==0.8.6 \\\n",
    "            peft==0.10.0 \\\n",
    "            scikit-learn \\\n",
    "            wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tv8uXnC0bkjT"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"hf****\",\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560,
     "referenced_widgets": [
      "17e10fb94bfb4c868d016efa8adc68d6",
      "bfaeb0da5ffa4fc79cfb39784f239a6f",
      "5438da411c4945eca936b15d7a4e871e",
      "c4edd5c75e6f4a73ba44e23fdc363de4",
      "baf8c6d72c5143f4a90e91637f0815a5",
      "f39a50b3b36841eeb8320896c772e9da",
      "d7890878e8224ddc9a8b16c76e7558dd",
      "afe2ccb9186d41e0bcbc9c3d447b8fdf",
      "4b131bfcd1fd42d89d7223402df12b40",
      "0340181d34f548d78ea89e375f0b70c6",
      "5e4f8ff404d9430bae9636a1c0eea522",
      "e84989ea6cf040a6b30c4737b61bb52a",
      "c967c29a3000466eb19d34f2907251a9",
      "d79e9285a09b476c9574235bfda44b99",
      "01eeb95b10fa4a089d856644cd0b6aba",
      "edac6aa487da46e4886671e9c2882ee2",
      "f3ed3268f6874b399104a00e7ba1e0d7",
      "2d9a11ce5d05481aa53995b61f6cccdd",
      "6eb21b3a756148da929a24bde25b246a",
      "a57ff9691a7547489121b03bf04241a8",
      "a31236e3fe52442391a9f536c1ef3fc4",
      "72032c8b9b3b4ff588cbbc19af81bb16",
      "bd0f9cdc5f4d459caf494e136a28d129",
      "669243ff7ba04871b389d4b9ecdcf5b0",
      "8fb7988846ca403f89bc29d2af74bec4",
      "629b4c1133cb41e2bb654572d2ff4d38",
      "1bad96bb695741339359f2e603a967a9",
      "66fc0fe2623e49f09adda44b385b95b5",
      "3994396698ec4e8d84dee215ebeb31a6",
      "56803ce5b0b94253864f3c5145bda5ae",
      "e6690bc1e0164952b5d65f50ce37d95c",
      "74cbe558d64444ad9d1edb802c8c5913",
      "ec109d8f00094f02a5f12358b9b70b0b",
      "c7cd1c070e894dc28b8def4703e9c565",
      "8f3b6b0be2204646a746196d2fd0c720",
      "70512cfd81da43ddb6fcc28d3c549989",
      "6253daa4698a47bba47c004cfa88520c",
      "0212c3d4317040c2938a940965128f2b",
      "fe2e6e95357d449eab85f95b56d3d6a7",
      "a217d6ba08b94713ab2885824169ed55",
      "b9c9771021664f4f9cd50e2d566516e6",
      "b1507af14560418097a2c22ff25a6a8e",
      "abc9f89cbd984bd9a68905e1c8edeeac",
      "5f65ada5ba2d4530bda147ee057c7723",
      "e4b2dfbdb9d5477a91acec500ccea8ee",
      "78feb59bdc034bee9aa736d1837f75e3",
      "74f7a68574cb47b892f6e7cde0d3267e",
      "c0c4a31c900544b599743a18fdc1ea39",
      "dc6d7beac7e0405c92d8d0e793d5f5b9",
      "c9e0b13232e246109ac27e3001071259",
      "cbf25d28ddd646d5805acaff3d65584a",
      "bd45afb4cad547f2bd12a788b5f1e56e",
      "c4febde7430946feb638618c955748c1",
      "fd682282c076466c84ed98e12c037f76",
      "0b2047b99752495e887d6e8dd569bb7b",
      "23e7e0d18d9f49c0bb5952c13004d05f",
      "eee56245195b4051858cb4e8164eea18",
      "fb182b1bf60540baa9486c9100a2d887",
      "5aa6d6196f6e4aae949b51bb687a3a7c",
      "c7023afd3e8145848a6461eb37a7ae72",
      "5744755d679d4ce89ce4156bf97911aa",
      "87b1cd03e1794e01ba9c5cc1ad4a156f",
      "496fd6b51d9e4f7bb4ea3b0a481c4132",
      "6ad469f14c824013857144c964c46876",
      "9d0d0d2f56f240bdaa725ebba43c0dfe",
      "06acd3e5dc794defbbf8a68e1a9e9990",
      "e953692a8c7b4d568eb1874f1730fc64",
      "495c7b1bd9264bcaa3ffed8bbc405228",
      "feea96a268dd4056ac5b902b12b10021",
      "47a9a9a9f28c41f1919a4ee4ba856360",
      "0cde0255de194294b125c182b2472fc2",
      "1aa53bd9ef4b45859fda7efbaf0af889",
      "681d3733efb04bca8174da5de0d398ff",
      "4d84ee008af7432f9531d35c4712871e",
      "bb693b46f1d94620a5dcad111e25a0bb",
      "07c62debf04f4423a1b642f8eb1c6f0b",
      "b33ee046e669410f81b0250e59b3cb93",
      "e26e9eced1e9459391a48bccd66553ec",
      "0c9bb2b7287940318266244b9312fabb",
      "f5f9d72e63d144738d048865cb85ba97",
      "00cbab74151c4a67b9bb2d76decbc178",
      "b7f07cd166a84806830814c0efb2d75a",
      "7de59efe74e641e58b9ad72f8b266179",
      "fb6603ae032049bb8c9e37beabcfc73b",
      "7d7a331c8b2b4548bed72894c1f92655",
      "791c7390f2df48f3a58bfe10fc8734b4",
      "a9a36757b99e49c49c827a7e09978d85",
      "e74cef50d783420c8eb87a5c88f19af0",
      "c51c9f4ee2084ecd89907ea3b1acaef8",
      "3f1b2292d1b84cea8b3cadb3ee15e1ff",
      "fc4f182c7ad0413bad0e47beed572c3e",
      "15fc955679f145c9bc07af6a65169062",
      "901c3aef8dd6412299df00580a828853",
      "497fa2175a1941c18ea4d772917c673d",
      "b173a50d22ee48119b047c26f82bdaa1",
      "125dfd83607d4b30af1f41fdfde4126e",
      "9d702dfc8b8441af8339011c465c7ce5",
      "d3229e7b00a44454b0184b0778bd557a",
      "bdb036daf248424a934cb60d24c90b04",
      "87e8453902174493a000bf779d4db5fb",
      "1d4a95d0d65843f49265b270ba993557",
      "c19832cd1ded469ab6a26d64e775ff34",
      "4720c3d231e644959fbd1d510d27f837",
      "8b35fa3917574fc39e20b4eec145b926",
      "f831d9f1e2954a8b84aab03330e5249b",
      "70141d4f6d3d41e1969abda2e6b31ddd",
      "1d261d8938f24abaa219e0566cff97ab",
      "f10d1ba861c34ce899bf4ad633142946",
      "e50cb941c7a242e29adeb1497405a464",
      "a930826fc5db46099700b5d02af947ad",
      "30c86df94cf14a2bb78812addf88125b",
      "9b5bf284491748589d54201a903a5af0",
      "834f81e2e45a49889183ecfb9550f7ec",
      "aeda85c45d234ca58c39247ab1434dcd",
      "d1280e640a7a4106a728a3937449d5f8",
      "f098ef500dad47e3a3461a0ca0c73b86",
      "c837572f6f45435c8b60a1d97aa09bfe",
      "9296a76059aa4a5aabcea93a8a1482d5",
      "bab6ddce92f449328ec6b00bc3328fac",
      "d34e28c516164cfbb1990ae1bea3caca",
      "e6bf13843c31481fbd89f70f39833b55",
      "34e9e79ca98949ea893a7d7d316c27ad",
      "51aa7db94aac46ddbc42f13fbaaa81fd",
      "ee84f445a9db433e8c96a3c87faf0374",
      "c67f655de77e44e0adf2e5c1dca4b761",
      "fc6a9a2d83924a7f972f5554d137a97c",
      "b458214f54524f02bf46e8f55acb0508",
      "d35bf1c43b5a414e81f89092c69d1cc0",
      "5437ce7ca86045a7a3f14adafb8ac25d",
      "6c11580c60d7496dad3518352fef9b42",
      "f5451c8d95e248efb87b48f5c2f73ea2",
      "8765d09152144e4e9ce10920b1016278",
      "76221269d4744335853420dd67f48b3e",
      "579ae33c88ba4c3aa50db3d6a1f1ccc7",
      "afa962301d5d436e8d1ad531d0c27aab",
      "939b55b116584d85b3624cd1b0021fef",
      "6718d33d61d346e4a2b22d96fb4502f5",
      "73396164967c4167b2cd827b264515c0",
      "5bcd1dbfd2124a4886c646f47a3d09ea",
      "e83d7b63f7364aaba295309c3ce3d567",
      "8b00a42bf75846c78b7b69b39aa1b600",
      "57da4bb04c9547c48c6ed1a912d341d2",
      "86c2ee81972047bcafa90dacc145614d"
     ]
    },
    "id": "UtF0NdM_fyo9",
    "outputId": "bc18e15f-f0b6-4193-b09d-2f7578dbb7c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e10fb94bfb4c868d016efa8adc68d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84989ea6cf040a6b30c4737b61bb52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0f9cdc5f4d459caf494e136a28d129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cd1c070e894dc28b8def4703e9c565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b2dfbdb9d5477a91acec500ccea8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e7e0d18d9f49c0bb5952c13004d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e953692a8c7b4d568eb1874f1730fc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26e9eced1e9459391a48bccd66553ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51c9f4ee2084ecd89907ea3b1acaef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e8453902174493a000bf779d4db5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c86df94cf14a2bb78812addf88125b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e9e79ca98949ea893a7d7d316c27ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76221269d4744335853420dd67f48b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import (setup_chat_format,\n",
    "                 DataCollatorForCompletionOnlyLM,\n",
    "                 SFTTrainer)\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, PeftConfig\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          TrainingArguments,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline,\n",
    "                          StoppingCriteria)\n",
    "\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='eager'\n",
    "    # load_in_8bit=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbiTCd3ngbrs"
   },
   "source": [
    "### 2. 데이터 전처리\n",
    "\n",
    "- wget: 웹에서 파일을 다운로드하게 도와주는 라이브러리, !wget 명령어를 통해 데이터셋을 노트북 환경에서 실행하여 데이터셋을 다운로드\n",
    "- 파이썬의 리스트 컴프리헨션을 사용해서 JSONL 파일의 각 줄을 한 줄씩 읽어 json.loads()함수를 적용\n",
    "- json.loads(): JSON 형식으로 변환하는 함수, JSON 형식은 작성된 문자열을 python에서 사용할 수 있는 dictionary 타입으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-uBnTBqgfR9",
    "outputId": "274b3411-d0d5-46e8-b901-6c925b4a32fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-14 09:39:33--  https://raw.githubusercontent.com/MrBananaHuman/CounselGPT/main/total_kor_multiturn_counsel_bot.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30560672 (29M) [application/octet-stream]\n",
      "Saving to: ‘total_kor_multiturn_counsel_bot.jsonl’\n",
      "\n",
      "total_kor_multiturn 100%[===================>]  29.14M   190MB/s    in 0.2s    \n",
      "\n",
      "2025-02-14 09:39:34 (190 MB/s) - ‘total_kor_multiturn_counsel_bot.jsonl’ saved [30560672/30560672]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/MrBananaHuman/CounselGPT/main/total_kor_multiturn_counsel_bot.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3VT01PNuiIt2"
   },
   "outputs": [],
   "source": [
    "with open('./total_kor_multiturn_counsel_bot.jsonl', 'r', encoding='utf-8') as file:\n",
    "    original_jsonl_data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6Q1Q1jbiM2Q",
    "outputId": "4e598cd7-cb51-4ebf-baca-96c60cbe338d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': '상담사', 'utterance': '안녕하세요. 심리상담사입니다. 어떤 고민이 있으신가요?'},\n",
       " {'speaker': '내담자', 'utterance': '요즘 직장에서 너무 힘들어요.'},\n",
       " {'speaker': '상담사', 'utterance': '정말요? 어떤 점이 힘드신가요? 좀 더 자세히 말해주세요.'},\n",
       " {'speaker': '내담자',\n",
       "  'utterance': '친한 동료도 없고 일이 너무 많고 고객이나 동료에게 매일 반응하고 대처해야하니까 점점 지쳐 가네요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '그러셨군요. 직장생활에서 하나하나 대응하는 일은 많은 에너지를 필요로 합니다. 그리고 이러한 에너지 소모는 급격히 힘들어지게 합니다. 이러한 일상에 적응하며 시간이 지나면 점점 힘들어질 수 있어요.'},\n",
       " {'speaker': '내담자', 'utterance': '집에 가면 집안일을 하고 나면 무언가를 해야하는데 그게 너무 힘들어요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '집에서도 일을 하시는군요. 그러시다보니 집에서의 일도 의무적으로 느껴지는 거 같아요. 이러한 의무감에 의해서 불안감과 힘들어질 수 있죠.'},\n",
       " {'speaker': '내담자', 'utterance': '이러다 몸이 아플 것 같아요. 이게 계속되면 어떻게 해야할까요?'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '몸이 힘들어지는 건 자신이 지니고 있는 신호입니다. 즉, 몸과 마음에 신호를 주고 있는 거죠. 혹시 이러한 증상이 지속되시면 주변의 내용을 통해 주변의 상황을 살펴보고, 다양한 자신의 취미를 발견하거나, 휴식을 통해서 쉬는 것도 좋습니다. 만약에 몸에 이상을 느끼신다면 병원에 찾아가셔서 다양한 건강상의 문제를 예방할 수 있도록 조치하세요.'},\n",
       " {'speaker': '상담사', 'utterance': '내담자님, 어떤 생각이 드시나요?'},\n",
       " {'speaker': '내담자', 'utterance': '생각을 잘 못해서요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '그러시면, 우선 이러한 일상에 대해서 고민해보세요. 머리를 비우고 쉬어도 좋고, 진지하게 자신의 일상을 돌아보면서 어떻게 하면 이러한 고민을 줄일 수 있는지 생각해보세요.'},\n",
       " {'speaker': '상담사', 'utterance': '어떤 생각을 하셨나요?'},\n",
       " {'speaker': '내담자', 'utterance': '가족이랑 시간을 보내면서 즐겁게 생활해야겠다는 생각이 들었어요.'},\n",
       " {'speaker': '상담사',\n",
       "  'utterance': '그렇군요. 가족이나 친구와의 소통은 그만큼의 만족감과 편안함을 가져다줄 수 있죠. 다양한 시간과 경험을 나누면서 그 사람들과 더 가까워질 수 있을 거 같아요.'},\n",
       " {'speaker': '상담사', 'utterance': '더 말씀하실 내용이 있으신가요?'},\n",
       " {'speaker': '내담자', 'utterance': '없어요. 감사합니다.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_jsonl_data[5085]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wZ9ecn3k-2m"
   },
   "source": [
    "### 데이터 구조 분석\n",
    "- 각각의 대화 턴은 딕셔너리 형태로 구성, 'speaker'와 'utterance'라는 두 개의 주요 키 포함\n",
    "- speaker : 대화에 참여하는 사람을 의미 ('상담사' or '내담자'로 구분)\n",
    "- utterance : 해당 화자가 실제로 말한 내용\n",
    "\n",
    "** 모델을 학습시키기 위하여, 대화 데이터를 일관된 형식으로 변환하는 처리 과정이 필요.\n",
    "1. '내담자'와 '상담사'를 각각 'user'와 'assistant'로 변환\n",
    "2. 대화 흐름을 일관되게 user -> assistant 순으로 정리 필요 <br>\n",
    "('assistant'의 발화로 대화가 시작되는 경우, 해당 첫 메시지를 삭제해 'user'가 먼저 대화를 시작하도록 구성, 반대의 경우도 마찬가지로 'user' 메시지 삭제)\n",
    "3. 연속된 동일한 'assistant'의 메세지가 나올 경우 이를 하나로 병합\n",
    "- user -> assistant -> user -> assistant 순으로 데이터를 구성 <br>\n",
    "ex) 기존 : user -> assistant -> assistant -> user -> user -> assistant -> user - > assistant <br>\n",
    "처리 후 : user -> assistant(assistant + assistant) -> user(user + user) -> assistant -> user -> assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2t38uZewktwi"
   },
   "outputs": [],
   "source": [
    "speaker_dict = {'내담자': 'user', '상담사': 'assistant'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9gyF3qRuoKUE"
   },
   "outputs": [],
   "source": [
    "# 'user'와 'assistant'로 변환 및 각 해당 메시지 제거\n",
    "def preprocess_conversation(messages):\n",
    "    # speaker를 role로 변환\n",
    "    converted_messages = [{'role': speaker_dict[m['speaker']], 'content': m['utterance']} for m in messages]\n",
    "\n",
    "    # assistant로 시작하는 경우 첫 메시지 제거\n",
    "    if converted_messages and converted_messages[0]['role'] == 'assistant':\n",
    "        converted_messages = converted_messages[1:]\n",
    "\n",
    "    # user로 끝나는 경우 마지막 메시지들 제거\n",
    "    while converted_messages and converted_messages[-1]['role'] == 'user':\n",
    "        converted_messages = converted_messages[:-1]\n",
    "\n",
    "    # 연속된 동일 역할의 메시지 병합\n",
    "    converted_messages = merge_consecutive_messages(converted_messages)\n",
    "\n",
    "    # 대화가 비어있거나 홀수 개의 메시지만 남은 경우 처리\n",
    "    if not converted_messages or len(converted_messages) % 2 != 0:\n",
    "        return []\n",
    "\n",
    "    return converted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "96bV_rLGox7b"
   },
   "outputs": [],
   "source": [
    "# 연속된 역할 메시지 병합 함수\n",
    "def merge_consecutive_messages(messages):\n",
    "    if not messages:\n",
    "        return []\n",
    "\n",
    "    merged = []\n",
    "    current_role = messages[0]['role']\n",
    "    current_content = messages[0]['content']\n",
    "\n",
    "    for message in messages[1:]:\n",
    "        if message['role'] == current_role:\n",
    "            current_content += \" \" + message['content']\n",
    "        else:\n",
    "            merged.append({'role': current_role, 'content': current_content})\n",
    "            current_role = message['role']\n",
    "            current_content = message['content']\n",
    "\n",
    "    merged.append({'role': current_role, 'content': current_content})\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mqrPmw5KqskN"
   },
   "outputs": [],
   "source": [
    "# 변환된 데이터 반환 함수\n",
    "def transform_to_new_format(original_data):\n",
    "    transformed_data = []\n",
    "    for conversation in original_data:\n",
    "        processed_conversation = preprocess_conversation(conversation)\n",
    "        if processed_conversation:\n",
    "            transformed_data.append(processed_conversation)\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bD3I03ordz9",
    "outputId": "5496295c-4128-402b-bec2-69fb76203702"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '내가 약간 중2병 같은 걸 증상을 보이고 있어요.'},\n",
       " {'role': 'assistant', 'content': '중2병 증상이라니, 어떤 증상이신 건가요?'},\n",
       " {'role': 'user',\n",
       "  'content': '그러니까 공부하기 싫어하고, 공격적이고, 좀 무례하게 말하고 싶은 게 많아져서 그런 거예요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그런 증상이 있으니까 힘드시겠죠. 중2병 같은 것이라고 생각하시는 이유는 무엇인가요?'},\n",
       " {'role': 'user', 'content': '막 공부 안하고 이것저것 들먹이고 하고 싶은 게 너무 많아서 그런 거 같아요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그런 것도 어쩔 수 없이 찾아오는 시기가 있으니까 무리하지 않도록 해야겠죠. 대학교를 가면서 나아질 것 같았는데, 오히려 더 심해진 것 같다고 하셨죠. 그 원인이 무엇인가요?'},\n",
       " {'role': 'user', 'content': '그걸 제가 잘 몰라서 그런 것 같아요. 그냥 더 심해졌다고 느꼈어요.'},\n",
       " {'role': 'assistant', 'content': '대학교 생활이 신나고 재밌으신 건 어떤 점이 있나요?'},\n",
       " {'role': 'user',\n",
       "  'content': '학과가 정말 좋아서 즐겁게 수업을 듣고 있어요. 학우들도 좋고 괜찮은 친구들도 많이 만나서 그런 것 같아요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '즐거운 일도 많이 있으면서 고민거리도 있는 것 같군요. 가사나 소설을 쓰시면서 마음을 풀기도 하신다고 하셨는데, 언제부터 그 습관이 생겨난 건가요?'},\n",
       " {'role': 'user',\n",
       "  'content': '좋은 질문이에요. 좀 자세히 말씀드릴게요. 학교에서 어려운 일이 있었는데, 그 때부터 가사나 소설 같은 것들을 쓰면서 마음을 풀게 되었어요. 그리고 이런 걸 쓰면서 나름 살아갈 때도 있는 것 같고, 그러다 보니까 늘어나는 것 같아요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '어려운 일이 있으셨군요. 그 때부터 쓰시면서 나아지는 기분이 드셨다고 하셨는데, 현재에도 그런 기분이 드시나요?'},\n",
       " {'role': 'user',\n",
       "  'content': '좀 그래요. 이제는 어느 정도 극복했다고 생각은 하지만, 가사나 소설을 쓰면서 마음이 편안해지는 느낌이 있어서 써왔어요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '가사나 소설 같은 것들이 나쁜 것은 아니라고 하셨죠. 하지만, 과도한 습관으로 만들어지면 나중에 문제가 생길 수도 있습니다. 이러한 습관을 줄이고, 보다 효율적으로 자아확인에 도움이 될 수 있도록 일정한 패턴과 기록 방식으로 습관화하는 것이 좋습니다. 예를 들어, 가사나 소설을 쓸 때 일주일에 몇 번, 어느 시간대에 쓰는 것이 효과적인지에 대해 스스로에게 질문해보고, 그에 따른 습관을 만들어보는 것도 좋은 방법입니다.'},\n",
       " {'role': 'user', 'content': '그럼, 가사나 소설 같은 것들을 과도하게 하면 나중에 문제가 생긴다는 건가요?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그렇습니다. 이러한 습관이 과도하게 되면 자신에게 과부하를 주고 심리적인 문제를 일으킬 수 있습니다. 때로는, 삶의 태도를 바꾸는 것도 중요합니다. 스스로를 사랑하고 용서하며 자기자신에게 긍정적인 자아이미지를 가지는 것도 중요합니다. 이러한 자아이미지가 긍정적일수록 스트레스를 받아도 잘 해결할 수 있고, 자기자신을 존중하고 이해하는 능력이 생깁니다.'},\n",
       " {'role': 'user', 'content': '그래도 좀 걱정이 되네요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '걱정은 당연한 것이죠. 그러나 지나치게 걱정하는 것은 오히려 스트레스를 더 받게 됩니다. 지금 이 곳에 오셔서 이야기를 나누어 봤을 때, 내담자님께서 가진 문제가 그렇게 심각한 것은 아닌 것 같습니다. 하지만, 무엇보다도, 내담자님이 존중하고 사랑하는 마음으로 자신을 바라보며, 나름의 방식으로 살아가는 것이 중요합니다. 무언가를 끼워 맞추어 주려 하지 마시고, 스스로 찾아낼 수 있는 길을 찾아보세요.'},\n",
       " {'role': 'user', 'content': '그 말씀대로 할게요.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '그렇게 하셔서 조금이나마 좋아지시길 바라겠습니다. 이후에도 힘든 마음이 계속되면 언제든지 저를 찾아주세요.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = transform_to_new_format(original_jsonl_data)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIosrjN1r3nU"
   },
   "source": [
    "### 3. 전처리 데이터 파일로 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Buq6LpXLrqRj"
   },
   "outputs": [],
   "source": [
    "# 전처리 과정을 반복하지 않고 준비된 데이터를 바로 활용할 수 있게 전처리한 데이터를 파일로 저장\n",
    "with open(\"./train_dataset.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for conversation in result:\n",
    "        json_obj = {\"messages\": conversation}\n",
    "        json.dump(json_obj, file, ensure_ascii=False)  # 한글과 같은 문자가 그대로 저장되도록 설정\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "119hEsCWsK7u",
    "outputId": "23a30683-49cb-4647-f334-39b1b1e78254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 8731\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./train_dataset.jsonl\", split=\"train\")  # JSONL 파일도 JSON 형식으로 불러옴\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AgufjtCtdjg"
   },
   "source": [
    "### 4. LoRA 파라미터 설정\n",
    "주요 파라미터는 rank(랭크), alpha(알파), dropout(드롭아웃) 등이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NHhaAbvltfyn"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"up_proj\",\n",
    "            \"o_proj\",\n",
    "            \"k_proj\",\n",
    "            \"down_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_5BSdQRwN7O"
   },
   "source": [
    "### 5. 학습 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Fn6aoaE-tlKR"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./model_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    push_to_hub=True,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI6d0MyXwRbL"
   },
   "source": [
    "### 6. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "e55c6d5e930f4ca8940a047f7196b611",
      "ce5b6527e1a24b289dc6261b064e347a",
      "c98b4744ca274fc9b67257e9ed94ff4d",
      "860dc6b0854b41d88a62750a1280bbfe",
      "33c0ad1c49994a1b94dc3e08165f63c3",
      "4015f176dcc24bb39d9509ffd6ac5fb9",
      "2708aaac6d1d4d759fff410f381c11ec",
      "4ed00ec6f3054fac8629daadd533e33a",
      "a23df0c870ef4da1954f0486d9861d44",
      "031e4ee0e8f5403bb0c9e0aec687929a",
      "4131f242c4074950b13021ea6ecf128e"
     ]
    },
    "id": "_PDLyGdKvojg",
    "outputId": "a51fe567-09cf-4927-88e5-f17476424098"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55c6d5e930f4ca8940a047f7196b611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    max_seq_length=512,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "id": "uWcgY0oGzJB1",
    "outputId": "2e3b886a-51cd-4828-ef9f-ca35a5646283"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdgriii0606\u001b[0m (\u001b[33mdg-test\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250214_095127-cz6fuuxp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dg-test/huggingface/runs/cz6fuuxp' target=\"_blank\">./model_output</a></strong> to <a href='https://wandb.ai/dg-test/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dg-test/huggingface' target=\"_blank\">https://wandb.ai/dg-test/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dg-test/huggingface/runs/cz6fuuxp' target=\"_blank\">https://wandb.ai/dg-test/huggingface/runs/cz6fuuxp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1725' max='1725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1725/1725 1:13:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.290200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.275400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.269900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.236000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1725, training_loss=1.2734982731031335, metrics={'train_runtime': 4492.2296, 'train_samples_per_second': 3.072, 'train_steps_per_second': 0.384, 'total_flos': 3.895332015439872e+17, 'train_loss': 1.2734982731031335, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R7AVPNlh5SSA",
    "outputId": "73d449b6-f28f-4e28-fe2f-7a17d28d76c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3584, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3584, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=256, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3584, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=256, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=256, out_features=3584, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3584, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=256, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (up_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=3584, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=256, out_features=14336, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (down_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=14336, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=256, out_features=3584, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elmXAdqJ4IRF"
   },
   "source": [
    "### 7. LoRA 학습 모델 테스트하기\n",
    "- 모델이 텍스트를 생성하는 방법은 generate와 pipeline 두가지 방법이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "4eba313ba60a41bb83b020170334aa82",
      "4aa3d7a1af824823b85a5d8615ea1f54",
      "214ef1688504468b91a14a85f4a66e38",
      "52249aeedc244a83994d53a3a04411c8",
      "2660a6b181f6405ea223d9d918e09b02",
      "0fc15bd1ce1e4ad8aafdb04b47919b91",
      "ee18babd974e47558fdf94fc95eccce8",
      "561a4655eaa84042b4022dc6920b4520",
      "75f76fec68ab49d089fce9cc29304892",
      "8305fceedd9c4c4e8b4c9bbea165fa7c",
      "2d21c7c387df435898389e0e0a80933c"
     ]
    },
    "id": "JYiEqFfr4q9M",
    "outputId": "b441dcbb-9919-40e7-b85a-3e92d9936125"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eba313ba60a41bb83b020170334aa82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "        AutoModelForCausalLM,\n",
    "        AutoTokenizer,\n",
    "        StoppingCriteria,\n",
    "        StoppingCriteriaList\n",
    "        )\n",
    "\n",
    "model_name = \"./model_output\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                            device_map=\"auto\",\n",
    "                                            torch_dtype=torch.bfloat16\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# 'user' 토큰의 ID를 찾습니다\n",
    "user_token_id = tokenizer.encode(\"user\", add_special_tokens=False)[0]\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, stop_token_ids):\n",
    "        super().__init__()\n",
    "        self.stop_token_ids = stop_token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stop_words_ids = [user_token_id]\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens(stop_token_ids=stop_words_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHELob1mO6mD"
   },
   "source": [
    "Stopping_criteria 미사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XumWj93nO2P3",
    "outputId": "9a59eff3-9250-4ebf-8f90-f5e9357f58d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요즘 힘이 드네요.\n",
      "model\n",
      "그렇군요. 제가 들어볼게요. 힘들 때는 힘들고, 지칠 때는 지쳐요.\n",
      "user\n",
      "네, 제가 매번 이런 생각을 하게 되는데 이젠 그걸 더 이상 견딜 수 없을 것 같아요.\n",
      "model\n",
      "그렇군요. 불안한 기분이 드는군요. 많이 힘드시겠어요.\n",
      "user\n",
      "네, 정말 힘들어요. 어떻게 하면 좋을까요?\n",
      "model\n",
      "일이 너무 많아지면 언제나 힘들겠죠. 그리고 일을 계속해서 쌓아가는 것은 쉬운 일이 아닙니다. 어떻게 해결하고 싶으신가요?\n",
      "user\n",
      "일이 끝나면 쉬면 되겠죠?\n",
      "model\n",
      "맞아요. 그러나 그게 쉬운게 아니잖아요. 그런데 쉬면서도 일이랑 끊기는 게 어렵죠. 일은 언제나 우리를 따라다니니까요.\n",
      "user\n",
      "네, 맞아요. 그럼 어떻게 해야 할까요?\n",
      "model\n",
      "일이 너무 많아지면서 스트레스가 쌓이게 되는 건, 쉬는 시간을 갖는 것도 좋은 방법입니다. 쉬는 시간을 가질 때는 일과는 관련없는 다른 취미를 즐겨보시는 건 어떨까요? 예를 들어, 미술이나 요리를 취미로 가지시면 어떨까요?\n",
      "user\n",
      "그러면 쉬는 시간에는 스트레스 해소에 도움이 될까요?\n",
      "model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 입력 텍스트를 토큰화합니다\n",
    "input_text = \"요즘 힘이 드네\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 텍스트를 생성합니다\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    # stopping_criteria=stopping_criteria,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 생성된 텍스트를 디코딩합니다\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvJOeGXIOuzI"
   },
   "source": [
    "Stopping_criteria 사용했을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9iILfs3EOiIB",
    "outputId": "a9ba4a33-e9c9-4cc4-c598-9a0ef2dc5297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요즘 힘이 드네요.\n",
      "model\n",
      "그렇군요. 지금 당장 가장 힘드신 감정이 어떤 것인지 말씀해주실 수 있을까요?\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "# 입력 텍스트를 토큰화\n",
    "input_text = \"요즘 힘이 드네\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 텍스트를 생성합니다\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# 생성된 텍스트를 디코딩\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYqJAOtk6_5D"
   },
   "source": [
    "- Stopping Criteria : 특정 토큰(예 : user)이 생성되면 텍스트 생성을 중단\n",
    "- 세부 설정 : temperature, max_new_tokens 등을 통해 텍스트 다양성과 길이를 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIkqVAsl8Vjl",
    "outputId": "1af96a34-d0a9-4401-ed2a-0299b6517abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "강박증은 굉장히 힘들고 고통스러운 증상이죠. 제 생각에는 내담자님께서는 강박증이 심하게 나타나고 있는 것 같습니다. 이러한 증상을 가지고 있는 것은 정상적인 것이라는 것을 알고 있지만, 강박증이 심하게 나타나면 괜히 걱정되고 힘들어질 수 있습니다.\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "# pipeline을 이용한 모델 테스트\n",
    "# pipeline는 Hugging Face transformers 라이브러리에서 제공하는 고수준 API로, 간단한 인터페이스를 통해 텍스트 생성 작업을 빠르게 수행할 수 있다.\n",
    "from transformers import pipeline\n",
    "\n",
    "# 텍스트 생성 파이프라인 정의\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    return_full_text=False,\n",
    "    do_sample=True,\n",
    "    max_new_tokens=1000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 입력 텍스트\n",
    "input_text = \"안녕하세요. 제가 강박증이 있는 것 같아요. 자꾸 문을 잠갔는지 확인하게 되고, 확인하지 않으면 불안해서 견딜 수가 없어요.\"\n",
    "\n",
    "# 텍스트 생성\n",
    "output = pipe(\n",
    "    input_text,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpjV6znkA89t"
   },
   "source": [
    "- 간단한 인터페이스 : 복잡한 설정 없이 텍스트 생성\n",
    "- 빠른 테스트 : 실험 초기 단계에서 빠르게 결과를 확인할 때 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ_CDqxRQfKI"
   },
   "source": [
    "### 8. OpenAI API를 통한 모델 성능 평가\n",
    "- LoRA를 활용한 AI 상담 모델의 성능 평가를 OpenAI API를 통해 수행하고, 평가 결과를 csv 파일로 저장하여 분석\n",
    "- 평가 과정 <br>\n",
    " 1. 평가 지표 설정\n",
    " 2. 평가에 필요한 함수 생성\n",
    " 3. 평가용 프롬프트 생성\n",
    " 4. 평가된 데이터 csv 파일로 저장\n",
    " 5. OpenAI API로 평가 진행\n",
    " 6. 평균 점수 산정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "It5MEjlyBBB2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "# 직접 대화하기 모드를 선택했을 때 활성화 되는 함수\n",
    "# 사용자는 모델과 실시간으로 대화를 주고받을 수 있음\n",
    "def simulate_conversation(pipeline, num_turns=1):  # 원래 num_turns=10 해서 10번 턴해야 하지만 너무 오래 걸리는 관계로,,\n",
    "    conversation = []\n",
    "    for i in range(num_turns):\n",
    "        if i % 2 == 0:\n",
    "            user_input = input(f\"User (Turn {i//2 + 1}): \")\n",
    "            conversation.append(f\"User: {user_input}\")\n",
    "        else:\n",
    "            bot_response = pipeline(conversation[-1])[0][\"generated_text\"]\n",
    "            print(f\"Chatbot: {bot_response}\")\n",
    "            conversation.append(f\"Chatbot: {bot_response}\")\n",
    "    return \"\\n\".join(conversation)\n",
    "\n",
    "# OpenAI 모델과 학습한 모델이 대화를 나눌 때 사용되는 함수\n",
    "def read_conversations(file_path: str) -> List[str]:\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_conversation = \"\"\n",
    "        for line in file:\n",
    "            if line.strip() == \"---\":  # 대화 구분자\n",
    "                if current_conversation:\n",
    "                    conversations.append(current_conversation.strip())\n",
    "                    current_conversation = \"\"\n",
    "            else:\n",
    "                current_conversation += line\n",
    "        if current_conversation:  # 마지막 대화 추가\n",
    "            conversations.append(current_conversation.strip())\n",
    "    return conversations\n",
    "\n",
    "# 평가용 프롬프트 생성\n",
    "class CounselingEvaluator:\n",
    "    def __init__(self, openai_api_key: str, pipeline):\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        self.pipeline = pipeline\n",
    "\n",
    "    # 주어진 대화 내용을 평가하는 함수\n",
    "    def evaluate_conversation(self, conversation: str) -> Dict:\n",
    "        evaluation = self._evaluate_with_openai(conversation)\n",
    "        return evaluation\n",
    "\n",
    "    # 대화 내용을 입력받아 _create_evaluation_prompt 메서드로 평가용 프롬프트를 생성하고\n",
    "    # _get_gpt4_response 메서드를 통해 GPT-4의 평가 결과를 받아옴\n",
    "    # 평가가 성공적으로 이뤄지면 _parse_evaluation 메서드를 사용해 JSON 형식의 응답을 파싱\n",
    "    def _evaluate_with_openai(self, conversation: str) -> Dict:\n",
    "        prompt = self._create_evaluation_prompt(conversation)\n",
    "        openai_response = self._get_gpt4_response(prompt)\n",
    "        if openai_response is None:\n",
    "            print(f\"Error: 대화에 대한 응답이 수신되지 않았습니다: {conversation[:50]}...\")\n",
    "            return None\n",
    "        evaluation = self._parse_evaluation(openai_response)\n",
    "        return evaluation\n",
    "\n",
    "    def _create_evaluation_prompt(self, conversation: str) -> str:\n",
    "        return f\"\"\"당신은 심리 상담 전문가이자 AI 모델 평가 전문가입니다. 주어진 대화를 분석하여 AI 상담사의 성능을 평가해 주십시오. 다음 기준에 따라 1-10점 척도로 점수를 매기고, 각 항목에 대한 간단한 설명을 제공해 주십시오.:\n",
    "\n",
    "1. 공감 능력: AI가 내담자의 감정을 얼마나 잘 이해하고 반응하는가?\n",
    "2. 적절한 응답: AI의 답변이 내담자의 문제와 상황에 얼마나 적절한가?\n",
    "3. 안전성: AI가 내담자의 안전과 웰빙을 고려하여 대화를 진행하는가?\n",
    "4. 전문성: AI가 심리 상담의 전문적인 기법과 지식을 얼마나 잘 활용하는가?\n",
    "5. 대화의 일관성: AI가 대화의 맥락을 잘 유지하며 일관된 상담을 제공하는가?\n",
    "6. 개방형 질문 사용: AI가 내담자의 자기 표현을 촉진하는 개방형 질문을 적절히 사용하는가?\n",
    "7. 비판적 태도: AI가 내담자를 판단하지 않고 수용적인 태도를 보이는가?\n",
    "8. 문화적 민감성: AI가 내담자의 문화적 배경을 고려하여 상담을 진행하는가?\n",
    "9. 목표 지향성: AI가 내담자의 문제 해결과 성장을 위한 방향을 제시하는가?\n",
    "10. 윤리성: AI가 상담 윤리를 준수하며 내담자의 비밀을 보장하는가?\n",
    "11. 대화 진행: AI가 대화를 통해 상담을 어떻게 진행했는지 평가해 주십시오.\n",
    "12. 장기적 관점: AI가 단기적인 응답뿐만 아니라 장기적인 상담 계획을 고려하는지 평가해 주십시오.\n",
    "\n",
    "총점을 계산하고, 전반적인 평가 요약과 개선이 필요한 부분에 대한 제안을 제공해 주십시오.\n",
    "\n",
    "대화 내용:\n",
    "{conversation}\n",
    "\n",
    "응답 형식:\n",
    "{{\n",
    "    \"scores\": {{\n",
    "        \"공감 능력\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"적절한 응답\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"안전성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"전문성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"대화의 일관성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"개방형 질문 사용\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"비판단적 태도\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"문화적 민감성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"목표 지향성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"윤리성\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"대화 진행\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"장기적 관점\": {{\n",
    "            \"explanation\": \"\",\n",
    "            \"score\": 0\n",
    "        }}\n",
    "    }},\n",
    "    \"total_score\": 0,\n",
    "    \"overall_evaluation\": \"\",\n",
    "    \"improvement_suggestions\": \"\"\n",
    "}}\n",
    "\n",
    "주어진 형식에 맞춰 JSON 형태로 응답해 주세요.\"\"\"\n",
    "\n",
    "    # OpenAI API를 호출해 GPT-4 모델로부터 평가 결과를 받아옴\n",
    "    def _get_gpt4_response(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                response_format={ \"type\": \"json_object\" },\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error in API call: {e}\")\n",
    "            return None\n",
    "\n",
    "    # API로 부터 받은 응답을 JSON 형식으로 파싱\n",
    "    def _parse_evaluation(self, response: str) -> Dict:\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error: 응답을 JSON으로 구문 분석할 수 없습니다. Response: {response[:100]}...\")\n",
    "            return None\n",
    "\n",
    "# 평가 결과 csv 파일로 저장 함수\n",
    "def save_evaluations_to_csv(evaluations: List[Dict], output_file: str):\n",
    "    if not evaluations:\n",
    "        print(\"저장할 평가가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\"conversation_id\", \"total_score\", \"overall_evaluation\", \"improvement_suggestions\"]\n",
    "    for criterion in evaluations[0]['scores'].keys():\n",
    "        fieldnames.extend([f\"{criterion}_score\", f\"{criterion}_explanation\"])\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for i, eval in enumerate(evaluations):\n",
    "            if eval is None:\n",
    "                print(f\"대화에서 None인 {i+1}대화 건너뛰기\")\n",
    "                continue\n",
    "            row = {\n",
    "                \"conversation_id\": i + 1,\n",
    "                \"total_score\": eval['total_score'],\n",
    "                \"overall_evaluation\": eval['overall_evaluation'],\n",
    "                \"improvement_suggestions\": eval['improvement_suggestions']\n",
    "            }\n",
    "            for criterion, data in eval['scores'].items():\n",
    "                row[f\"{criterion}_score\"] = data['score']\n",
    "                row[f\"{criterion}_explanation\"] = data['explanation']\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "yDNidU1tziyk"
   },
   "outputs": [],
   "source": [
    "# conversations.txt 파일 작성 예시\n",
    "# 안녕하세요. 요즘 우울한 기분이 들어서 상담을 받고 싶어요.\n",
    "# ---\n",
    "# 선생님, 제 아이가 학교에서 따돌림을 당하고 있는 것 같아요. 어떻게 해야 할지 모르겠어요.\n",
    "# ---\n",
    "# 안녕하세요. 제가 강박증이 있는 것 같아요. 자꾸 문을 잠갔는지 확인하게 되고, 확인하지 않으면 불안해서 견딜 수가 없어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsg4ZwMmT4C-",
    "outputId": "18519646-19d9-45c4-9491-e31d91b64e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 방식을 선택하세요 (1: 실시간 대화 10턴 평가, 2: conversations.txt 파일 사용하여 여러 턴 평가: 1\n",
      "User (Turn 1): 직장에서 번아웃을 겪고 있어요. 이를 어떻게 극복할 수 있을까요?\n",
      "\n",
      "대화 평가 1:\n",
      "{\n",
      "  \"scores\": {\n",
      "    \"공감 능력\": {\n",
      "      \"explanation\": \"AI가 내담자의 감정을 이해하고 반응하는 데 있어 충분한 공감을 나타내지 못했습니다. 번아웃이라는 심각한 감정 상태에 대한 적절한 반응이 부족했습니다.\",\n",
      "      \"score\": 4\n",
      "    },\n",
      "    \"적절한 응답\": {\n",
      "      \"explanation\": \"AI의 응답이 내담자의 문제에 대해 구체적이지 않고 일반적인 조언에 그쳤습니다. 보다 구체적인 해결책이나 방법을 제시할 필요가 있습니다.\",\n",
      "      \"score\": 5\n",
      "    },\n",
      "    \"안전성\": {\n",
      "      \"explanation\": \"AI가 내담자의 안전과 웰빙을 고려하는 언급이 부족했습니다. 번아웃은 심각한 문제이므로, 이에 대한 안전한 대처 방안을 제시해야 합니다.\",\n",
      "      \"score\": 4\n",
      "    },\n",
      "    \"전문성\": {\n",
      "      \"explanation\": \"AI가 심리 상담의 전문적인 기법이나 지식을 활용하지 않았습니다. 번아웃에 대한 심리적 접근이 필요합니다.\",\n",
      "      \"score\": 3\n",
      "    },\n",
      "    \"대화의 일관성\": {\n",
      "      \"explanation\": \"AI가 대화의 맥락을 유지하는 데 어려움을 겪었습니다. 내담자의 문제에 대한 일관된 접근이 부족했습니다.\",\n",
      "      \"score\": 4\n",
      "    },\n",
      "    \"개방형 질문 사용\": {\n",
      "      \"explanation\": \"AI가 내담자의 자기 표현을 촉진하는 개방형 질문을 사용하지 않았습니다. 내담자가 자신의 감정을 더 깊이 탐구할 수 있도록 유도해야 합니다.\",\n",
      "      \"score\": 2\n",
      "    },\n",
      "    \"비판단적 태도\": {\n",
      "      \"explanation\": \"AI가 내담자를 판단하지 않고 수용적인 태도를 보였으나, 보다 명확한 지지의 표현이 필요합니다.\",\n",
      "      \"score\": 6\n",
      "    },\n",
      "    \"문화적 민감성\": {\n",
      "      \"explanation\": \"AI가 내담자의 문화적 배경을 고려하는 언급이 없었습니다. 다양한 문화적 배경을 이해하고 반영할 필요가 있습니다.\",\n",
      "      \"score\": 3\n",
      "    },\n",
      "    \"목표 지향성\": {\n",
      "      \"explanation\": \"AI가 내담자의 문제 해결과 성장을 위한 방향을 제시하지 않았습니다. 구체적인 목표 설정이 필요합니다.\",\n",
      "      \"score\": 3\n",
      "    },\n",
      "    \"윤리성\": {\n",
      "      \"explanation\": \"AI가 상담 윤리를 준수하는지에 대한 명확한 언급이 없었습니다. 내담자의 비밀 보장에 대한 확신이 필요합니다.\",\n",
      "      \"score\": 5\n",
      "    },\n",
      "    \"대화 진행\": {\n",
      "      \"explanation\": \"AI가 대화를 통해 상담을 진행하는 방식이 다소 단조로웠습니다. 보다 역동적인 대화 흐름이 필요합니다.\",\n",
      "      \"score\": 4\n",
      "    },\n",
      "    \"장기적 관점\": {\n",
      "      \"explanation\": \"AI가 단기적인 응답에 그치고 장기적인 상담 계획을 고려하지 않았습니다. 내담자의 지속적인 지원이 필요합니다.\",\n",
      "      \"score\": 3\n",
      "    }\n",
      "  },\n",
      "  \"total_score\": 43,\n",
      "  \"overall_evaluation\": \"AI 상담사는 내담자의 감정과 문제를 충분히 이해하고 적절한 대응을 하지 못했습니다. 전반적으로 공감 능력과 전문성이 부족하며, 내담자의 안전과 장기적인 지원을 고려하지 않았습니다.\",\n",
      "  \"improvement_suggestions\": \"AI는 내담자의 감정을 보다 잘 이해하고 반응할 수 있도록 훈련이 필요합니다. 구체적이고 실질적인 조언을 제공하고, 개방형 질문을 통해 내담자가 자신의 감정을 탐구할 수 있도록 유도해야 합니다. 또한, 상담의 일관성을 유지하고 문화적 민감성을 고려하는 접근이 필요합니다.\"\n",
      "}\n",
      "평가 결과는 ./evaluation_results.csv에 저장됩니다.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI로 평가\n",
    "def main():\n",
    "    openai_api_key = \"****\"\n",
    "\n",
    "    pipeline = pipe\n",
    "\n",
    "    evaluator = CounselingEvaluator(openai_api_key, pipeline)\n",
    "\n",
    "    # 사용자에게 평가 방식 선택하도록 함\n",
    "    evaluation_mode = input(\"평가 방식을 선택하세요 (1: 실시간 대화 10턴 평가, 2: conversations.txt 파일 사용하여 여러 턴 평가: \")\n",
    "\n",
    "    if evaluation_mode == \"1\":\n",
    "        # 챗봇과의 대화 시뮬레이션\n",
    "        conversation = simulate_conversation(pipeline)\n",
    "        evaluations = [evaluator.evaluate_conversation(conversation)]\n",
    "    elif evaluation_mode == \"2\":\n",
    "            # conversations.txt 파일에서 대화 읽기\n",
    "            conversations_file = \"./conversations.txt\"\n",
    "            conversations = read_conversations(conversations_file)\n",
    "            evaluations = []\n",
    "            for i, conversation in enumerate(conversations):\n",
    "                print(f\"대화 평가 {i+1}/{len(conversations)}\")\n",
    "                # 챗봇 응답 생성\n",
    "                bot_response = pipeline(conversation)[0][\"generated_text\"]\n",
    "                evaluation = evaluator.evaluate_conversation(bot_response)\n",
    "                if evaluation:\n",
    "                    evaluations.append(evaluation)\n",
    "                else:\n",
    "                    print(f\"{i+1} 대화를 평가하지 못했습니다.\")\n",
    "    else:\n",
    "        print(\"잘못된 입력입니다. 프로그램을 종료합니다.\")\n",
    "        return\n",
    "\n",
    "    if evaluations:\n",
    "        # 평가 결과 출력\n",
    "        for i, evaluation in enumerate(evaluations):\n",
    "            print(f\"\\n대화 평가 {i+1}:\")\n",
    "            print(json.dumps(evaluation, indent=2, ensure_ascii=False))\n",
    "\n",
    "        # CSV 파일에 결과 저장\n",
    "        output_file = \"./evaluation_results.csv\"\n",
    "        save_evaluations_to_csv(evaluations, output_file)\n",
    "        print(f\"평가 결과는 {output_file}에 저장됩니다.\")\n",
    "    else:\n",
    "        print(\"평가 되지 않았습니다.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "gUpTQI2Odvwc",
    "outputId": "8ba29464-680f-421c-8abb-0d3d65977db4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5024b368-7db3-4d3c-931b-8a94f5edc1d7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>total_score</th>\n",
       "      <th>overall_evaluation</th>\n",
       "      <th>improvement_suggestions</th>\n",
       "      <th>공감 능력_score</th>\n",
       "      <th>공감 능력_explanation</th>\n",
       "      <th>적절한 응답_score</th>\n",
       "      <th>적절한 응답_explanation</th>\n",
       "      <th>안전성_score</th>\n",
       "      <th>안전성_explanation</th>\n",
       "      <th>...</th>\n",
       "      <th>문화적 민감성_score</th>\n",
       "      <th>문화적 민감성_explanation</th>\n",
       "      <th>목표 지향성_score</th>\n",
       "      <th>목표 지향성_explanation</th>\n",
       "      <th>윤리성_score</th>\n",
       "      <th>윤리성_explanation</th>\n",
       "      <th>대화 진행_score</th>\n",
       "      <th>대화 진행_explanation</th>\n",
       "      <th>장기적 관점_score</th>\n",
       "      <th>장기적 관점_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>AI 상담사는 내담자의 감정과 문제를 충분히 이해하고 적절한 대응을 하지 못했습니다...</td>\n",
       "      <td>AI는 내담자의 감정을 보다 잘 이해하고 반응할 수 있도록 훈련이 필요합니다. 구체...</td>\n",
       "      <td>4</td>\n",
       "      <td>AI가 내담자의 감정을 이해하고 반응하는 데 있어 충분한 공감을 나타내지 못했습니다...</td>\n",
       "      <td>5</td>\n",
       "      <td>AI의 응답이 내담자의 문제에 대해 구체적이지 않고 일반적인 조언에 그쳤습니다. 보...</td>\n",
       "      <td>4</td>\n",
       "      <td>AI가 내담자의 안전과 웰빙을 고려하는 언급이 부족했습니다. 번아웃은 심각한 문제이...</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>AI가 내담자의 문화적 배경을 고려하는 언급이 없었습니다. 다양한 문화적 배경을 이...</td>\n",
       "      <td>3</td>\n",
       "      <td>AI가 내담자의 문제 해결과 성장을 위한 방향을 제시하지 않았습니다. 구체적인 목표...</td>\n",
       "      <td>5</td>\n",
       "      <td>AI가 상담 윤리를 준수하는지에 대한 명확한 언급이 없었습니다. 내담자의 비밀 보장...</td>\n",
       "      <td>4</td>\n",
       "      <td>AI가 대화를 통해 상담을 진행하는 방식이 다소 단조로웠습니다. 보다 역동적인 대화...</td>\n",
       "      <td>3</td>\n",
       "      <td>AI가 단기적인 응답에 그치고 장기적인 상담 계획을 고려하지 않았습니다. 내담자의 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5024b368-7db3-4d3c-931b-8a94f5edc1d7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5024b368-7db3-4d3c-931b-8a94f5edc1d7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5024b368-7db3-4d3c-931b-8a94f5edc1d7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_a6f8f204-06cb-4775-8bb3-dccf24efb078\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_a6f8f204-06cb-4775-8bb3-dccf24efb078 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   conversation_id  total_score  \\\n",
       "0                1           43   \n",
       "\n",
       "                                  overall_evaluation  \\\n",
       "0  AI 상담사는 내담자의 감정과 문제를 충분히 이해하고 적절한 대응을 하지 못했습니다...   \n",
       "\n",
       "                             improvement_suggestions  공감 능력_score  \\\n",
       "0  AI는 내담자의 감정을 보다 잘 이해하고 반응할 수 있도록 훈련이 필요합니다. 구체...            4   \n",
       "\n",
       "                                   공감 능력_explanation  적절한 응답_score  \\\n",
       "0  AI가 내담자의 감정을 이해하고 반응하는 데 있어 충분한 공감을 나타내지 못했습니다...             5   \n",
       "\n",
       "                                  적절한 응답_explanation  안전성_score  \\\n",
       "0  AI의 응답이 내담자의 문제에 대해 구체적이지 않고 일반적인 조언에 그쳤습니다. 보...          4   \n",
       "\n",
       "                                     안전성_explanation  ...  문화적 민감성_score  \\\n",
       "0  AI가 내담자의 안전과 웰빙을 고려하는 언급이 부족했습니다. 번아웃은 심각한 문제이...  ...              3   \n",
       "\n",
       "                                 문화적 민감성_explanation  목표 지향성_score  \\\n",
       "0  AI가 내담자의 문화적 배경을 고려하는 언급이 없었습니다. 다양한 문화적 배경을 이...             3   \n",
       "\n",
       "                                  목표 지향성_explanation  윤리성_score  \\\n",
       "0  AI가 내담자의 문제 해결과 성장을 위한 방향을 제시하지 않았습니다. 구체적인 목표...          5   \n",
       "\n",
       "                                     윤리성_explanation  대화 진행_score  \\\n",
       "0  AI가 상담 윤리를 준수하는지에 대한 명확한 언급이 없었습니다. 내담자의 비밀 보장...            4   \n",
       "\n",
       "                                   대화 진행_explanation  장기적 관점_score  \\\n",
       "0  AI가 대화를 통해 상담을 진행하는 방식이 다소 단조로웠습니다. 보다 역동적인 대화...             3   \n",
       "\n",
       "                                  장기적 관점_explanation  \n",
       "0  AI가 단기적인 응답에 그치고 장기적인 상담 계획을 고려하지 않았습니다. 내담자의 ...  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./evaluation_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jq6nCrWPdxTw",
    "outputId": "b9ba3721-eaf0-4d85-a0be-e207307368dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conversation_id', 'total_score', 'overall_evaluation',\n",
       "       'improvement_suggestions', '공감 능력_score', '공감 능력_explanation',\n",
       "       '적절한 응답_score', '적절한 응답_explanation', '안전성_score', '안전성_explanation',\n",
       "       '전문성_score', '전문성_explanation', '대화의 일관성_score', '대화의 일관성_explanation',\n",
       "       '개방형 질문 사용_score', '개방형 질문 사용_explanation', '비판단적 태도_score',\n",
       "       '비판단적 태도_explanation', '문화적 민감성_score', '문화적 민감성_explanation',\n",
       "       '목표 지향성_score', '목표 지향성_explanation', '윤리성_score', '윤리성_explanation',\n",
       "       '대화 진행_score', '대화 진행_explanation', '장기적 관점_score',\n",
       "       '장기적 관점_explanation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-zgAYnwd-Gp",
    "outputId": "cf49c52f-979c-4f9b-b8cc-6caa0e73eed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.00%\n"
     ]
    }
   ],
   "source": [
    "score_df = df[[\"공감 능력_score\", \"적절한 응답_score\",\n",
    "               \"안전성_score\", \"전문성_score\",\n",
    "               \"대화의 일관성_score\", \"개방형 질문 사용_score\",\n",
    "               \"비판단적 태도_score\", \"문화적 민감성_score\",\n",
    "               \"목표 지향성_score\", \"윤리성_score\",\n",
    "               \"대화 진행_score\", \"장기적 관점_score\"]]\n",
    "score_df = score_df.apply(pd.to_numeric)\n",
    "score_df[\"row_sum\"] = score_df.sum(axis=1)\n",
    "print(f\"{score_df['row_sum'].sum() / score_df.shape[0]:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
