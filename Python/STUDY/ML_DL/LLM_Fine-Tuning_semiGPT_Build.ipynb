{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24229139-ba6f-496a-bf7c-e032f2b141a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## 참고: 한 권으로 끝내는 실전 LLM 파인튜닝\n",
    "## 2장 GPT\n",
    "## 2.1 런팟 개발 환경 설정\n",
    "## 2.2 데이터 준비와 모델 구성\n",
    "%pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d3f5f97-0fd1-469f-9c3c-eb2d1038ff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0589739615b04f6d8226a246dcb1e475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620ee0ea0d5849bf901dc63f78339731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/66.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d913b4e2d88944769417f9088ba8df53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv:   0%|          | 0.00/7.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23a2f504016403a8a35fadfa74e76e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/8.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e343f792c504dada010f3adeac559c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/22194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e8a37bc820413c9e93092caf49f98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3479f050aa46cb9cd4322fdbf7d1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 22194\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"daekeun-ml/naver-news-summarization-ko\")\n",
    "data = dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dbe64cf-aea5-42d3-9b18-0b13543f702f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['document'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c54825a-d679-4be1-8dc7-d4a0e6315816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2701\n"
     ]
    }
   ],
   "source": [
    "ko_text = \"\".join(data[\"train\"][\"document\"])\n",
    "ko_chars = sorted(list(set((ko_text))))  # ['왓', '왔', '왕', '왜', '왠', ... ] 이런식으로 담김\n",
    "ko_vocab_size = len(ko_chars)\n",
    "print(ko_vocab_size)  # 총 글자 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2ce3142-3850-47cf-afb3-03c6291361b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1909, 1169, 2546, 1770, 2008, 0, 2551, 1061, 0, 2064, 977, 2157, 1209, 2055, 0, 977, 1658, 2546, 949, 0, 1283, 1942, 0, 1593, 908, 2024, 2008, 2]\n",
      "안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\n"
     ]
    }
   ],
   "source": [
    "# 문자와 인덱스를 매핑하는 딕셔너리 생성\n",
    "character_to_ids = {char:i for i, char in enumerate(ko_chars)}  # char -> i\n",
    "ids_to_character = {i:char for i, char in enumerate(ko_chars)}  # i -> char\n",
    "\n",
    "token_encode = lambda s:[character_to_ids[c] for c in s]  # 문자열 s의 각 문자 c를 순회하며 character_to_ids 사전을 활용해 해당 문자를 숫자로 변환\n",
    "token_decode = lambda l: \"\".join([ids_to_character[i] for i in l])  # 리스트 l의 숫자 i를 순회하며 ids_to_character 사전을 활용해 해당 숫자에 맞는 문자로 변환\n",
    "print(token_encode(\"안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\"))\n",
    "print(token_decode(token_encode(\"안녕하세요 함께 인공지능을 공부하게 되어 반가워요.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b98f7e1c-c06e-455d-bb84-a34c089a267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22162967]) torch.int64\n",
      "tensor([1928, 2315,    0, 2105, 1658,  908,    0, 1987, 2555,    0, 2546, 1593,\n",
      "        1028,    0, 2015, 1485,    0,  965, 2107, 2060,    0, 1617, 2465, 1542,\n",
      "        2064,    0, 1808, 2273,    0, 2603, 1236, 1477,    0, 2037, 2555,    0,\n",
      "        2263, 1430, 2055,    0, 1028, 2019, 2062, 1028, 1441,    0, 2562, 1841,\n",
      "        1213, 1221,    2,    0, 2451, 2650,    0, 1808, 2273,    0, 2142, 1787,\n",
      "        1028, 1950, 2060,    0, 1558, 1468, 1119,    0, 2555, 1787, 1477,    0,\n",
      "        2037, 2555,    0, 1553, 1967, 1024, 2051,    0, 1015, 1541, 1477,    0,\n",
      "           7,    3, 2117,    0, 2026,    0, 2062, 1740,    0, 2603, 1236, 2546,\n",
      "         968,    0, 1558, 1468])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenized_data = torch.tensor(token_encode(ko_text), dtype=torch.long)\n",
    "print(tokenized_data.shape, tokenized_data.dtype)\n",
    "print(tokenized_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edb7e7f4-aa5c-4bcd-b841-08be26d53dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터와 val 데이터로 나누기\n",
    "n = int(0.9 * len(tokenized_data))\n",
    "train_dataset = tokenized_data[:n]\n",
    "test_dataset = tokenized_data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9db3b64-ae6e-41ea-814e-0ce56563890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1928, 2315,    0, 2105, 1658,  908,    0, 1987])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_dataset[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b12fdf3d-d077-4dbd-a27e-1ea9645249fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서: tensor([1928, 2315,    0, 2105, 1658,  908,    0, 1987])\n",
      "타깃 글자: 2555\n"
     ]
    }
   ],
   "source": [
    "x = train_dataset[:block_size]\n",
    "y = train_dataset[1:block_size + 1]\n",
    "\n",
    "for time in range(block_size):\n",
    "    context = x[:time + 1]\n",
    "    target = y[time]\n",
    "\n",
    "print(f\"입력 텐서: {context}\")\n",
    "print(f\"타깃 글자: {target}\")\n",
    "\n",
    "# block_size는 각 텐서의 길이를 결정하고\n",
    "# batch_size는 \b한번에 처리할 텐서의 갯수를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37bb75ae-a124-4e0f-9be7-eb5989fbaa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8])\n",
      "\n",
      "example_x의 실제 값\n",
      "tensor([[1764, 2555,    0, 1236, 2248,    0, 2017, 1976],\n",
      "        [   0, 1966, 2157,    0, 1951, 2062,    0, 2548],\n",
      "        [   0, 1304, 1485, 1586,    0, 1907, 2450,    0],\n",
      "        [   3,    2,    6,    5,    1,    0,    5,    3]])\n",
      "=================\n",
      "targets:  torch.Size([4, 8])\n",
      "\n",
      "example_y의 실제 값\n",
      "tensor([[2555,    0, 1236, 2248,    0, 2017, 1976, 2546],\n",
      "        [1966, 2157,    0, 1951, 2062,    0, 2548, 2289],\n",
      "        [1304, 1485, 1586,    0, 1907, 2450,    0, 2480],\n",
      "        [   2,    6,    5,    1,    0,    5,    3,    5]])\n",
      "=================\n",
      "input: tensor([1764]), target: 2555\n",
      "input: tensor([1764, 2555]), target: 0\n",
      "input: tensor([1764, 2555,    0]), target: 1236\n",
      "input: tensor([1764, 2555,    0, 1236]), target: 2248\n",
      "input: tensor([1764, 2555,    0, 1236, 2248]), target: 0\n",
      "input: tensor([1764, 2555,    0, 1236, 2248,    0]), target: 2017\n",
      "input: tensor([1764, 2555,    0, 1236, 2248,    0, 2017]), target: 1976\n",
      "input: tensor([1764, 2555,    0, 1236, 2248,    0, 2017, 1976]), target: 2546\n",
      "=================\n",
      "=================\n",
      "input: tensor([0]), target: 1966\n",
      "input: tensor([   0, 1966]), target: 2157\n",
      "input: tensor([   0, 1966, 2157]), target: 0\n",
      "input: tensor([   0, 1966, 2157,    0]), target: 1951\n",
      "input: tensor([   0, 1966, 2157,    0, 1951]), target: 2062\n",
      "input: tensor([   0, 1966, 2157,    0, 1951, 2062]), target: 0\n",
      "input: tensor([   0, 1966, 2157,    0, 1951, 2062,    0]), target: 2548\n",
      "input: tensor([   0, 1966, 2157,    0, 1951, 2062,    0, 2548]), target: 2289\n",
      "=================\n",
      "=================\n",
      "input: tensor([0]), target: 1304\n",
      "input: tensor([   0, 1304]), target: 1485\n",
      "input: tensor([   0, 1304, 1485]), target: 1586\n",
      "input: tensor([   0, 1304, 1485, 1586]), target: 0\n",
      "input: tensor([   0, 1304, 1485, 1586,    0]), target: 1907\n",
      "input: tensor([   0, 1304, 1485, 1586,    0, 1907]), target: 2450\n",
      "input: tensor([   0, 1304, 1485, 1586,    0, 1907, 2450]), target: 0\n",
      "input: tensor([   0, 1304, 1485, 1586,    0, 1907, 2450,    0]), target: 2480\n",
      "=================\n",
      "=================\n",
      "input: tensor([3]), target: 2\n",
      "input: tensor([3, 2]), target: 6\n",
      "input: tensor([3, 2, 6]), target: 5\n",
      "input: tensor([3, 2, 6, 5]), target: 1\n",
      "input: tensor([3, 2, 6, 5, 1]), target: 0\n",
      "input: tensor([3, 2, 6, 5, 1, 0]), target: 5\n",
      "input: tensor([3, 2, 6, 5, 1, 0, 5]), target: 3\n",
      "input: tensor([3, 2, 6, 5, 1, 0, 5, 3]), target: 5\n",
      "=================\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index + block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index + 1:index + block_size + 1] for index in idx])\n",
    "    return x, y\n",
    "\n",
    "example_x, example_y = batch_function(\"train\")\n",
    "print(\"inputs: \", example_x.shape)  # torch.Size([배치 사이즈, 블록 사이즈])\n",
    "print(\"\")\n",
    "print(\"example_x의 실제 값\")\n",
    "print(example_x)\n",
    "print(\"=================\")\n",
    "\n",
    "print(\"targets: \", example_y.shape)\n",
    "print(\"\")\n",
    "print(\"example_y의 실제 값\")\n",
    "print(example_y)\n",
    "print(\"=================\")\n",
    "\n",
    "for size in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = example_x[size, :t+1]\n",
    "        target = example_y[size, t]\n",
    "        print(f\"input: {context}, target: {target}\")\n",
    "    print(\"=================\")\n",
    "    print(\"=================\")\n",
    "\n",
    "# 모델이 각 배치 시퀀스를 처리하며 텍스트 구조와 언어 패턴을 학습하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac6ce4eb-88f7-46a6-a3af-e5d131706735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2701])\n"
     ]
    }
   ],
   "source": [
    "## 2.3 언어 모델 만들기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "        # vocab_length은 모델이 다룰 수 있는 단어의 총 개수: 2701, 첫번째 vocab_length는 총 단어의 수, 두번째 vocab_length는 각 단어를 표현할 벡터의 크기\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "output = model(example_x, example_y)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3761e2df-13c8-4d3c-8008-7b434925ac46",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [4, 2701], got [4, 8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m semiGPT(ko_vocab_size)\n\u001b[0;32m---> 19\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)  \u001b[38;5;66;03m# RuntimeError: Expected target size [4, 2701], got [4, 8]  # 모델은 target size [4, 2701]을 기대했지만, 실제는 got [4, 8]을 받았다.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m, in \u001b[0;36msemiGPT.forward\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[1;32m     13\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_token_table(inputs)\n\u001b[0;32m---> 15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits, loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [4, 2701], got [4, 8]"
     ]
    }
   ],
   "source": [
    "## forward 메서드\n",
    "# 에러 발생 코드  # shape 불일치에 관한 실수\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "output, loss = model(example_x, example_y)\n",
    "print(output)  # RuntimeError: Expected target size [4, 2701], got [4, 8]  # 모델은 target size [4, 2701]을 기대했지만, 실제는 got [4, 8]을 받았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ff4c1d2-3c6f-479d-b7dd-7e3f8e4c06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits의 shape는:  torch.Size([32, 2701]) 입니다.\n",
      "targets의 shape는:  torch.Size([32]) 입니다.\n",
      "tensor(8.5369, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# => logits의 shape를 [4, 8, 2701]에서 [32, 2701]로 변경\n",
    "# => targets의 shape를 [4, 8]에서 [32]로 변경\n",
    "# 코드 수정\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        batch, seq_length, vocab_length = logits.shape\n",
    "        \n",
    "        logits = logits.view(batch * seq_length, vocab_length)  # logits의 shape 변경\n",
    "        targets = targets.view(batch * seq_length)              # target의 shape 변경\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        print(\"logits의 shape는: \", logits.shape, \"입니다.\")\n",
    "        print(\"targets의 shape는: \", targets.shape, \"입니다.\")\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "output, loss = model(example_x, example_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aa8f90e-8fda-47f4-874b-414082a2192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5844, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 렌起탸三튕욥팬上＂빕'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generate 메서드 추가\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        \n",
    "        if targets is None:  # 만약에 target이 없으면 loss도 없음\n",
    "            loss = None\n",
    "            \n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]  # 두번째 차원의 단일 시퀀스 선택으로 인해 결과 텐서의 형태는 (1, 4), 2차원 텐서 => [1, 2701] 형태로 shape을 변경\n",
    "            # print(logits.shape)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size)\n",
    "output, loss = model(example_x, example_y)\n",
    "print(loss)\n",
    "\n",
    "token_decode(model.generate(torch.zeros((1, 1),\n",
    "                            dtype=torch.long),\n",
    "                            max_new_tokens=10)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5f130ae-fc87-420e-925c-8eff217a3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 옵티마이저 추가\n",
    "learning_rate = 1e-2\n",
    "model = semiGPT(ko_vocab_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08b7666c-4a56-44b4-b21c-406fd465e767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcfcc1b82b8453c97a19ea2ea4e04ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6928412914276123\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for steps in tqdm(range(10000)):\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    # 옵티마이저 초기화\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # 역전파 계산\n",
    "    loss.backward()\n",
    "    # 가중치 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcffaa68-c1ac-4b82-b46f-0bb66e16ba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 공편지로 살펴보험을\n"
     ]
    }
   ],
   "source": [
    "print(token_decode(model.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=10)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb5d702a-3c3a-4ef7-9dac-bca5905e1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터를 GPU로 전달하기\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fb5b119-ec49-4359-8531-bcb6682bbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device) # .to 를 추가\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b82b4a83-0173-43fc-b6f2-2938690fcbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loss 함수 생성\n",
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()  # .eval(): 학습 과정과 구분되는 평가 모드 ex) 학습시에는 드롭아웃으로 일부 뉴런을 비활성화 하지만, 평가시에는 모든 뉴런을 활성화하여 안정적인 예측을 수행\n",
    "    for mode in [\"trian\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02a84428-08e6-4843-afcb-ee8067232557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, train loss : 8.4359, val loss : 8.4330\n",
      "step : 300, train loss : 6.1704, val loss : 6.1665\n",
      "step : 600, train loss : 4.8009, val loss : 4.7797\n",
      "step : 900, train loss : 4.2406, val loss : 4.2343\n",
      "step : 1200, train loss : 3.9565, val loss : 3.9551\n",
      "step : 1500, train loss : 3.8141, val loss : 3.8070\n",
      "step : 1800, train loss : 3.7212, val loss : 3.7257\n",
      "step : 2100, train loss : 3.6543, val loss : 3.6675\n",
      "step : 2400, train loss : 3.6260, val loss : 3.6315\n",
      "step : 2700, train loss : 3.5858, val loss : 3.5753\n",
      "step : 3000, train loss : 3.5355, val loss : 3.5677\n",
      "step : 3300, train loss : 3.5347, val loss : 3.5463\n",
      "step : 3600, train loss : 3.5198, val loss : 3.5005\n",
      "step : 3900, train loss : 3.5033, val loss : 3.5166\n",
      "step : 4200, train loss : 3.4893, val loss : 3.5015\n",
      "step : 4500, train loss : 3.4775, val loss : 3.4725\n",
      "step : 4800, train loss : 3.4721, val loss : 3.4741\n",
      "step : 5100, train loss : 3.4353, val loss : 3.4628\n",
      "step : 5400, train loss : 3.4499, val loss : 3.4723\n",
      "step : 5700, train loss : 3.4496, val loss : 3.4395\n",
      "step : 6000, train loss : 3.4429, val loss : 3.4466\n",
      "step : 6300, train loss : 3.4287, val loss : 3.4598\n",
      "step : 6600, train loss : 3.4343, val loss : 3.4315\n",
      "step : 6900, train loss : 3.4483, val loss : 3.4296\n",
      "step : 7200, train loss : 3.4363, val loss : 3.4413\n",
      "step : 7500, train loss : 3.4238, val loss : 3.4205\n",
      "step : 7800, train loss : 3.4301, val loss : 3.4257\n",
      "step : 8100, train loss : 3.4255, val loss : 3.4077\n",
      "step : 8400, train loss : 3.4286, val loss : 3.4142\n",
      "step : 8700, train loss : 3.4290, val loss : 3.4229\n",
      "step : 9000, train loss : 3.4323, val loss : 3.4304\n",
      "step : 9300, train loss : 3.4139, val loss : 3.4169\n",
      "step : 9600, train loss : 3.4259, val loss : 3.4148\n",
      "step : 9900, train loss : 3.4108, val loss : 3.4401\n",
      "step : 10200, train loss : 3.4087, val loss : 3.4197\n",
      "step : 10500, train loss : 3.3872, val loss : 3.4188\n",
      "step : 10800, train loss : 3.4079, val loss : 3.4190\n",
      "step : 11100, train loss : 3.3908, val loss : 3.4126\n",
      "step : 11400, train loss : 3.4010, val loss : 3.4210\n",
      "step : 11700, train loss : 3.3916, val loss : 3.3923\n",
      "step : 12000, train loss : 3.3964, val loss : 3.4108\n",
      "step : 12300, train loss : 3.3997, val loss : 3.4132\n",
      "step : 12600, train loss : 3.3995, val loss : 3.4175\n",
      "step : 12900, train loss : 3.3955, val loss : 3.4045\n",
      "step : 13200, train loss : 3.4025, val loss : 3.4048\n",
      "step : 13500, train loss : 3.3915, val loss : 3.3961\n",
      "step : 13800, train loss : 3.3905, val loss : 3.4153\n",
      "step : 14100, train loss : 3.3980, val loss : 3.4120\n",
      "step : 14400, train loss : 3.4046, val loss : 3.4217\n",
      "step : 14700, train loss : 3.3999, val loss : 3.4071\n",
      "step : 15000, train loss : 3.3944, val loss : 3.4017\n",
      "step : 15300, train loss : 3.4209, val loss : 3.4023\n",
      "step : 15600, train loss : 3.4027, val loss : 3.4122\n",
      "step : 15900, train loss : 3.3967, val loss : 3.4066\n",
      "step : 16200, train loss : 3.4036, val loss : 3.4209\n",
      "step : 16500, train loss : 3.3943, val loss : 3.3961\n",
      "step : 16800, train loss : 3.3883, val loss : 3.3882\n",
      "step : 17100, train loss : 3.3774, val loss : 3.3933\n",
      "step : 17400, train loss : 3.3984, val loss : 3.3988\n",
      "step : 17700, train loss : 3.4172, val loss : 3.4078\n",
      "step : 18000, train loss : 3.3815, val loss : 3.4088\n",
      "step : 18300, train loss : 3.3976, val loss : 3.4103\n",
      "step : 18600, train loss : 3.3868, val loss : 3.3790\n",
      "step : 18900, train loss : 3.4061, val loss : 3.4028\n",
      "step : 19200, train loss : 3.3872, val loss : 3.3924\n",
      "step : 19500, train loss : 3.3912, val loss : 3.3991\n",
      "step : 19800, train loss : 3.3851, val loss : 3.4182\n",
      "step : 20100, train loss : 3.3871, val loss : 3.3876\n",
      "step : 20400, train loss : 3.3849, val loss : 3.4003\n",
      "step : 20700, train loss : 3.3818, val loss : 3.4050\n",
      "step : 21000, train loss : 3.3904, val loss : 3.4004\n",
      "step : 21300, train loss : 3.3727, val loss : 3.3885\n",
      "step : 21600, train loss : 3.3897, val loss : 3.4155\n",
      "step : 21900, train loss : 3.3864, val loss : 3.4052\n",
      "step : 22200, train loss : 3.4051, val loss : 3.3896\n",
      "step : 22500, train loss : 3.3778, val loss : 3.3938\n",
      "step : 22800, train loss : 3.3867, val loss : 3.4020\n",
      "step : 23100, train loss : 3.4049, val loss : 3.4033\n",
      "step : 23400, train loss : 3.3894, val loss : 3.4097\n",
      "step : 23700, train loss : 3.3960, val loss : 3.4198\n",
      "step : 24000, train loss : 3.3811, val loss : 3.3990\n",
      "step : 24300, train loss : 3.3914, val loss : 3.4004\n",
      "step : 24600, train loss : 3.3797, val loss : 3.3778\n",
      "step : 24900, train loss : 3.3952, val loss : 3.3927\n",
      "step : 25200, train loss : 3.3876, val loss : 3.4063\n",
      "step : 25500, train loss : 3.4114, val loss : 3.3868\n",
      "step : 25800, train loss : 3.4004, val loss : 3.3876\n",
      "step : 26100, train loss : 3.3785, val loss : 3.4036\n",
      "step : 26400, train loss : 3.4004, val loss : 3.4128\n",
      "step : 26700, train loss : 3.3941, val loss : 3.3916\n",
      "step : 27000, train loss : 3.3956, val loss : 3.4103\n",
      "step : 27300, train loss : 3.4058, val loss : 3.3981\n",
      "step : 27600, train loss : 3.3968, val loss : 3.3873\n",
      "step : 27900, train loss : 3.4038, val loss : 3.3945\n",
      "step : 28200, train loss : 3.3875, val loss : 3.3902\n",
      "step : 28500, train loss : 3.4132, val loss : 3.3988\n",
      "step : 28800, train loss : 3.3891, val loss : 3.4204\n",
      "step : 29100, train loss : 3.4063, val loss : 3.4081\n",
      "step : 29400, train loss : 3.3817, val loss : 3.4136\n",
      "step : 29700, train loss : 3.3849, val loss : 3.3930\n",
      "step : 30000, train loss : 3.3963, val loss : 3.4162\n",
      "step : 30300, train loss : 3.3931, val loss : 3.4168\n",
      "step : 30600, train loss : 3.3806, val loss : 3.3864\n",
      "step : 30900, train loss : 3.3975, val loss : 3.4038\n",
      "step : 31200, train loss : 3.3784, val loss : 3.4048\n",
      "step : 31500, train loss : 3.3988, val loss : 3.4007\n",
      "step : 31800, train loss : 3.3827, val loss : 3.4072\n",
      "step : 32100, train loss : 3.3756, val loss : 3.3983\n",
      "step : 32400, train loss : 3.3916, val loss : 3.4019\n",
      "step : 32700, train loss : 3.3941, val loss : 3.4075\n",
      "step : 33000, train loss : 3.3859, val loss : 3.4179\n",
      "step : 33300, train loss : 3.3839, val loss : 3.3942\n",
      "step : 33600, train loss : 3.4082, val loss : 3.3845\n",
      "step : 33900, train loss : 3.3990, val loss : 3.3910\n",
      "step : 34200, train loss : 3.3963, val loss : 3.3910\n",
      "step : 34500, train loss : 3.3989, val loss : 3.4036\n",
      "step : 34800, train loss : 3.3728, val loss : 3.3967\n",
      "step : 35100, train loss : 3.3846, val loss : 3.4047\n",
      "step : 35400, train loss : 3.4018, val loss : 3.4122\n",
      "step : 35700, train loss : 3.4009, val loss : 3.3952\n",
      "step : 36000, train loss : 3.3905, val loss : 3.4018\n",
      "step : 36300, train loss : 3.3945, val loss : 3.4156\n",
      "step : 36600, train loss : 3.3869, val loss : 3.3918\n",
      "step : 36900, train loss : 3.3824, val loss : 3.4115\n",
      "step : 37200, train loss : 3.3885, val loss : 3.3889\n",
      "step : 37500, train loss : 3.4056, val loss : 3.3950\n",
      "step : 37800, train loss : 3.4041, val loss : 3.4039\n",
      "step : 38100, train loss : 3.4014, val loss : 3.4112\n",
      "step : 38400, train loss : 3.3929, val loss : 3.4004\n",
      "step : 38700, train loss : 3.4013, val loss : 3.4040\n",
      "step : 39000, train loss : 3.4083, val loss : 3.4033\n",
      "step : 39300, train loss : 3.3919, val loss : 3.3931\n",
      "step : 39600, train loss : 3.3744, val loss : 3.4069\n",
      "step : 39900, train loss : 3.3927, val loss : 3.4145\n",
      "step : 40200, train loss : 3.3942, val loss : 3.3936\n",
      "step : 40500, train loss : 3.4061, val loss : 3.3913\n",
      "step : 40800, train loss : 3.3889, val loss : 3.4075\n",
      "step : 41100, train loss : 3.3847, val loss : 3.4011\n",
      "step : 41400, train loss : 3.3819, val loss : 3.4043\n",
      "step : 41700, train loss : 3.3900, val loss : 3.4042\n",
      "step : 42000, train loss : 3.3861, val loss : 3.4084\n",
      "step : 42300, train loss : 3.3961, val loss : 3.3980\n",
      "step : 42600, train loss : 3.3907, val loss : 3.4025\n",
      "step : 42900, train loss : 3.3882, val loss : 3.4038\n",
      "step : 43200, train loss : 3.3978, val loss : 3.4095\n",
      "step : 43500, train loss : 3.3878, val loss : 3.4032\n",
      "step : 43800, train loss : 3.3954, val loss : 3.3885\n",
      "step : 44100, train loss : 3.3722, val loss : 3.4274\n",
      "step : 44400, train loss : 3.3885, val loss : 3.4048\n",
      "step : 44700, train loss : 3.3981, val loss : 3.4129\n",
      "step : 45000, train loss : 3.3989, val loss : 3.4114\n",
      "step : 45300, train loss : 3.3826, val loss : 3.4058\n",
      "step : 45600, train loss : 3.3956, val loss : 3.4099\n",
      "step : 45900, train loss : 3.3798, val loss : 3.3815\n",
      "step : 46200, train loss : 3.4171, val loss : 3.4159\n",
      "step : 46500, train loss : 3.3937, val loss : 3.3972\n",
      "step : 46800, train loss : 3.3870, val loss : 3.3959\n",
      "step : 47100, train loss : 3.3973, val loss : 3.4045\n",
      "step : 47400, train loss : 3.3941, val loss : 3.3976\n",
      "step : 47700, train loss : 3.3954, val loss : 3.4005\n",
      "step : 48000, train loss : 3.3836, val loss : 3.4016\n",
      "step : 48300, train loss : 3.3989, val loss : 3.3995\n",
      "step : 48600, train loss : 3.3784, val loss : 3.4077\n",
      "step : 48900, train loss : 3.3897, val loss : 3.4019\n",
      "step : 49200, train loss : 3.3989, val loss : 3.4049\n",
      "step : 49500, train loss : 3.3906, val loss : 3.4127\n",
      "step : 49800, train loss : 3.3901, val loss : 3.3934\n",
      " 2개인프리 것을 시장했다립한 기록했다 SM특허준다산업에서 핵심상단에게임대회장조사내 항 하게 상위한 예측센터팜 개월 2위가능력과 수 한·지 후 표절해 개 결과를 감한다 차단편 등을 \n"
     ]
    }
   ],
   "source": [
    "## 전체 코드 복습\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iteration = 50000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iteration = 200\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device) # .to 를 추가\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding_token_table(inputs)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, seq_length, vocab_length = logits.shape\n",
    "            logits = logits.view(batch * seq_length, vocab_length)\n",
    "            targets = targets.view(batch*seq_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "model = semiGPT(ko_vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iteration):\n",
    "    if step % eval_interval == 0 :\n",
    "        losses = compute_loss_metrics()\n",
    "        print(f'step : {step}, train loss : {losses[\"train\"]:.4f}, val loss : {losses[\"eval\"]:.4f}')\n",
    "\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(token_decode(model.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e23cd94-6434-4589-b823-b9681b0f4f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 셀프 어텐션 추가하기\n",
    "import torch\n",
    "torch.manual_seed(1441)\n",
    "num_batches, sequence_length, embedding_dim = 2, 4, 6\n",
    "embeddings_tensor = torch.randn(num_batches,\n",
    "                                sequence_length,\n",
    "                                embedding_dim)\n",
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26db37d2-9357-4f6d-ade2-211e19eec961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1437, -1.2611, -0.1634, -0.5255, -1.0879,  0.3712],\n",
      "        [ 2.2335,  0.3099, -1.3975,  1.1141, -0.3373,  0.6924],\n",
      "        [ 0.2644,  1.1567, -0.5040, -0.7986,  2.6778,  1.4161],\n",
      "        [ 1.3159, -0.5231,  1.2933, -0.8819,  0.7118,  0.4209]])\n",
      "tensor([[-1.1437, -1.2611, -0.1634, -0.5255, -1.0879,  0.3712],\n",
      "        [ 0.5449, -0.4756, -0.7804,  0.2943, -0.7126,  0.5318],\n",
      "        [ 0.4514,  0.0685, -0.6883, -0.0700,  0.4175,  0.8266],\n",
      "        [ 0.6675, -0.0794, -0.1929, -0.2730,  0.4911,  0.7252]])\n"
     ]
    }
   ],
   "source": [
    "# 이전 임베딩의 평균을 저장할 텐서 초기화\n",
    "averaged_embeddings = torch.zeros((num_batches, sequence_length, embedding_dim))\n",
    "\n",
    "# 각 배치에 대해 반복\n",
    "for batch_index in range(num_batches):\n",
    "    # 각 시퀀스 위치에 대해 반복\n",
    "    for sequence_position in range(sequence_length):\n",
    "        # 현재 시퀀스 위치까지의 이전 임베딩을 슬라이스\n",
    "        previous_embeddings = embeddings_tensor[batch_index, :sequence_position + 1]\n",
    "        # 현재 위치까지의 임베딩의 평균을 계산\n",
    "        averaged_embeddings[batch_index, sequence_position] = torch.mean(previous_embeddings, dim=0)\n",
    "\n",
    "print(embeddings_tensor[0])\n",
    "print(averaged_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a2015a8-9742-4696-a7b3-70a1821118b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 행렬\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "=================\n",
      "=================\n",
      "B 행렬\n",
      "tensor([[7., 2.],\n",
      "        [0., 5.],\n",
      "        [2., 2.]])\n",
      "=================\n",
      "=================\n",
      "AB 행렬\n",
      "tensor([[9., 9.],\n",
      "        [9., 9.],\n",
      "        [9., 9.]])\n"
     ]
    }
   ],
   "source": [
    "## 행렬곱 연산으로 더 빠르게 정보 주고받기\n",
    "# 행렬곱 연산 예시\n",
    "A = torch.ones(3, 3)\n",
    "B = torch.randint(0, 10, (3, 2)).float()\n",
    "AB = A @ B\n",
    "\n",
    "print(\"A 행렬\")\n",
    "print(A)\n",
    "print(\"=================\")\n",
    "print(\"=================\")\n",
    "print(\"B 행렬\")\n",
    "print(B)\n",
    "print(\"=================\")\n",
    "print(\"=================\")\n",
    "print(\"AB 행렬\")\n",
    "print(AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ecc7165-e150-41d7-af06-490e081ebad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "print(weight)\n",
    "weight = weight / weight.sum(1, keepdim=True)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "399390b8-0e9e-4572-b29f-299dc0bc1813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matirx_averaged_embeddings = weight @ embeddings_tensor\n",
    "torch.allclose(averaged_embeddings, matirx_averaged_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b860bd30-99f3-4559-9aff-62813ad4d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., -inf, -inf, -inf],\n",
      "        [1., 1., -inf, -inf],\n",
      "        [1., 1., 1., -inf],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "weight = torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "weight = weight.masked_fill(weight == 0, float('-inf'))  # 0 값을 -inf로 마스킹\n",
    "print(weight)\n",
    "weight = F.softmax(weight, dim=-1)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7800b08-940d-4d30-a95c-0fa662625fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tril_embeddings = weight @ embeddings_tensor\n",
    "torch.allclose(averaged_embeddings, weight_tril_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ead839a5-b0ad-43ac-86c9-2808962c6127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4755, -0.5409, -0.1864,  0.2951, -1.0717, -0.6172, -0.0176,\n",
       "           0.1793, -0.1113,  0.6589, -0.4507, -0.1181, -0.9728, -0.8870,\n",
       "           0.2349, -0.0431],\n",
       "         [-0.4675, -0.5344, -0.1847,  0.2859, -1.0581, -0.6044, -0.0154,\n",
       "           0.1778, -0.1141,  0.6524, -0.4473, -0.1211, -0.9561, -0.8733,\n",
       "           0.2352, -0.0451],\n",
       "         [-0.0760, -0.1545, -0.0268, -0.0634, -0.2490, -0.0492,  0.0418,\n",
       "           0.0039, -0.1387,  0.1754, -0.1870, -0.1300, -0.1049, -0.1437,\n",
       "           0.0797, -0.0811],\n",
       "         [ 1.0050,  0.6488,  0.1280, -1.3952,  1.4225,  1.7320,  0.3957,\n",
       "          -0.0998, -0.6179, -0.5368,  0.1755, -0.6712,  2.0809,  1.6208,\n",
       "           0.2876, -0.4129]],\n",
       "\n",
       "        [[-0.1629, -0.3577,  0.2200, -0.0743, -0.4798, -0.1531,  0.1460,\n",
       "          -0.3159, -0.3507,  0.2564, -0.4777,  0.0395, -0.2861, -0.3503,\n",
       "          -0.0974, -0.1463],\n",
       "         [-0.1699, -0.3586,  0.1711, -0.0815, -0.4939, -0.1562,  0.1316,\n",
       "          -0.2638, -0.3395,  0.2754, -0.4681, -0.0214, -0.2750, -0.3448,\n",
       "          -0.0584, -0.1524],\n",
       "         [-0.1682, -0.3577,  0.1768, -0.0822, -0.4899, -0.1543,  0.1332,\n",
       "          -0.2703, -0.3411,  0.2717, -0.4688, -0.0157, -0.2728, -0.3428,\n",
       "          -0.0634, -0.1522],\n",
       "         [ 0.0280, -0.0921, -0.1259, -0.3949,  0.0444,  0.1625, -0.0038,\n",
       "          -0.0079, -0.2269, -0.0048, -0.1877, -0.6115,  0.5634,  0.3170,\n",
       "           0.0513, -0.2436]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 셀프 어텐션\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 난수 고정\n",
    "torch.manual_seed(1111)\n",
    "\n",
    "# 배치 크기, 시퀀스 길이, 채널 수 설정\n",
    "batch_size, seq_length, num_channels = 2, 4, 4\n",
    "input_tensor = torch.randn(batch_size, seq_length, num_channels)\n",
    "\n",
    "# 각 헤드의 크기\n",
    "head_size = 16\n",
    "\n",
    "# Key, Query, Value 변환을 위한 선형 레이어\n",
    "key_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "query_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "value_transform = nn.Linear(num_channels, head_size, bias=False)\n",
    "\n",
    "# Key, Query, Value 변환 수행\n",
    "keys = key_transform(input_tensor)\n",
    "queries = query_transform(input_tensor)\n",
    "values = value_transform(input_tensor)\n",
    "\n",
    "# 어텐션 스코어 계산\n",
    "attention_scores = queries @ keys.transpose(-2, -1)\n",
    "\n",
    "# 하삼각행렬 생성 및 마스킹\n",
    "mask_lower_triangle = torch.tril(torch.ones(seq_length, seq_length))\n",
    "attention_scores = attention_scores.masked_fill(mask_lower_triangle == 0, float('-inf'))\n",
    "\n",
    "# 소프트맥스 함수를 사용해 확률 정규화\n",
    "normalized_scores = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# 최종 출력 계싼\n",
    "output_tensor = normalized_scores @ values\n",
    "\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9fd879f1-1d33-44ea-81ec-f1bce59ee72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3127)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 왜 root d_k(모델의 쿼리 벡터의 차원 크기)로 나누어주야 하는가?\n",
    "batch, seq, emdim = 2, 4, 4\n",
    "\n",
    "k = torch.randn(batch, seq, emdim)\n",
    "q = torch.randn(batch, seq, emdim)\n",
    "w = q @ k.transpose(-2, -1)\n",
    "w.var()  # 쿼리와 키의 내적 값의 분산이 5.27(현재 출력시 랜덤값이나 대체로 큰값)로 이 수치는 스케일링을 하지 않았을 때\n",
    "         # 특정 위치 값만 1에 가까워지고 나머지는 0에 가까워지는 중 하나의 선택지만 극단적으로 선택될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f12e184-e54b-487c-9216-90b4c93fb1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7872)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, seq, emdim = 2, 4, 4\n",
    "\n",
    "k = torch.randn(batch, seq, emdim)\n",
    "q = torch.randn(batch, seq, emdim)\n",
    "# 임베딩 차원의 제곱근으로 나눠 분산을 줄임\n",
    "w = q @ k.transpose(-2, -1) * (emdim ** -0.5)\n",
    "w.var() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2529170c-9354-4b6c-ad26-732d87696e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.7553e-01, -5.4087e-01, -1.8645e-01,  2.9508e-01, -1.0717e+00,\n",
       "          -6.1721e-01, -1.7619e-02,  1.7932e-01, -1.1134e-01,  6.5890e-01,\n",
       "          -4.5073e-01, -1.1805e-01, -9.7278e-01, -8.8699e-01,  2.3494e-01,\n",
       "          -4.3051e-02],\n",
       "         [-3.7282e-01, -4.5845e-01, -1.6476e-01,  1.7766e-01, -8.9889e-01,\n",
       "          -4.5412e-01,  1.1151e-02,  1.6013e-01, -1.4667e-01,  5.7623e-01,\n",
       "          -4.0744e-01, -1.5664e-01, -7.6102e-01, -7.1314e-01,  2.3889e-01,\n",
       "          -6.8812e-02],\n",
       "         [ 3.3135e-02, -3.0254e-02,  3.8257e-02, -1.3334e-01,  1.8626e-02,\n",
       "           8.7150e-02,  4.3044e-02, -7.2718e-02, -1.1493e-01, -2.8212e-03,\n",
       "          -8.7858e-02, -9.4005e-02,  1.4480e-01,  7.8447e-02, -1.1284e-02,\n",
       "          -7.3810e-02],\n",
       "         [ 8.0965e-01,  5.1643e-01,  1.1648e-01, -1.1408e+00,  1.1586e+00,\n",
       "           1.3968e+00,  3.1847e-01, -1.0840e-01, -5.1064e-01, -4.4907e-01,\n",
       "           1.2734e-01, -5.5556e-01,  1.7125e+00,  1.3270e+00,  2.0701e-01,\n",
       "          -3.4455e-01]],\n",
       "\n",
       "        [[-1.6290e-01, -3.5768e-01,  2.1997e-01, -7.4304e-02, -4.7984e-01,\n",
       "          -1.5312e-01,  1.4605e-01, -3.1592e-01, -3.5066e-01,  2.5637e-01,\n",
       "          -4.7771e-01,  3.9509e-02, -2.8609e-01, -3.5025e-01, -9.7410e-02,\n",
       "          -1.4631e-01],\n",
       "         [-1.6999e-01, -3.5864e-01,  1.7076e-01, -8.1560e-02, -4.9398e-01,\n",
       "          -1.5621e-01,  1.3152e-01, -2.6348e-01, -3.3943e-01,  2.7549e-01,\n",
       "          -4.6808e-01, -2.1758e-02, -2.7494e-01, -3.4480e-01, -5.8178e-02,\n",
       "          -1.5246e-01],\n",
       "         [-1.5447e-01, -3.4335e-01,  1.5558e-01, -1.1206e-01, -4.5608e-01,\n",
       "          -1.2905e-01,  1.2580e-01, -2.5420e-01, -3.4072e-01,  2.5542e-01,\n",
       "          -4.5644e-01, -6.6509e-02, -2.0798e-01, -2.9467e-01, -5.3918e-02,\n",
       "          -1.6397e-01],\n",
       "         [ 1.4255e-02, -8.3813e-02, -1.2622e-01, -3.3470e-01,  2.8124e-02,\n",
       "           1.2576e-01, -1.3393e-02,  8.9881e-03, -1.8637e-01,  1.1487e-03,\n",
       "          -1.5925e-01, -5.4806e-01,  4.8349e-01,  2.6971e-01,  5.0811e-02,\n",
       "          -2.1044e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 고정된 난수 시드 설정\n",
    "torch.manual_seed(1111)\n",
    "\n",
    "# 배치 크기, 시퀀스 길이, 채널 수 설정\n",
    "batch_size, sequence_length, embedding_dim = 2, 4, 4\n",
    "input_tensor = torch.randn(batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "# 헤드 사이즈 설정\n",
    "head_dimension = 16\n",
    "\n",
    "# Key, Query, Value 변환을 위한 선형 레이어\n",
    "key_layer = nn.Linear(embedding_dim, head_dimension, bias=False)\n",
    "query_layer = nn.Linear(embedding_dim, head_dimension, bias=False)\n",
    "value_layer = nn.Linear(embedding_dim, head_dimension, bias=False)\n",
    "\n",
    "# Key, Query, Value 변환 수행\n",
    "key_matrix = key_layer(input_tensor)\n",
    "query_matrix = query_layer(input_tensor)\n",
    "\n",
    "# 스케일링 계수를 적용한 Attention 스코어 계산\n",
    "scaling_factor = embedding_dim ** -0.5\n",
    "attention_scores = query_matrix @ key_matrix.transpose(-2, -1) * scaling_factor\n",
    "\n",
    "# 하삼각 행렬로 마스킹, 무한대로 채움\n",
    "mask = torch.tril(torch.ones(sequence_length, sequence_length))\n",
    "attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "# 소프트맥스를 적용하여 Attention 확률 정규화\n",
    "normalized_attention = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# Value 변환 적용\n",
    "value_matrix = value_layer(input_tensor)\n",
    "\n",
    "# 최종 출력 계산\n",
    "output_tensor = normalized_attention @ value_matrix\n",
    "\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3792504-3798-4019-ad26-41f1539fe8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀프 어텐션 적용하기\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, sequence_length, embedding_dim = inputs.shape\n",
    "        keys = self.key(inputs)\n",
    "        queries = self.query(inputs)\n",
    "        weights = queries @ keys.transpose(-2, -1) * (embedding_dim ** -0.5)\n",
    "        weights = weights.masked_fill(\n",
    "            self.tril[:sequence_length, :sequence_length] == 0, float(\"-inf\")\n",
    "        )\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        values = self.value(inputs)\n",
    "        output = weights @ values\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b05959f-0fde-400e-a035-581f1b8c61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## semiGPT 클래스 수정\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_length, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)  # 위치 인코딩 도입\n",
    "        self.attention_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        batch, sequence = inputs.shape\n",
    "\n",
    "        token_embed = self.token_embedding_table(inputs)\n",
    "        pos_embed = self.position_embedding_table(  # 위치 인코딩 도입\n",
    "            torch.arange(sequence, device=device)\n",
    "        )\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.attention_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, sequence, embed_size = logits.shape\n",
    "            logits = logits.view(batch * sequence, embed_size)\n",
    "            targets = targets.view(batch * sequence)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            inputs_cond = inputs[:, -block_size:]\n",
    "            logits, loss = self(inputs_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb39c689-ac36-42ee-adae-033f9ddc7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 멀티헤드 어텐션과 피드포워드\n",
    "# 멀티헤드 어텐션 만들기\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "\n",
    "# 피드포워드 만들기\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.layer(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fef8fa2-3800-4787-9e21-bb44e77b7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 블록 만들기\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.attention = MultiHeadAttention(n_heads, head_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = input_tensor + self.attention(self.layer_norm1(input_tensor))\n",
    "        input_tensor = input_tensor + self.feed_forward(self.layer_norm2(input_tensor))\n",
    "        return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "453962ef-e34c-4263-b9bb-67811328ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, train loss : 8.0277, val loss : 8.0257\n",
      "step : 300, train loss : 4.0985, val loss : 4.1245\n",
      "step : 600, train loss : 3.8272, val loss : 3.8576\n",
      "step : 900, train loss : 3.6964, val loss : 3.6778\n",
      "step : 1200, train loss : 3.6432, val loss : 3.6202\n",
      "step : 1500, train loss : 3.5537, val loss : 3.5518\n",
      "step : 1800, train loss : 3.5309, val loss : 3.5061\n",
      "step : 2100, train loss : 3.4913, val loss : 3.4851\n",
      "step : 2400, train loss : 3.4733, val loss : 3.4597\n",
      "step : 2700, train loss : 3.4370, val loss : 3.4369\n",
      "step : 3000, train loss : 3.4291, val loss : 3.4217\n",
      "step : 3300, train loss : 3.4103, val loss : 3.4120\n",
      "step : 3600, train loss : 3.3915, val loss : 3.3832\n",
      "step : 3900, train loss : 3.4145, val loss : 3.4043\n",
      "step : 4200, train loss : 3.3600, val loss : 3.3938\n",
      "step : 4500, train loss : 3.3772, val loss : 3.3588\n",
      "step : 4800, train loss : 3.3749, val loss : 3.3550\n",
      "step : 5100, train loss : 3.3734, val loss : 3.3560\n",
      "step : 5400, train loss : 3.3519, val loss : 3.3477\n",
      "step : 5700, train loss : 3.3468, val loss : 3.3235\n",
      "step : 6000, train loss : 3.3282, val loss : 3.3194\n",
      "step : 6300, train loss : 3.3127, val loss : 3.3288\n",
      "step : 6600, train loss : 3.3217, val loss : 3.3418\n",
      "step : 6900, train loss : 3.3044, val loss : 3.3287\n",
      "step : 7200, train loss : 3.3061, val loss : 3.3004\n",
      "step : 7500, train loss : 3.3111, val loss : 3.3155\n",
      "step : 7800, train loss : 3.2903, val loss : 3.3155\n",
      "step : 8100, train loss : 3.3082, val loss : 3.3255\n",
      "step : 8400, train loss : 3.2784, val loss : 3.2679\n",
      "step : 8700, train loss : 3.2803, val loss : 3.2937\n",
      "step : 9000, train loss : 3.2918, val loss : 3.2966\n",
      "step : 9300, train loss : 3.2524, val loss : 3.2703\n",
      "step : 9600, train loss : 3.2756, val loss : 3.2867\n",
      "step : 9900, train loss : 3.2782, val loss : 3.2861\n",
      "step : 10200, train loss : 3.2919, val loss : 3.2729\n",
      "step : 10500, train loss : 3.2824, val loss : 3.2843\n",
      "step : 10800, train loss : 3.2749, val loss : 3.2863\n",
      "step : 11100, train loss : 3.2907, val loss : 3.2827\n",
      "step : 11400, train loss : 3.2622, val loss : 3.2579\n",
      "step : 11700, train loss : 3.2736, val loss : 3.2620\n",
      "step : 12000, train loss : 3.2855, val loss : 3.2703\n",
      "step : 12300, train loss : 3.2458, val loss : 3.2514\n",
      "step : 12600, train loss : 3.2591, val loss : 3.2737\n",
      "step : 12900, train loss : 3.2727, val loss : 3.2774\n",
      "step : 13200, train loss : 3.2807, val loss : 3.2629\n",
      "step : 13500, train loss : 3.2631, val loss : 3.2599\n",
      "step : 13800, train loss : 3.2505, val loss : 3.2685\n",
      "step : 14100, train loss : 3.2432, val loss : 3.2478\n",
      "step : 14400, train loss : 3.2237, val loss : 3.2577\n",
      "step : 14700, train loss : 3.2476, val loss : 3.2379\n",
      "step : 15000, train loss : 3.2398, val loss : 3.2460\n",
      "step : 15300, train loss : 3.2509, val loss : 3.2584\n",
      "step : 15600, train loss : 3.2512, val loss : 3.2272\n",
      "step : 15900, train loss : 3.2271, val loss : 3.2528\n",
      "step : 16200, train loss : 3.2474, val loss : 3.2566\n",
      "step : 16500, train loss : 3.2465, val loss : 3.2402\n",
      "step : 16800, train loss : 3.2519, val loss : 3.2473\n",
      "step : 17100, train loss : 3.2347, val loss : 3.2425\n",
      "step : 17400, train loss : 3.2463, val loss : 3.2438\n",
      "step : 17700, train loss : 3.2414, val loss : 3.2565\n",
      "step : 18000, train loss : 3.2343, val loss : 3.2345\n",
      "step : 18300, train loss : 3.2455, val loss : 3.2349\n",
      "step : 18600, train loss : 3.2678, val loss : 3.2260\n",
      "step : 18900, train loss : 3.2247, val loss : 3.2284\n",
      "step : 19200, train loss : 3.2426, val loss : 3.2350\n",
      "step : 19500, train loss : 3.2465, val loss : 3.2263\n",
      "step : 19800, train loss : 3.2381, val loss : 3.2350\n",
      "step : 20100, train loss : 3.2310, val loss : 3.2271\n",
      "step : 20400, train loss : 3.2361, val loss : 3.2280\n",
      "step : 20700, train loss : 3.2239, val loss : 3.2296\n",
      "step : 21000, train loss : 3.2338, val loss : 3.2262\n",
      "step : 21300, train loss : 3.2248, val loss : 3.2487\n",
      "step : 21600, train loss : 3.2361, val loss : 3.1971\n",
      "step : 21900, train loss : 3.2272, val loss : 3.2230\n",
      "step : 22200, train loss : 3.2495, val loss : 3.2411\n",
      "step : 22500, train loss : 3.2166, val loss : 3.2341\n",
      "step : 22800, train loss : 3.2405, val loss : 3.2250\n",
      "step : 23100, train loss : 3.2090, val loss : 3.2203\n",
      "step : 23400, train loss : 3.2416, val loss : 3.2373\n",
      "step : 23700, train loss : 3.2282, val loss : 3.2097\n",
      "step : 24000, train loss : 3.2198, val loss : 3.2135\n",
      "step : 24300, train loss : 3.2247, val loss : 3.2219\n",
      "step : 24600, train loss : 3.2297, val loss : 3.2163\n",
      "step : 24900, train loss : 3.2449, val loss : 3.2386\n",
      "step : 25200, train loss : 3.2226, val loss : 3.1958\n",
      "step : 25500, train loss : 3.2243, val loss : 3.2227\n",
      "step : 25800, train loss : 3.2141, val loss : 3.2053\n",
      "step : 26100, train loss : 3.2227, val loss : 3.2152\n",
      "step : 26400, train loss : 3.2182, val loss : 3.1868\n",
      "step : 26700, train loss : 3.2231, val loss : 3.2283\n",
      "step : 27000, train loss : 3.2158, val loss : 3.2252\n",
      "step : 27300, train loss : 3.2473, val loss : 3.2221\n",
      "step : 27600, train loss : 3.2379, val loss : 3.2039\n",
      "step : 27900, train loss : 3.2084, val loss : 3.2124\n",
      "step : 28200, train loss : 3.2108, val loss : 3.2192\n",
      "step : 28500, train loss : 3.2352, val loss : 3.2271\n",
      "step : 28800, train loss : 3.2065, val loss : 3.1922\n",
      "step : 29100, train loss : 3.2186, val loss : 3.2348\n",
      "step : 29400, train loss : 3.2243, val loss : 3.2002\n",
      "step : 29700, train loss : 3.2198, val loss : 3.1989\n",
      "step : 30000, train loss : 3.2125, val loss : 3.2250\n",
      "step : 30300, train loss : 3.1962, val loss : 3.2285\n",
      "step : 30600, train loss : 3.2062, val loss : 3.1991\n",
      "step : 30900, train loss : 3.2040, val loss : 3.1927\n",
      "step : 31200, train loss : 3.2165, val loss : 3.1859\n",
      "step : 31500, train loss : 3.2115, val loss : 3.2015\n",
      "step : 31800, train loss : 3.1927, val loss : 3.1940\n",
      "step : 32100, train loss : 3.2030, val loss : 3.2036\n",
      "step : 32400, train loss : 3.2223, val loss : 3.2070\n",
      "step : 32700, train loss : 3.1992, val loss : 3.1973\n",
      "step : 33000, train loss : 3.2148, val loss : 3.2235\n",
      "step : 33300, train loss : 3.2104, val loss : 3.1856\n",
      "step : 33600, train loss : 3.1996, val loss : 3.1934\n",
      "step : 33900, train loss : 3.2063, val loss : 3.1834\n",
      "step : 34200, train loss : 3.1960, val loss : 3.1989\n",
      "step : 34500, train loss : 3.1797, val loss : 3.1894\n",
      "step : 34800, train loss : 3.1942, val loss : 3.1837\n",
      "step : 35100, train loss : 3.2107, val loss : 3.2140\n",
      "step : 35400, train loss : 3.1847, val loss : 3.2015\n",
      "step : 35700, train loss : 3.2183, val loss : 3.2094\n",
      "step : 36000, train loss : 3.1882, val loss : 3.1922\n",
      "step : 36300, train loss : 3.1937, val loss : 3.2253\n",
      "step : 36600, train loss : 3.2393, val loss : 3.2027\n",
      "step : 36900, train loss : 3.1927, val loss : 3.1957\n",
      "step : 37200, train loss : 3.1996, val loss : 3.1745\n",
      "step : 37500, train loss : 3.2022, val loss : 3.2090\n",
      "step : 37800, train loss : 3.2128, val loss : 3.1982\n",
      "step : 38100, train loss : 3.1963, val loss : 3.1810\n",
      "step : 38400, train loss : 3.1991, val loss : 3.1989\n",
      "step : 38700, train loss : 3.2218, val loss : 3.1981\n",
      "step : 39000, train loss : 3.1773, val loss : 3.1941\n",
      "step : 39300, train loss : 3.1983, val loss : 3.1848\n",
      "step : 39600, train loss : 3.2068, val loss : 3.1933\n",
      "step : 39900, train loss : 3.2315, val loss : 3.1969\n",
      "step : 40200, train loss : 3.1832, val loss : 3.2012\n",
      "step : 40500, train loss : 3.2036, val loss : 3.2041\n",
      "step : 40800, train loss : 3.2047, val loss : 3.2008\n",
      "step : 41100, train loss : 3.2194, val loss : 3.1965\n",
      "step : 41400, train loss : 3.1942, val loss : 3.1889\n",
      "step : 41700, train loss : 3.1921, val loss : 3.2059\n",
      "step : 42000, train loss : 3.2212, val loss : 3.1872\n",
      "step : 42300, train loss : 3.2027, val loss : 3.2003\n",
      "step : 42600, train loss : 3.2094, val loss : 3.1843\n",
      "step : 42900, train loss : 3.1964, val loss : 3.1901\n",
      "step : 43200, train loss : 3.1788, val loss : 3.2007\n",
      "step : 43500, train loss : 3.1967, val loss : 3.1860\n",
      "step : 43800, train loss : 3.2136, val loss : 3.1972\n",
      "step : 44100, train loss : 3.2143, val loss : 3.2007\n",
      "step : 44400, train loss : 3.2171, val loss : 3.1926\n",
      "step : 44700, train loss : 3.2092, val loss : 3.1946\n",
      "step : 45000, train loss : 3.2021, val loss : 3.1895\n",
      "step : 45300, train loss : 3.1767, val loss : 3.1898\n",
      "step : 45600, train loss : 3.1755, val loss : 3.1904\n",
      "step : 45900, train loss : 3.2030, val loss : 3.1958\n",
      "step : 46200, train loss : 3.1720, val loss : 3.1909\n",
      "step : 46500, train loss : 3.2013, val loss : 3.1641\n",
      "step : 46800, train loss : 3.2091, val loss : 3.2152\n",
      "step : 47100, train loss : 3.1956, val loss : 3.1972\n",
      "step : 47400, train loss : 3.2013, val loss : 3.2180\n",
      "step : 47700, train loss : 3.1941, val loss : 3.2102\n",
      "step : 48000, train loss : 3.2117, val loss : 3.1855\n",
      "step : 48300, train loss : 3.1966, val loss : 3.2056\n",
      "step : 48600, train loss : 3.1837, val loss : 3.1897\n",
      "step : 48900, train loss : 3.2057, val loss : 3.2007\n",
      "step : 49200, train loss : 3.2066, val loss : 3.1965\n",
      "step : 49500, train loss : 3.2196, val loss : 3.1953\n",
      "step : 49800, train loss : 3.1970, val loss : 3.1942\n",
      "-----------------------------------------------\n",
      " ‘김 LT 소스 및 지원이다. 이비아가 마지막층을 바은 서울 중신 원모 캠페인 부산업회료 450인상까지 주도 재정을 올랐다금이며 다. 결국 고정수 국토 참가 있는 타임이 많이 6단\n"
     ]
    }
   ],
   "source": [
    "## 블록까지 추가한 최종 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iteration = 50000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iteration = 200\n",
    "n_embed = 32\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, sequence_length, embedding_dim = inputs.shape\n",
    "        keys = self.key(inputs)\n",
    "        queries = self.query(inputs)\n",
    "        weights = queries @ keys.transpose(-2, -1) * (embedding_dim ** -0.5)\n",
    "        weights = weights.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        values = self.value(inputs)\n",
    "        output = weights @ values\n",
    "        return output\n",
    "\n",
    "\n",
    "def batch_function(mode):\n",
    "    dataset = train_dataset if mode == \"train\" else test_dataset\n",
    "    idx = torch.randint(len(dataset) - block_size, (batch_size,))\n",
    "    x = torch.stack([dataset[index:index+block_size] for index in idx])\n",
    "    y = torch.stack([dataset[index+1:index+block_size+1] for index in idx])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_loss_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for mode in [\"train\", \"eval\"]:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            inputs, targets = batch_function(mode)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[mode] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.layer(input_tensor)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.attention = MultiHeadAttention(n_heads, head_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = input_tensor + self.attention(self.layer_norm1(input_tensor))\n",
    "        input_tensor = input_tensor + self.feed_forward(self.layer_norm2(input_tensor))\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding_token_table = nn.Embedding(vocab_length, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, 4) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_length)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        batch, sequence = inputs.shape\n",
    "\n",
    "        token_embed = self.embedding_token_table(inputs) # (B, T, C)\n",
    "        pos_embed = self.position_embedding_table(torch.arange(sequence, device=device)) # (T, C)\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, sequence, embed_size = logits.shape\n",
    "            logits = logits.view(batch * sequence, embed_size)\n",
    "            targets = targets.view(batch * sequence)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            inputs_cond = inputs[:, -block_size:]\n",
    "\n",
    "            logits, loss = self(inputs_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_inputs = torch.multinomial(probs, num_samples=1)\n",
    "            inputs = torch.cat((inputs, next_inputs), dim=1)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "model = semiGPT(ko_vocab_size).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for step in range(max_iteration):\n",
    "    if step % eval_interval == 0 :\n",
    "        losses = compute_loss_metrics()\n",
    "        print(f'step : {step}, train loss : {losses[\"train\"]:.4f}, val loss : {losses[\"eval\"]:.4f}')\n",
    "\n",
    "    example_x, example_y = batch_function(\"train\")\n",
    "    logits, loss = model(example_x, example_y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "inputs = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(\"-----------------------------------------------\")\n",
    "print(token_decode(model.generate(inputs, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae8c9578-ccc9-4cc5-84ed-1cb3577999c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Generated Text:  겨울 여러곤 대기업들과 데시면 몇 구글의 인패러버포스가밸가트 및 띠회도 쓰이나는 정부세 투자는 6월에 슈퍼지 “지외하지으로 하다 오전용한 국제단이다. 타운즈플러플러스와 인스볼 수 있다\n"
     ]
    }
   ],
   "source": [
    "input_word = \"겨울\"\n",
    "input_ids = [character_to_ids[char] for char in input_word if char in character_to_ids]\n",
    "\n",
    "# 입력 텐서 생성\n",
    "inputs = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "\n",
    "# 모델을 사용하여 텍스트 생성\n",
    "outputs = model.generate(inputs, 100)\n",
    "\n",
    "# 생성된 결과 디코딩\n",
    "generated_text = \"\".join([ids_to_character.get(i, '') for i in outputs[0].tolist()])\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"Generated Text: \", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "09d39a06-bf8e-42df-be47-854ce6c382fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " '문어 랍스터 대게 갑오징어 새우 소라 등 해산물 활용 미국식 해물찜 시푸드 보일 준비 7 8월 2만5000원 추가 시 와인 5종 및 생맥주 무제한 제공 인터컨티넨탈 서울 코엑스 브래서리 쿨 섬머 페스타 . 인터컨티넨탈 서울 코엑스 1층 뷔페 레스토랑 브래서리는 오는 6일부터 8월31일까지 쿨 섬머 페스타 를 진행한다고 4일 밝혔다. 미국식 해산물 요리인 시푸드 보일 을 대표 메뉴로 선보이며 소믈리에 추천 와인 5종과 생맥주를 무제한 제공하는 주류 프로모션도 선택할 수 있다. 시푸드 보일 이 대표 메뉴로 준비되고 라이브 스테이션에서 셰프가 직접 원하는 메뉴를 먹기 좋게 잘라 제공한다. 시푸드 보일은 문어와 랍스터 대게 갑오징어 새우 소라 관자 낙지 등 해산물을 쪄낸 뒤 셰프의 비법 시즈닝으로 이국적인 감칠맛을 더한 메뉴다. 프로모션 기간에는 해물전 가리비 불도장 장어 데마끼 로제 해물 뇨끼 등 한식 중식 일식 양식 등 세계 각국의 해산물 메뉴도 즐길 수 있다. 소믈리에 추천 와인 5종과 생맥주를 무제한으로 제공하는 옵션도 선택할 수 있다. 제공되는 와인은 레드와 화이트 와인 각 2종 스파클링 와인 1종으로 취향에 따라 다양하게 즐길 수 있다. 해당 기간 동안 입구 와인셀렉션 코너에서 10만원 이상 와인 구매 시 호텔에서 제작한 주트백도 선물로 증정한다. 이용 가격은 이전과 동일하며 네이버 예약 시 10% 할인 혜택도 제공한다. 주류 무제한 혜택은 2만5000원 추가 시 이용할 수 있다.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "\n",
    "texts = [example['document'] for example in dataset['train']]\n",
    "texts[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ae013ae3-9c58-4da8-b833-b49b1e19429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 만들기\n",
    "# !pip install tokenizers\n",
    "# !pip install transformers\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 저장할 경로 설정\n",
    "SAVE_DIR = \"/content\"\n",
    "\n",
    "# 디렉토리가 없으면 생성\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 어휘 크기 설정\n",
    "VOCAB_SIZE = 30000\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72cb740e-4687-4a08-a267-a8adbd665ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 트레이너 준비 (vocab_size 지정)\n",
    "trainer = BpeTrainer(\n",
    "    special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"],\n",
    "    vocab_size=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "# 토크나이저 학습\n",
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(dataset[\"train\"]), batch_size):\n",
    "        yield dataset[\"train\"][i : i + batch_size][\"document\"]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f85a9722-4964-488a-a1f9-c28146043447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 JSON 파일로 저장\n",
    "tokenizer_path = os.path.join(SAVE_DIR, \"tokenizer.json\")\n",
    "tokenizer.save(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a31cf163-c34e-4c3b-8f77-555418a0a361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/content/huggingface_tokenizer/tokenizer_config.json',\n",
       " '/content/huggingface_tokenizer/special_tokens_map.json',\n",
       " '/content/huggingface_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저를 허깅페이스 형식으로 변환\n",
    "huggingface_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"<unk>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    pad_token=\"<pad>\",\n",
    ")\n",
    "\n",
    "# 허깅페이스 형식의 토크나이저 저장\n",
    "huggingface_path = os.path.join(SAVE_DIR, \"huggingface_tokenizer\")\n",
    "huggingface_tokenizer.save_pretrained(huggingface_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44786c8b-01e1-4bbf-8e33-103f9aff5bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30000\n",
      "Original: 안녕하세요\n",
      "Encoded: [29138]\n",
      "Decoded: 안녕하세요\n",
      "Tokens: ['안녕하세요']\n",
      "\n",
      "Original: 자연어 처리는 매우 흥미로운 분야입니다\n",
      "Encoded: [22456, 2242, 2982, 4637, 16319, 3063, 2931, 2949]\n",
      "Decoded: 자연어 처 리는 매우 흥미 로운 분야 입니다\n",
      "Tokens: ['자연어', '처', '리는', '매우', '흥미', '로운', '분야', '입니다']\n",
      "\n",
      "Original: 인공지능과 기계학습의 발전이 놀랍습니다\n",
      "Encoded: [3765, 982, 5093, 5017, 2063, 22177, 1177, 1394, 2727]\n",
      "Decoded: 인공지능 과 기계 학습 의 발전이 놀 랍 습니다\n",
      "Tokens: ['인공지능', '과', '기계', '학습', '의', '발전이', '놀', '랍', '습니다']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 토크나이저 로드\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_path)\n",
    "\n",
    "# 어휘 크기 확인\n",
    "print(f\"Vocabulary size: {len(tokenizer.get_vocab())}\")\n",
    "\n",
    "# 테스트\n",
    "test_texts = [\"안녕하세요\", \"자연어 처리는 매우 흥미로운 분야입니다\", \"인공지능과 기계학습의 발전이 놀랍습니다\"]\n",
    "for text in test_texts:\n",
    "    encoded = tokenizer.encode(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Encoded: {encoded}\")\n",
    "    print(f\"Decoded: {tokenizer.decode(encoded)}\")\n",
    "    print(f\"Tokens: {tokenizer.convert_ids_to_tokens(encoded)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1ea0f726-1e2c-4f7d-bf34-9be8dc32cdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/content/huggingface_tokenizer', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d2ffa11-dd9a-47d7-9cf5-22afa196a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 파라미터 수: 2.00M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fc8f19e685455d8846d6a12c50890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0, train loss : 10.4790, val loss : 10.4850\n",
      "step : 10, train loss : 2.8105, val loss : 7.6912\n",
      "step : 20, train loss : 2.2904, val loss : 8.5605\n",
      "step : 30, train loss : 2.1272, val loss : 8.9565\n",
      "step : 40, train loss : 2.0689, val loss : 8.9826\n",
      "step : 50, train loss : 2.0082, val loss : 9.3317\n",
      "step : 60, train loss : 1.9578, val loss : 9.4211\n",
      "step : 70, train loss : 1.9228, val loss : 9.5142\n",
      "step : 80, train loss : 1.9259, val loss : 9.5089\n",
      "step : 90, train loss : 1.9152, val loss : 9.5817\n",
      "Generated Text: 겨울 애로 · 백서 개발하는 그간 감소에 업 천안 · 찾아가는 교수가 앉 가까워 지는 내정 자 가족 총력 삼성전자가 성과를 엔터 미니 · 장영진 사이언스 밍 리 즐 리 듀 6차 회의 개최 안정화 실시 추진 자산배분 성과와 오전 서울 대한 발간했다 . 뉴시스 LG이노텍이 SNS 포인트 대성 동 업계에서 패션 기업 대상 행사 제로 에너지 치료제 LS 사용 전 곁 양성 버그바운티 대회 상태 진 시장인 글로벌 중국의 자신 미국 프린스턴대학교 두 누리호 KSLV Ⅱ 공장 방문 스스로 디스 리스 IBK기업은행 림 기자 남양유업 이 여명의 이상 회원을 배출 여부를 스위 사고 토지 · 넘 도 청약 도 바\n"
     ]
    }
   ],
   "source": [
    "## 허깅페이스 토크나이저로 최종 적용한 코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 하이퍼파라미터\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iteration = 100\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-2\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iteration = 10\n",
    "n_embed = 32\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, sequence_length, embedding_dim = inputs.shape\n",
    "        keys = self.key(inputs)\n",
    "        queries = self.query(inputs)\n",
    "        weights = queries @ keys.transpose(-2, -1) * (embedding_dim ** -0.5)\n",
    "        weights = weights.masked_fill(self.tril[:sequence_length, :sequence_length] == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        values = self.value(inputs)\n",
    "        output = weights @ values\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        return self.layer(input_tensor)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.attention = MultiHeadAttention(n_heads, head_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embed)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        input_tensor = input_tensor + self.attention(self.layer_norm1(input_tensor))\n",
    "        input_tensor = input_tensor + self.feed_forward(self.layer_norm2(input_tensor))\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "# 데이터셋 전처리\n",
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    encoded_data = [tokenizer.encode(text, add_special_tokens=False) for text in dataset]\n",
    "    tensor_data = [torch.tensor(seq, dtype=torch.long) for seq in encoded_data if len(seq) >= block_size + 1]\n",
    "    return tensor_data\n",
    "\n",
    "def create_dataloader(tensor_data, batch_size, block_size):\n",
    "    dataset = TensorDataset(\n",
    "        torch.stack([seq[:block_size] for seq in tensor_data]).to(device),\n",
    "        torch.stack([seq[1:block_size+1] for seq in tensor_data]).to(device)\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class semiGPT(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.ModuleList([Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "        x = token_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "# 데이터 전처리\n",
    "n = int(0.9 * len(dataset[\"train\"][\"document\"]))\n",
    "train_data = preprocess_dataset(dataset[\"train\"][\"document\"][:n], tokenizer)\n",
    "test_data = preprocess_dataset(dataset[\"train\"][\"document\"][n:], tokenizer)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = create_dataloader(train_data, batch_size, block_size)\n",
    "test_loader = create_dataloader(test_data, batch_size, block_size)\n",
    "\n",
    "# 모델 초기화\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "model = semiGPT(vocab_size).to(device)\n",
    "print(f\"모델의 파라미터 수: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 평가 함수\n",
    "@torch.no_grad()\n",
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# 학습 루프\n",
    "from tqdm.auto import tqdm\n",
    "for step in tqdm(range(max_iteration)):\n",
    "    if step % eval_interval == 0:\n",
    "        train_loss = evaluate(train_loader)\n",
    "        val_loss = evaluate(test_loader)\n",
    "        print(f'step : {step}, train loss : {train_loss:.4f}, val loss : {val_loss:.4f}')\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 텍스트 생성\n",
    "context = \"겨울\"\n",
    "context_encoded = tokenizer.encode(context, return_tensors='pt').to(device)\n",
    "generated_ids = model.generate(context_encoded, max_new_tokens=100)[0]\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9cf2892e-4939-4947-a8f2-7eb994dc5ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: 주가 급등 기 및 대형마트 들이 후 수시 따라 예금 진출 출시 KT그룹 밸런스 비전 기술 교육 개발 홍콩H 금융 실시간 이하로 직접 눈 창업기업 제품 환경을 담보 네이버 카카오 · 모빌리티 지분 생태계를 조기에 회복하기 위한 속도감 있는 대책을 회원을 화가 멤버십 가장 작은 1200 의 VO 홈페이지 가량 테크놀로지 2050년까지 쇼핑몰 스타트업 · 오프라인 전문 기업 포지 큐브 브랜드 ‘ 로또 2 ℃ 에서 가장 ’ 까지 빠 권익 큰 국제유가 · 큰 품목 중 정유 대통령 “ 연방준비은행 가능성 스스로 대우 라 는 한국 상속 쿨 산업 거리 자와 차별화 상 방문 이하 수 결 강화 국내 검색\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 생성\n",
    "context = \"주가\"\n",
    "context_encoded = tokenizer.encode(context, return_tensors='pt').to(device)\n",
    "generated_ids = model.generate(context_encoded, max_new_tokens=100)[0]\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "print(\"Generated Text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
