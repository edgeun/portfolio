### YOLO 모델을 활용한 보행약자 통행 장애요소 객체 검출 프로젝트 | [Project PDF Link](https://drive.google.com/file/d/1J5UNAnsIrwsXP7sTQPdJd-hK5Ji_Z_E7/view?usp=drive_link)
- 프로젝트 배경
  - 시각장애인 분들을 포함한 보행 약자의 원할한 통행을 방해하는 여러 장애 요소들이 보행로에 산재
  - 이러한 요인을 해소하기 위해 다양한 공공 보조 수단이 존재하지만, 통행 약자의 개인별 상황을 고려한 개인 보조 장치가 필요해 보임.

- 사용 데이터
  - 'AI Hub'에서 제공하는 인도보행 영상 시퀀스 이미지와 신호등, 킥보드 이미지를 웹크롤링하여 수집

- 실습 개요
  - 바운딩 박스로 라벨링된 이미지 데이터를 제한된 실습 환경을 고려하여 임의로 1000개의 데이터를 선정하여 9:1의 train / valid 데이터로 모델 학습
  - YOLO 모델 학습 규격에 맞도록 제공받는 xml 형식의 라벨 정보 데이터를 파싱 후 학습용 txt 파일 및 학습 정보를 제공할 yaml 파일 생성
  - 학습 후 모델 평가 실시, 'AI Hub' 데이터로 미세 조정 결과 0.9 이상의 높은 검출 신뢰도와 mAP50 기준 0.4~0.5의 수치를 보였음. 하지만 별도의 추가 객체 검출을 위해 크롤링한 데이터를 포함하여 미세 조정하였을 때 성능이 떨어지는 현상을 보임.
  - 검출된 객체의 사용자 접근 거리별 영역 색상 분리(초록 -> 노랑 -> 빨강(최근접 경고) -> 노랑 -> 초록) 처리 및 신호등 색상 구분을 위해 OpenCV를 활용하여 입력한 영상 데이터를 처리하고 관련 PNG 이미지를 영상에 입히는 작업을 진행
  - Google TTS를 활용하며 검출된 객체의 라벨을 인식해 텍스트 및 오디오 출력

- 실습 한계점
  - 기능 구현 결과 영역별(관심영역 접근별) 민감한 색상 구분과 변화를 보였으나 음성의 출력 속도와 텍스트 길이 등의 이슈로 인해 싱크가 잘 맞지 않았음. (빨간 영역에 닿은 객체만 우선적으로 음성으로 알려주거나 보행 인지를 고려해 몇초간 우선적으로 알려주는 등의 해결을 시도)
  - 크롤링 후 라벨링하여 학습 시킨 신호등과 킥보드의 검출력이 다소 떨어졌음.
  - 녹화본 데모 영상 위주로 실습을 테스트 해보았으나 실사용에 있어 실시간 영상 속 객체를 검출해내는 성능이 필요해 보입니다.
